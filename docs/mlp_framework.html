<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.4.0"/>
    <title>mlp_framework API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>




            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#MLP">MLP</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MLP.__init__">MLP</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP._mx_size_map">_mx_size_map</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP._act_func_map">_act_func_map</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP._forward">_forward</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP._stochastic_descent">_stochastic_descent</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP._backprop">_backprop</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP._apply_grad">_apply_grad</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP.learn">learn</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP._static_plot">_static_plot</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP._dynamic_plot">_dynamic_plot</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP.export_mlp">export_mlp</a>
                        </li>
                        <li>
                                <a class="function" href="#MLP.import_mlp">import_mlp</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#main">main</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
mlp_framework    </h1>

                        <div class="docstring"><p>Features a flexible class implementation of dense neural networks from the first principles, as well as
several auxiliary methods for testing and debugging.</p>
</div>

                        <input id="mod-mlp_framework-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-mlp_framework-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="sd">Features a flexible class implementation of dense neural networks from the first principles, as well as</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="sd">several auxiliary methods for testing and debugging.</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="kn">import</span> <span class="nn">time</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a><span class="kn">import</span> <span class="nn">copy</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span> <span class="nn">itertools</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">import</span> <span class="nn">multiprocessing</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">Process</span><span class="p">,</span> <span class="n">Queue</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="n">np</span><span class="o">.</span><span class="n">seterr</span><span class="p">(</span><span class="n">under</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="sd">    A class implementation of a multilayer perceptron with a customizable size and configuration.</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="sd">    It employs only the core Python and numpy functions. The performance optimization is done via numpy vectorization.</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="sd">    Multiple activation and loss functions are supported and can be easily extended.</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a><span class="sd">    Provides two plotting methods (static / real-time).</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="sd">    Allows for storing the image of the network after the training in a file, as well as importing other saved images.</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mlp_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="sd">        The function is the constructor for a `MLP` class, which initializes the parameters and the state </span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a><span class="sd">        of the newly created neural network object.</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="sd">        The `MLP` object state is defined by mapping between each layer and the corresponding matrices that </span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="sd">        contain values for weights and biases plus the name of the activation function. It is represented as </span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="sd">        the dictionary with the following structure:</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a><span class="sd">        `{layer: ([w, b], &#39;act_func&#39;)}`, where:</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a><span class="sd">            - `w` is the matrix of weights for the layer;</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a><span class="sd">            - `b` is the matrix of biases for the layer;</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a><span class="sd">            - `&#39;act_func&#39;` is the activation function for the layer;</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a><span class="sd">            - `layer` is the number of the layer.</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a><span class="sd">        :param mlp_layout: A layout of the neural network. Defined as a tuple of type:</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a><span class="sd">        `((inp, l1, l2, ... , ln), act_map)`, where:</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a><span class="sd">            - `inp` denotes the size of the input;</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a><span class="sd">            - `ln` denotes the number of neurons in the n-th layer.</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a><span class="sd">            - `act_map` denotes types of activation funcs for each layer.</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a><span class="sd">        :param rand_range: A tuple containing the lower and upper limits for random initialization of </span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a><span class="sd">        parameter matrices</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="sd">        :param train_data: A sequence of training data samples of type:</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="sd">        `((a1, b1, c1, ...), (a2, b2, c2, ...), (an, bn, cn, ...))`, where:</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="sd">            - each sample contains `j` input and `k` output values, with `j` defined by `mlp_layout[0][0]`</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">        :param name: The name of the neural network that identifies it</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="sd">        :param rate: Is used to define the speed of learning in the neural network. It determines how quickly </span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">        the network adjusts its weights and biases during the training process. Defaults to `1` (optional)</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a>        <span class="c1"># to store shape of each parameter mx</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mx_size_map</span><span class="p">(</span><span class="n">mlp_layout</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">)</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>        <span class="c1"># to store activation funcs for each layer</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span> <span class="o">=</span> <span class="n">mlp_layout</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>        <span class="c1"># to map act. funcs and derivatives to each layer</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_func_map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span><span class="p">)</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a>        <span class="c1"># to store training dataset</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_train</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span> <span class="o">=</span> <span class="n">mlp_layout</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a>        <span class="c1"># to store the state of MLP</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>        <span class="c1"># to store activations</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>        <span class="c1"># to store the gradient</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>        <span class="c1"># constant for finite difference computation</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="mf">1e-1</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>        <span class="c1"># to define the speed of learning</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">rate</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>        <span class="c1"># initializes the parameters and the gradient</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>            <span class="c1"># gets the sizes of parameter matrixes for each layer</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>            <span class="n">w_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>            <span class="n">b_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>            <span class="c1"># creates two random matrixes for weights and biases per each layer</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>            <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">rand_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rand_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">w_shape</span><span class="p">)</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">rand_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rand_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">b_shape</span><span class="p">)</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>            <span class="c1"># initializes gradient table with empty matrixes of the same shape</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>            <span class="n">w_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">w_shape</span><span class="p">)</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>            <span class="n">b_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">b_shape</span><span class="p">)</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_grad</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">]</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>    <span class="c1"># helper methods for __init__</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>    <span class="k">def</span> <span class="nf">_mx_size_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a><span class="sd">        The function creates a mapping between each layer in a neural network and the sizes of</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a><span class="sd">        the corresponding weight and bias matrices.</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a><span class="sd">        </span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a><span class="sd">        :param params: The `params` parameter is a list that represents the number of neurons in each layer</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a><span class="sd">        of a neural network. For example, if `params = (10, 20, 30)`, it means that the neural network has 3</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a><span class="sd">        layers with 10 neurons in the input layer, 20 in the inner layer, and 30 in the output layer</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a><span class="sd">        :return: a dictionary where the keys are the layer numbers and the values are tuples. Each tuple</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a><span class="sd">        contains the size of the weight matrix and the size of the bias matrix for that layer:</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a><span class="sd">                `{1:(w1_size, b1_size), </span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a><span class="sd">                  2:(w2_size, b2_size),</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="sd">                  n:(wn_size, bn_size)}`.</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>        <span class="n">mlp_layout</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)):</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>            <span class="n">mlp_layout</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="p">]),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="p">]))</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>        <span class="k">return</span> <span class="n">mlp_layout</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>    <span class="k">def</span> <span class="nf">_act_func_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">act_map</span><span class="p">):</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a><span class="sd">        The function converts a provided map of activation functions for each layer into a</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a><span class="sd">        dictionary with direct references to the functions and their derivatives for each layer.</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a><span class="sd">        </span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a><span class="sd">        :param act_map: The `act_map` parameter is a dictionary that maps layer numbers to activation</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a><span class="sd">        function names. Each key-value pair in the dictionary represents a layer in the neural network,</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a><span class="sd">        where the key is the layer number (an integer) and the value is the name of the activation function</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a><span class="sd">        (a string)</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a><span class="sd">        :return: a new dictionary where each layer number is mapped to a tuple containing the activation</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a><span class="sd">        function and its derivative for that layer.</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>        <span class="c1"># validate the input</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">act_map</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span><span class="p">,</span> <span class="s2">&quot;Invalid activation map (length)&quot;</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="s2">&quot;Invalid layer number(s)&quot;</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">func_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="s2">&quot;Invalid function name(s)&quot;</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>        <span class="c1"># return a new dict with funcs and their derivatives mapped to each layer</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>        <span class="k">return</span> <span class="p">{</span><span class="n">layer</span><span class="p">:</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">func_name</span><span class="p">),</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">func_name</span> <span class="o">+</span> <span class="s2">&quot;_der&quot;</span><span class="p">))</span> <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>    <span class="c1"># methods for printing to the console</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">with_full_report</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a><span class="sd">        The function returns a string representation of the current state of the model, including</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a><span class="sd">        the model name, cost, parameters, and performance on training samples.</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a><span class="sd">        </span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="sd">        :param with_full_report: The (optional) `with_full_report` parameter is a boolean flag that determines whether</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="sd">        to include additional information in the string representation of the model&#39;s state. If</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a><span class="sd">        `with_full_report` is set to `True`, the string will include the values of parameters for each</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a><span class="sd">        neuron and the actual output of the model on all training samples. Defaults to `False`</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="sd">        :return: a string that represents the current state of the model.</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>        <span class="n">state_str</span>  <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;1) Model: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>        <span class="n">state_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;2) Cost: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>        <span class="k">if</span> <span class="n">with_full_report</span><span class="p">:</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>            <span class="c1"># prints out parameter matrixes (weights and biases)</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>            <span class="c1"># for each of the layers of the MLP</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>            <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;3) State of MLP parameters:</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>            <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">param_lst</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>                <span class="n">state_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Layer: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>                <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param_mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_lst</span><span class="p">):</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>                    <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>                        <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;&lt;&lt;&lt; Weights &gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>                    <span class="k">else</span><span class="p">:</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>                        <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt;&lt;&lt; Biases &gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>                    <span class="n">state_str</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">param_mx</span><span class="p">)</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>                <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">---------------</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>            <span class="c1"># prints out mapping between input values and</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>            <span class="c1"># the actual output of the MLP for all data samples</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>            <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;4) Total MLP performance:&quot;</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>            <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>                <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(&quot;</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>                <span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>                <span class="k">for</span> <span class="n">inp_val</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">:</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>                    <span class="n">state_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">inp_val</span><span class="si">}</span><span class="s2">, &quot;</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>                
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>                <span class="n">state_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">)&quot;</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>        <span class="k">return</span> <span class="n">state_str</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>    <span class="k">def</span> <span class="nf">_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a><span class="sd">        The function computes the current cost value of the neural network by calculating the mean</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a><span class="sd">        squared error (MSE) between the network&#39;s output and the expected output for each training data sample.</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a><span class="sd">        :return: the current cost value of the neural network.</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>        <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>        <span class="c1"># iterates through each entry of the training data</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>            <span class="c1"># splits training data array </span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>            <span class="c1"># into input/expected output subarrays</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>            <span class="nb">input</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>            <span class="n">expected</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">:</span> <span class="p">]])</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>            <span class="c1"># feeds input subarray into the MLP</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>            <span class="c1"># collects the output data array</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>            <span class="c1"># verifies that there is no mismatch between </span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>            <span class="c1"># the sizes of the expected and actual output arrays</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>            <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>            
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>            <span class="c1"># iterates through both arrays and </span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>            <span class="c1"># computes corresponding error values</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>            <span class="n">err_vals</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>            <span class="c1"># adds the accumulated error to the total cost</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>            <span class="n">result</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">err_vals</span><span class="p">)</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>        <span class="c1"># computes the mean error value per 1 sample of training data</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>        <span class="n">result</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>        <span class="k">return</span> <span class="n">result</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>    <span class="c1"># methods for computing activations</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>    <span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a><span class="sd">        The function computes the sigmoid of a given input.</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a><span class="sd">        </span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a><span class="sd">        :param x: The parameter `x` is the input value for which we want to compute the sigmoid function</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a><span class="sd">        :return: the value of sigmoid(x).</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>    <span class="k">def</span> <span class="nf">_relu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a><span class="sd">        The function computes the ReLU (Rectified Linear Unit) of a given input.</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a><span class="sd">        </span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a><span class="sd">        :param x: The parameter `x` is a scalar or an array-like object representing the input to the ReLU</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a><span class="sd">        function</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a><span class="sd">        :return: the maximum value between 0 and the input value x.</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>    <span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a><span class="sd">        The function computes the softmax in a stable manner of a given input.</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a><span class="sd">        </span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a><span class="sd">        :param x: The parameter `x` is a numpy array representing the input values for which we want to</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a><span class="sd">        compute the softmax function</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a><span class="sd">        :return: the softmax of the input array x.</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>        <span class="n">norm_x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>        <span class="n">exp_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">norm_x</span><span class="p">)</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>        <span class="n">result</span> <span class="o">=</span> <span class="n">exp_z</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_z</span><span class="p">)</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>        <span class="k">return</span> <span class="n">result</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>    <span class="c1"># methods for computing activation derivatives</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>    <span class="k">def</span> <span class="nf">_sigmoid_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a><span class="sd">        Computes the derivative values for f(x)=1/(1+e**(-x)).</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>    <span class="k">def</span> <span class="nf">_relu_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a><span class="sd">        Computes the derivative values for f(x)=max(0, x).</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>    <span class="k">def</span> <span class="nf">_softmax_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a><span class="sd">        Computes the derivative values for f(x)=softmax([x]).</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigmoid_der</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>    <span class="c1"># methods for computing loss derivatives</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>    <span class="k">def</span> <span class="nf">_abs_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a><span class="sd">        Computes the derivative values for f(x) = |x|.</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>        <span class="c1"># creates an array of ones with the same shape as input arr</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>        <span class="n">der_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>        <span class="c1"># replaces elements less than or equal to zero with -1</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>        <span class="n">der_arr</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>        <span class="k">return</span> <span class="n">der_arr</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>    <span class="k">def</span> <span class="nf">_sqr_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a><span class="sd">        Computes the derivative values for f(x)=x**2.</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>    <span class="k">def</span> <span class="nf">_half_sqr_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a><span class="sd">        Computes the derivative values for f(x)=(x**2)/2.</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>    <span class="k">def</span> <span class="nf">_lncosh_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a><span class="sd">        Computes the derivative values for f(x)=ln(cosh(x)).</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>    <span class="c1"># core methods for the learning process</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_arr</span><span class="p">):</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a><span class="sd">        The function takes an input array and passes it through each layer of a neural network,</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a><span class="sd">        applying weights, biases, and activation functions to produce the final output.</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a><span class="sd">        </span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a><span class="sd">        :param input_arr: The input_arr is a numpy array that represents the input to the neural network. It</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a><span class="sd">        is the input that will be forwarded through each layer of the network</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a><span class="sd">        :return: the activations of the last layer of the neural network.</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>        <span class="c1"># initializes input as the 0-th layer of activations</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a>        <span class="n">a0</span> <span class="o">=</span> <span class="n">input_arr</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a0</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a>        <span class="c1"># iterates through each layer of the MLP</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">param_lst</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>            <span class="c1"># gets activation func for current layer</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a>            <span class="n">act_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a>            <span class="c1"># retrieves values of weights</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a>            <span class="c1"># and biases of the layer</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a>            <span class="n">w</span> <span class="o">=</span> <span class="n">param_lst</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">param_lst</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a>            <span class="c1"># applies weights</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a>            <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>            <span class="c1"># applies biases</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a>            <span class="n">a1</span> <span class="o">+=</span> <span class="n">b</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a>            <span class="c1"># applies the activation function</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a>            <span class="n">a1</span> <span class="o">=</span> <span class="n">act_func</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a>            <span class="c1"># saves the resulting activations</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">a1</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a>            <span class="c1"># assigns the activations as</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>            <span class="c1"># an input for the next layer</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>            <span class="n">a0</span> <span class="o">=</span> <span class="n">a1</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>        <span class="c1"># returns activations of the last layer</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>        <span class="k">return</span> <span class="n">a0</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>    <span class="k">def</span> <span class="nf">_stochastic_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="p">):</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a><span class="sd">        The function implements the stochastic gradient descent algorithm for a neural</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a><span class="sd">        network, updating the parameters based on the computed gradients.</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a><span class="sd">        It does the following operations:</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a><span class="sd">            Keeps track of the total accumulated error value and the count of current samples.</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a><span class="sd">            Shuffles the training data randomly and iterates through each data sample.</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a><span class="sd">            For each sample, it </span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a><span class="sd">                1. splits the training data array into input and expected output subarrays;</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a><span class="sd">                2. computes activations for each layer, and saves the last layer activations (actual output);</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a><span class="sd">                3. sums up squares of error values of each activation in the last layer;</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a><span class="sd">                4. performs backpropagation and computes the gradient.</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a><span class="sd">            If the batch size is reached, it applies the accumulated gradient values to the parameters.</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a><span class="sd">            Finally, it updates the current cost of the model as an average error value per one sample.</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a><span class="sd">        </span>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a><span class="sd">        :param batch_ratio: The `batch_ratio` parameter is a float value that represents the ratio of the</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a><span class="sd">        total number of training samples that should be used in each batch. For example, if `batch_ratio` is</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a><span class="sd">        set to `0.5`, it means that each batch will contain `50%` of the total training samples.</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a>        <span class="c1"># keeps the total accumulated error value</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a>        <span class="n">total_err</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a>        <span class="c1"># keeps the count of current samples</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>        <span class="n">sample_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>        <span class="c1"># shuffles the training data randomly</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>        <span class="c1"># iterates through each data sample</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a>        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a>            <span class="c1"># splits training data array </span>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a>            <span class="c1"># into input/expected output subarrays</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a>            <span class="nb">input</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a>            <span class="n">expected</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">:</span> <span class="p">]])</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a>            <span class="c1"># computes activations for each layer</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>            <span class="c1"># and saves the last layer activations</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>            <span class="c1"># verifies that there is no mismatch between </span>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>            <span class="c1"># the sizes of the expected and actual output arrays</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>            <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>            <span class="c1"># computes activation derivative values for the last layer</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>            <span class="c1"># as a difference between expected and actual outputs of MLP</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>            <span class="c1"># sums up squares of error values </span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>            <span class="c1"># of each activation in the last layer</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>            <span class="n">total_err</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">da_next</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>            <span class="c1"># performs backpropagation and computes the gradient</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_backprop</span><span class="p">(</span><span class="n">da_next</span><span class="p">)</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>            <span class="n">sample_counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>            <span class="c1"># checks if the batch size is reached</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>            <span class="k">if</span> <span class="n">sample_counter</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">batch_ratio</span><span class="p">:</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>                <span class="c1"># applies accumulated gradient values to the parameters</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_grad</span><span class="p">(</span><span class="n">sample_counter</span><span class="p">)</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>                <span class="n">sample_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a>        <span class="c1"># applies the gradient of the samples</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>        <span class="c1"># that didn&#39;t reach the size of the batch</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>        <span class="k">if</span> <span class="n">sample_counter</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_apply_grad</span><span class="p">(</span><span class="n">sample_counter</span><span class="p">)</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>        <span class="c1"># updates the current cost of the model </span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>        <span class="c1"># as an average error value per 1 sample</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="n">total_err</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>    
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>    <span class="k">def</span> <span class="nf">_backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">da_next</span><span class="p">):</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a><span class="sd">        The function performs backpropagation to compute and accumulate the gradients of the weights and biases for later application to the parameters of a neural network.</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a><span class="sd">        </span>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a><span class="sd">        :param da_next: da_next is the derivative of the cost function with respect to the activations of</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a><span class="sd">        the next layer. It represents the backpropagated error from the next layer</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a>        <span class="c1"># iterates through each layer backwards starting from the last one</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a>            <span class="c1"># gets the weights for the current layer</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a>            <span class="n">layer_wghts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a>            
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>            <span class="c1"># gets the derivative of activation func for the current layer</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>            <span class="n">der_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>            <span class="c1"># computes the derivative expression vectorized for all activations</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>            <span class="n">der_expr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lncosh_der</span><span class="p">(</span><span class="n">da_next</span><span class="p">)</span> <span class="o">*</span> <span class="n">der_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a>            <span class="c1"># computes bias derivatives for the entire layer</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>            <span class="n">db</span> <span class="o">=</span> <span class="n">der_expr</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>            <span class="c1"># computes weights derivatives for the entire layer</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>            <span class="n">dw</span> <span class="o">=</span> <span class="n">der_expr</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a>            <span class="n">dw</span> <span class="o">=</span> <span class="n">dw</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a>            <span class="c1"># computes the gradient for current layer</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>            <span class="n">grad_mxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dw</span><span class="p">,</span> <span class="n">db</span><span class="p">]</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a>            <span class="c1"># adds up the gradient values of the current sample</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a>            <span class="c1"># to the gradient sum of previously computed samples</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">]):</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a>                <span class="n">mx</span> <span class="o">+=</span> <span class="n">grad_mxs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>            <span class="c1"># computes activation derivatives for the previous layer</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">@</span> <span class="n">layer_wghts</span><span class="o">.</span><span class="n">T</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a>    <span class="k">def</span> <span class="nf">_apply_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a><span class="sd">        The function applies the stored gradient values to the parameters of a neural network.</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a><span class="sd">        </span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a><span class="sd">        :param num_samples: The `num_samples` parameter represents the number of samples used to compute the</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a><span class="sd">        gradient. It is used to normalize the gradient update step by dividing it by the number of samples.</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a><span class="sd">        This helps to ensure that the gradient update is not too large or too small, regardless of the</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a><span class="sd">        number of samples used in the batch.</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos">420</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos">421</span></a>        <span class="c1"># iterates through parameters</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos">422</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">param_mxs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos">423</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param_mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_mxs</span><span class="p">):</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos">424</span></a>                <span class="c1"># gets the gradient values</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos">425</span></a>                <span class="n">grad_mx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos">426</span></a>                <span class="c1"># subtracts the gradient from the parameters</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos">427</span></a>                <span class="n">param_mx</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_mx</span> <span class="o">/</span> <span class="n">num_samples</span><span class="p">)</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos">428</span></a>                <span class="c1"># resets the gradient to zero for next training cycle</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos">429</span></a>                <span class="n">grad_mx</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-430"><a href="#L-430"><span class="linenos">430</span></a>
</span><span id="L-431"><a href="#L-431"><span class="linenos">431</span></a>    <span class="c1"># API method for setting up the learning parameters</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos">432</span></a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">_stochastic_descent</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plot_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">plot_dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">upd_interval</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">with_full_report</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_cost</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos">433</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos">434</span></a><span class="sd">        The function trains a neural network for a specified number of epochs using a chosen</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos">435</span></a><span class="sd">        algorithm, reports the state of the model, plots the cost function, and computes the total run time.</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos">436</span></a><span class="sd">        </span>
</span><span id="L-437"><a href="#L-437"><span class="linenos">437</span></a><span class="sd">        :param num_epochs: The number of epochs, which is the number of times the neural network will be</span>
</span><span id="L-438"><a href="#L-438"><span class="linenos">438</span></a><span class="sd">        trained on the dataset</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos">439</span></a><span class="sd">        :param algorithm: Determines the learning algorithm to be used for training the neural network. </span>
</span><span id="L-440"><a href="#L-440"><span class="linenos">440</span></a><span class="sd">        The default value is `stochastic_descent`, but you can pass any other function that implements a </span>
</span><span id="L-441"><a href="#L-441"><span class="linenos">441</span></a><span class="sd">        different learning algorithm</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos">442</span></a><span class="sd">        :param rate: The learning rate of the neural network. It determines how quickly the parameters of the </span>
</span><span id="L-443"><a href="#L-443"><span class="linenos">443</span></a><span class="sd">        network are updated during training. A higher learning rate can result in faster convergence, but it </span>
</span><span id="L-444"><a href="#L-444"><span class="linenos">444</span></a><span class="sd">        may also cause the network to overshoot the optimal solution. Defaults to `1` (optional)</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos">445</span></a><span class="sd">        :param batch_ratio: Determines the ratio of the training data used in each iteration of the learning </span>
</span><span id="L-446"><a href="#L-446"><span class="linenos">446</span></a><span class="sd">        algorithm</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos">447</span></a><span class="sd">        :param threshold: Is used to determine when to stop the learning process. If the current cost of the </span>
</span><span id="L-448"><a href="#L-448"><span class="linenos">448</span></a><span class="sd">        model falls below the threshold value, the learning process will stop</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos">449</span></a><span class="sd">        :param stop: Determines whether the learning process should stop when the threshold cost is reached. </span>
</span><span id="L-450"><a href="#L-450"><span class="linenos">450</span></a><span class="sd">        If set to `True`, the learning process will stop when the cost falls below the specified threshold. </span>
</span><span id="L-451"><a href="#L-451"><span class="linenos">451</span></a><span class="sd">        Otherwise, the learning process will continue until all epochs are completed, regardless of the cost. </span>
</span><span id="L-452"><a href="#L-452"><span class="linenos">452</span></a><span class="sd">        Defaults to `True` (optional)</span>
</span><span id="L-453"><a href="#L-453"><span class="linenos">453</span></a><span class="sd">        :param plot_static: Determines whether to plot the graph of the cost function after training the neural </span>
</span><span id="L-454"><a href="#L-454"><span class="linenos">454</span></a><span class="sd">        network. If set to `True`, the graph will be plotted using the `static_plot` method. Defaults to `False` </span>
</span><span id="L-455"><a href="#L-455"><span class="linenos">455</span></a><span class="sd">        (optional)</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos">456</span></a><span class="sd">        :param plot_dynamic: Determines whether or not to plot the cost function in real-time during the learning </span>
</span><span id="L-457"><a href="#L-457"><span class="linenos">457</span></a><span class="sd">        process. If set to `True`, a separate process will be created to handle the plotting. Defaults to `False` </span>
</span><span id="L-458"><a href="#L-458"><span class="linenos">458</span></a><span class="sd">        (optional)</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos">459</span></a><span class="sd">        :param upd_interval: Determines the number of epochs after which the dynamic cost plot is updated during </span>
</span><span id="L-460"><a href="#L-460"><span class="linenos">460</span></a><span class="sd">        the learning process. Defaults to `20` (optional)</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos">461</span></a><span class="sd">        :param with_full_report: Determines whether to include a full report of the neural network&#39;s state after </span>
</span><span id="L-462"><a href="#L-462"><span class="linenos">462</span></a><span class="sd">        training. If set to `True`, the full report will be printed to the console. Otherwise, only a summary </span>
</span><span id="L-463"><a href="#L-463"><span class="linenos">463</span></a><span class="sd">        of the neural network&#39;s state will be printed. Defaults to `False` (optional)</span>
</span><span id="L-464"><a href="#L-464"><span class="linenos">464</span></a><span class="sd">        :param return_cost: Determines whether the function should return the list of cost values. If set to </span>
</span><span id="L-465"><a href="#L-465"><span class="linenos">465</span></a><span class="sd">        `True`, the function will return a list of cost values for each epoch. Defaults to `False` (optional)</span>
</span><span id="L-466"><a href="#L-466"><span class="linenos">466</span></a><span class="sd">        :return: If the `return_cost` parameter is set to `True`, it returns a list of cost values. Otherwise it </span>
</span><span id="L-467"><a href="#L-467"><span class="linenos">467</span></a><span class="sd">        returns `None`</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos">468</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-469"><a href="#L-469"><span class="linenos">469</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">plot_dynamic</span> <span class="ow">and</span> <span class="n">plot_static</span><span class="p">),</span> <span class="s2">&quot;Incorrect plotting mode setup&quot;</span>
</span><span id="L-470"><a href="#L-470"><span class="linenos">470</span></a>        <span class="c1"># initializes threshold flag</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos">471</span></a>        <span class="n">thresh_flag</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-472"><a href="#L-472"><span class="linenos">472</span></a>        <span class="c1"># sets the learning rate</span>
</span><span id="L-473"><a href="#L-473"><span class="linenos">473</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">rate</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos">474</span></a>
</span><span id="L-475"><a href="#L-475"><span class="linenos">475</span></a>        <span class="c1"># reports the initial state of the MLP</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos">476</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt;&lt;&lt; INITIAL STATE &gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos">477</span></a>        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">(</span><span class="n">with_full_report</span><span class="p">))</span>
</span><span id="L-478"><a href="#L-478"><span class="linenos">478</span></a>
</span><span id="L-479"><a href="#L-479"><span class="linenos">479</span></a>        <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="L-480"><a href="#L-480"><span class="linenos">480</span></a>            <span class="c1"># initializes table for cost/epoch</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos">481</span></a>            <span class="n">cost_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-482"><a href="#L-482"><span class="linenos">482</span></a>        <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>  
</span><span id="L-483"><a href="#L-483"><span class="linenos">483</span></a>            <span class="c1"># initializes lists for plotting</span>
</span><span id="L-484"><a href="#L-484"><span class="linenos">484</span></a>            <span class="n">x_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-485"><a href="#L-485"><span class="linenos">485</span></a>            <span class="n">y_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos">486</span></a>        <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos">487</span></a>            <span class="c1"># creates a queue for communication with the plotting process</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos">488</span></a>            <span class="n">queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">()</span>
</span><span id="L-489"><a href="#L-489"><span class="linenos">489</span></a>            <span class="c1"># creates the plotting process</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos">490</span></a>            <span class="n">plotting_process</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_plot</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">queue</span><span class="p">,))</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos">491</span></a>            <span class="n">plotting_process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span id="L-492"><a href="#L-492"><span class="linenos">492</span></a>
</span><span id="L-493"><a href="#L-493"><span class="linenos">493</span></a>        <span class="c1"># initializes the progress bar</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos">494</span></a>        <span class="n">EMPTY</span><span class="p">,</span> <span class="n">COMPLETE</span> <span class="o">=</span> <span class="s1">&#39; - &#39;</span><span class="p">,</span> <span class="s1">&#39; # &#39;</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos">495</span></a>        <span class="n">BAR_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos">496</span></a>        <span class="n">CURSOR_UP</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[1A&#39;</span>
</span><span id="L-497"><a href="#L-497"><span class="linenos">497</span></a>        <span class="n">CLEAR</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\x1b</span><span class="s1">[2K&#39;</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos">498</span></a>        <span class="n">CLEAR_LINE</span> <span class="o">=</span> <span class="n">CURSOR_UP</span> <span class="o">+</span> <span class="n">CLEAR</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos">499</span></a>        <span class="n">counter</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-500"><a href="#L-500"><span class="linenos">500</span></a>        <span class="n">ratio</span> <span class="o">=</span> <span class="n">num_epochs</span> <span class="o">/</span> <span class="n">BAR_LENGTH</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos">501</span></a>        <span class="n">progress</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">EMPTY</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">BAR_LENGTH</span><span class="si">}</span><span class="s2">]&quot;</span>
</span><span id="L-502"><a href="#L-502"><span class="linenos">502</span></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt;&lt;&lt; LEARNING PROGRESS &gt;&gt;&gt;</span><span class="se">\n</span><span class="si">{</span><span class="n">progress</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-503"><a href="#L-503"><span class="linenos">503</span></a>
</span><span id="L-504"><a href="#L-504"><span class="linenos">504</span></a>        <span class="n">start_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="L-505"><a href="#L-505"><span class="linenos">505</span></a>        <span class="c1"># repeats the learning procedure &lt;num_epochs&gt; times</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos">506</span></a>        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos">507</span></a>            <span class="c1"># executes the selected algorithm</span>
</span><span id="L-508"><a href="#L-508"><span class="linenos">508</span></a>            <span class="n">algorithm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="p">)</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos">509</span></a>            <span class="c1"># update the progress bar if needed</span>
</span><span id="L-510"><a href="#L-510"><span class="linenos">510</span></a>            <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">ratio</span><span class="o">*</span><span class="n">counter</span><span class="p">:</span>
</span><span id="L-511"><a href="#L-511"><span class="linenos">511</span></a>                <span class="nb">print</span><span class="p">(</span><span class="n">CLEAR_LINE</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos">512</span></a>                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">COMPLETE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">counter</span><span class="si">}{</span><span class="n">EMPTY</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">BAR_LENGTH</span><span class="o">-</span><span class="n">counter</span><span class="p">)</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</span><span id="L-513"><a href="#L-513"><span class="linenos">513</span></a>                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="L-514"><a href="#L-514"><span class="linenos">514</span></a>
</span><span id="L-515"><a href="#L-515"><span class="linenos">515</span></a>            <span class="c1"># gets the current cost of the model</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos">516</span></a>            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span>
</span><span id="L-517"><a href="#L-517"><span class="linenos">517</span></a>            <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos">518</span></a>                <span class="c1"># saves the values for the cost table</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos">519</span></a>                <span class="n">cost_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="L-520"><a href="#L-520"><span class="linenos">520</span></a>            <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>
</span><span id="L-521"><a href="#L-521"><span class="linenos">521</span></a>                <span class="c1"># saves the values for the plot</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos">522</span></a>                <span class="n">x_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-523"><a href="#L-523"><span class="linenos">523</span></a>                <span class="n">y_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="L-524"><a href="#L-524"><span class="linenos">524</span></a>            <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos">525</span></a>                <span class="c1"># updates the cost plot every &lt;upd_interval&gt; cycles</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos">526</span></a>                <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">upd_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-527"><a href="#L-527"><span class="linenos">527</span></a>                    <span class="c1"># sends data to the plotting process</span>
</span><span id="L-528"><a href="#L-528"><span class="linenos">528</span></a>                    <span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos">529</span></a>
</span><span id="L-530"><a href="#L-530"><span class="linenos">530</span></a>            <span class="c1"># detects the time when the threshold cost is reached</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos">531</span></a>            <span class="k">if</span> <span class="n">cost</span> <span class="o">&lt;</span> <span class="n">threshold</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">thresh_flag</span><span class="p">:</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos">532</span></a>                <span class="n">thresh_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos">533</span></a>                <span class="n">thresh_iter</span> <span class="o">=</span> <span class="n">epoch</span>
</span><span id="L-534"><a href="#L-534"><span class="linenos">534</span></a>                <span class="n">thresh_flag</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-535"><a href="#L-535"><span class="linenos">535</span></a>                <span class="c1"># stops learning</span>
</span><span id="L-536"><a href="#L-536"><span class="linenos">536</span></a>                <span class="k">if</span> <span class="n">stop</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos">537</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="n">CLEAR_LINE</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</span><span id="L-538"><a href="#L-538"><span class="linenos">538</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">COMPLETE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">BAR_LENGTH</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</span><span id="L-539"><a href="#L-539"><span class="linenos">539</span></a>                    <span class="k">break</span>
</span><span id="L-540"><a href="#L-540"><span class="linenos">540</span></a>        
</span><span id="L-541"><a href="#L-541"><span class="linenos">541</span></a>        <span class="n">end_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos">542</span></a>
</span><span id="L-543"><a href="#L-543"><span class="linenos">543</span></a>        <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="L-544"><a href="#L-544"><span class="linenos">544</span></a>            <span class="c1"># signals the plot process to terminate</span>
</span><span id="L-545"><a href="#L-545"><span class="linenos">545</span></a>            <span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos">546</span></a>
</span><span id="L-547"><a href="#L-547"><span class="linenos">547</span></a>        <span class="c1"># reports the final state of the model</span>
</span><span id="L-548"><a href="#L-548"><span class="linenos">548</span></a>        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">(</span><span class="n">with_full_report</span><span class="p">))</span>
</span><span id="L-549"><a href="#L-549"><span class="linenos">549</span></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Time elapsed: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">end_t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_t</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">s.&quot;</span><span class="p">)</span>
</span><span id="L-550"><a href="#L-550"><span class="linenos">550</span></a>        <span class="c1"># prints how much time was needed to pass the cost threshold</span>
</span><span id="L-551"><a href="#L-551"><span class="linenos">551</span></a>        <span class="k">if</span> <span class="n">thresh_flag</span><span class="p">:</span>
</span><span id="L-552"><a href="#L-552"><span class="linenos">552</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">threshold</span><span class="si">}</span><span class="s2">% of cost was reached in </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">thresh_t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_t</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">s (</span><span class="si">{</span><span class="n">thresh_iter</span><span class="si">}</span><span class="s2"> iterations).&quot;</span><span class="p">)</span>
</span><span id="L-553"><a href="#L-553"><span class="linenos">553</span></a>
</span><span id="L-554"><a href="#L-554"><span class="linenos">554</span></a>        <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>
</span><span id="L-555"><a href="#L-555"><span class="linenos">555</span></a>            <span class="c1"># plots the cost</span>
</span><span id="L-556"><a href="#L-556"><span class="linenos">556</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_static_plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
</span><span id="L-557"><a href="#L-557"><span class="linenos">557</span></a>
</span><span id="L-558"><a href="#L-558"><span class="linenos">558</span></a>        <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="L-559"><a href="#L-559"><span class="linenos">559</span></a>            <span class="k">return</span> <span class="n">cost_vals</span>
</span><span id="L-560"><a href="#L-560"><span class="linenos">560</span></a>
</span><span id="L-561"><a href="#L-561"><span class="linenos">561</span></a>    <span class="c1"># methods for plotting the cost</span>
</span><span id="L-562"><a href="#L-562"><span class="linenos">562</span></a>    <span class="k">def</span> <span class="nf">_static_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
</span><span id="L-563"><a href="#L-563"><span class="linenos">563</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="L-564"><a href="#L-564"><span class="linenos">564</span></a><span class="sd">        The function plots the cost of a model against the number of training iterations.</span>
</span><span id="L-565"><a href="#L-565"><span class="linenos">565</span></a><span class="sd">        </span>
</span><span id="L-566"><a href="#L-566"><span class="linenos">566</span></a><span class="sd">        :param x_vals: Is a list or array of values representing the number of learning iterations. </span>
</span><span id="L-567"><a href="#L-567"><span class="linenos">567</span></a><span class="sd">        These values will be plotted on the x-axis of the graph</span>
</span><span id="L-568"><a href="#L-568"><span class="linenos">568</span></a><span class="sd">        :param y_vals: Represents the values of the cost function for each corresponding value </span>
</span><span id="L-569"><a href="#L-569"><span class="linenos">569</span></a><span class="sd">        in the `x_vals` parameter. These values are used to plot the cost vs. number of training </span>
</span><span id="L-570"><a href="#L-570"><span class="linenos">570</span></a><span class="sd">        iterations</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos">571</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-572"><a href="#L-572"><span class="linenos">572</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</span><span id="L-573"><a href="#L-573"><span class="linenos">573</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of learning iterations&#39;</span><span class="p">)</span>
</span><span id="L-574"><a href="#L-574"><span class="linenos">574</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost of the model&#39;</span><span class="p">)</span>
</span><span id="L-575"><a href="#L-575"><span class="linenos">575</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-576"><a href="#L-576"><span class="linenos">576</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span id="L-577"><a href="#L-577"><span class="linenos">577</span></a>    <span class="k">def</span> <span class="nf">_dynamic_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span><span id="L-578"><a href="#L-578"><span class="linenos">578</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos">579</span></a><span class="sd">        The function plots the cost function over the number of learning iterations in real-time as the data comes in through </span>
</span><span id="L-580"><a href="#L-580"><span class="linenos">580</span></a><span class="sd">        a queue. It updates the plot with each new piece of data received through the queue, allowing for live visualization </span>
</span><span id="L-581"><a href="#L-581"><span class="linenos">581</span></a><span class="sd">        of the training process. The plot remains open and dynamically updates until a `None` value is received through the </span>
</span><span id="L-582"><a href="#L-582"><span class="linenos">582</span></a><span class="sd">        queue.</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos">583</span></a>
</span><span id="L-584"><a href="#L-584"><span class="linenos">584</span></a><span class="sd">        :param args: The first element of `args` is expected to be a queue that provides tuples of `(iteration, cost)`, where iteration </span>
</span><span id="L-585"><a href="#L-585"><span class="linenos">585</span></a><span class="sd">        is an integer representing the iteration number, and cost is a float representing the cost value at that iteration</span>
</span><span id="L-586"><a href="#L-586"><span class="linenos">586</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos">587</span></a>        <span class="c1"># gets the queue</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos">588</span></a>        <span class="n">queue</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-589"><a href="#L-589"><span class="linenos">589</span></a>        <span class="c1"># small constant for scaling of axes</span>
</span><span id="L-590"><a href="#L-590"><span class="linenos">590</span></a>        <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>
</span><span id="L-591"><a href="#L-591"><span class="linenos">591</span></a>        <span class="c1"># initializes the plot</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos">592</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span id="L-593"><a href="#L-593"><span class="linenos">593</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">)</span>
</span><span id="L-594"><a href="#L-594"><span class="linenos">594</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span>
</span><span id="L-595"><a href="#L-595"><span class="linenos">595</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-596"><a href="#L-596"><span class="linenos">596</span></a>        
</span><span id="L-597"><a href="#L-597"><span class="linenos">597</span></a>        <span class="n">x_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-598"><a href="#L-598"><span class="linenos">598</span></a>        <span class="n">y_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-599"><a href="#L-599"><span class="linenos">599</span></a>
</span><span id="L-600"><a href="#L-600"><span class="linenos">600</span></a>        <span class="c1"># creates the line object</span>
</span><span id="L-601"><a href="#L-601"><span class="linenos">601</span></a>        <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</span><span id="L-602"><a href="#L-602"><span class="linenos">602</span></a>        <span class="n">cost_text</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
</span><span id="L-603"><a href="#L-603"><span class="linenos">603</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos">604</span></a>
</span><span id="L-605"><a href="#L-605"><span class="linenos">605</span></a>        <span class="k">try</span><span class="p">:</span>
</span><span id="L-606"><a href="#L-606"><span class="linenos">606</span></a>            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span><span id="L-607"><a href="#L-607"><span class="linenos">607</span></a>                <span class="c1"># checks if there&#39;s data in the queue</span>
</span><span id="L-608"><a href="#L-608"><span class="linenos">608</span></a>                <span class="k">if</span> <span class="ow">not</span> <span class="n">queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
</span><span id="L-609"><a href="#L-609"><span class="linenos">609</span></a>                    <span class="c1"># gets values from the queue</span>
</span><span id="L-610"><a href="#L-610"><span class="linenos">610</span></a>                    <span class="n">data</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span><span id="L-611"><a href="#L-611"><span class="linenos">611</span></a>                    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-612"><a href="#L-612"><span class="linenos">612</span></a>                        <span class="k">break</span>
</span><span id="L-613"><a href="#L-613"><span class="linenos">613</span></a>                    
</span><span id="L-614"><a href="#L-614"><span class="linenos">614</span></a>                    <span class="c1"># stores the values for plotting</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos">615</span></a>                    <span class="n">iteration</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">data</span>
</span><span id="L-616"><a href="#L-616"><span class="linenos">616</span></a>                    <span class="n">x_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
</span><span id="L-617"><a href="#L-617"><span class="linenos">617</span></a>                    <span class="n">y_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="L-618"><a href="#L-618"><span class="linenos">618</span></a>
</span><span id="L-619"><a href="#L-619"><span class="linenos">619</span></a>                    <span class="c1"># updates the xdata and ydata of the line</span>
</span><span id="L-620"><a href="#L-620"><span class="linenos">620</span></a>                    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
</span><span id="L-621"><a href="#L-621"><span class="linenos">621</span></a>
</span><span id="L-622"><a href="#L-622"><span class="linenos">622</span></a>                    <span class="c1"># sets appropriate plot limits</span>
</span><span id="L-623"><a href="#L-623"><span class="linenos">623</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span><span class="o">-</span><span class="n">eps</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span>
</span><span id="L-624"><a href="#L-624"><span class="linenos">624</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y_vals</span><span class="p">)</span><span class="o">-</span><span class="n">eps</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_vals</span><span class="p">)</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span>
</span><span id="L-625"><a href="#L-625"><span class="linenos">625</span></a>
</span><span id="L-626"><a href="#L-626"><span class="linenos">626</span></a>                    <span class="c1"># updates the cost text with the current value of y</span>
</span><span id="L-627"><a href="#L-627"><span class="linenos">627</span></a>                    <span class="n">cost_text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Current accuracy: </span><span class="si">{</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">cost</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</span><span id="L-628"><a href="#L-628"><span class="linenos">628</span></a>
</span><span id="L-629"><a href="#L-629"><span class="linenos">629</span></a>                    <span class="c1"># draws the updated plot</span>
</span><span id="L-630"><a href="#L-630"><span class="linenos">630</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</span><span id="L-631"><a href="#L-631"><span class="linenos">631</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># pauses for a short duration to display the plot</span>
</span><span id="L-632"><a href="#L-632"><span class="linenos">632</span></a>
</span><span id="L-633"><a href="#L-633"><span class="linenos">633</span></a>        <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
</span><span id="L-634"><a href="#L-634"><span class="linenos">634</span></a>            <span class="k">pass</span>
</span><span id="L-635"><a href="#L-635"><span class="linenos">635</span></a>        
</span><span id="L-636"><a href="#L-636"><span class="linenos">636</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span id="L-637"><a href="#L-637"><span class="linenos">637</span></a>
</span><span id="L-638"><a href="#L-638"><span class="linenos">638</span></a>    <span class="c1"># methods for storing a MLP object state in memory</span>
</span><span id="L-639"><a href="#L-639"><span class="linenos">639</span></a>    <span class="k">def</span> <span class="nf">export_mlp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">):</span>
</span><span id="L-640"><a href="#L-640"><span class="linenos">640</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-641"><a href="#L-641"><span class="linenos">641</span></a><span class="sd">        The function creates a dictionary, that contains the current parameters and activation</span>
</span><span id="L-642"><a href="#L-642"><span class="linenos">642</span></a><span class="sd">        functions of a neural network, and saves it to a file, which can be later used by the method </span>
</span><span id="L-643"><a href="#L-643"><span class="linenos">643</span></a><span class="sd">        `import_mlp` to restore the state of the neural network.</span>
</span><span id="L-644"><a href="#L-644"><span class="linenos">644</span></a><span class="sd">        </span>
</span><span id="L-645"><a href="#L-645"><span class="linenos">645</span></a><span class="sd">        :param data_path: Is a string that represents the path where the neural network image will be saved</span>
</span><span id="L-646"><a href="#L-646"><span class="linenos">646</span></a><span class="sd">        as a numpy file</span>
</span><span id="L-647"><a href="#L-647"><span class="linenos">647</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-648"><a href="#L-648"><span class="linenos">648</span></a>        <span class="n">mlp_state</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="L-649"><a href="#L-649"><span class="linenos">649</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="L-650"><a href="#L-650"><span class="linenos">650</span></a>            <span class="n">act_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>
</span><span id="L-651"><a href="#L-651"><span class="linenos">651</span></a>            <span class="n">mlp_state</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">act_func</span><span class="p">)</span>
</span><span id="L-652"><a href="#L-652"><span class="linenos">652</span></a>
</span><span id="L-653"><a href="#L-653"><span class="linenos">653</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">mlp_state</span><span class="p">)</span>
</span><span id="L-654"><a href="#L-654"><span class="linenos">654</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The image was successfully exported to &quot;</span> <span class="o">+</span> <span class="n">data_path</span><span class="p">)</span>
</span><span id="L-655"><a href="#L-655"><span class="linenos">655</span></a>    <span class="k">def</span> <span class="nf">import_mlp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">):</span>
</span><span id="L-656"><a href="#L-656"><span class="linenos">656</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-657"><a href="#L-657"><span class="linenos">657</span></a><span class="sd">        The function imports new parameters for a neural network and updates the corresponding class instance </span>
</span><span id="L-658"><a href="#L-658"><span class="linenos">658</span></a><span class="sd">        attributes. It collaborates with the method `export_mlp` to streamline the storage of neural networks.</span>
</span><span id="L-659"><a href="#L-659"><span class="linenos">659</span></a><span class="sd">        </span>
</span><span id="L-660"><a href="#L-660"><span class="linenos">660</span></a><span class="sd">        :param data_path: Is a string that represents the path to the file from which the new parameters for </span>
</span><span id="L-661"><a href="#L-661"><span class="linenos">661</span></a><span class="sd">        the neural network will be imported</span>
</span><span id="L-662"><a href="#L-662"><span class="linenos">662</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-663"><a href="#L-663"><span class="linenos">663</span></a>        <span class="c1"># read new parameters from the file</span>
</span><span id="L-664"><a href="#L-664"><span class="linenos">664</span></a>        <span class="n">new_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="s1">&#39;TRUE&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="L-665"><a href="#L-665"><span class="linenos">665</span></a>        <span class="c1"># creates temp dicts to store new configuration</span>
</span><span id="L-666"><a href="#L-666"><span class="linenos">666</span></a>        <span class="n">act_funcs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="L-667"><a href="#L-667"><span class="linenos">667</span></a>        <span class="n">new_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="L-668"><a href="#L-668"><span class="linenos">668</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">new_state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="L-669"><a href="#L-669"><span class="linenos">669</span></a>            <span class="c1"># checks if there is act func</span>
</span><span id="L-670"><a href="#L-670"><span class="linenos">670</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="L-671"><a href="#L-671"><span class="linenos">671</span></a>                <span class="n">new_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-672"><a href="#L-672"><span class="linenos">672</span></a>                <span class="c1"># retrieves act func name</span>
</span><span id="L-673"><a href="#L-673"><span class="linenos">673</span></a>                <span class="n">act_funcs</span> <span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-674"><a href="#L-674"><span class="linenos">674</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-675"><a href="#L-675"><span class="linenos">675</span></a>                <span class="n">new_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="L-676"><a href="#L-676"><span class="linenos">676</span></a>                <span class="c1"># assigns the default act func</span>
</span><span id="L-677"><a href="#L-677"><span class="linenos">677</span></a>                <span class="n">act_funcs</span> <span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span>
</span><span id="L-678"><a href="#L-678"><span class="linenos">678</span></a>        <span class="c1"># updated weights and biases</span>
</span><span id="L-679"><a href="#L-679"><span class="linenos">679</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">new_params</span><span class="p">)</span>
</span><span id="L-680"><a href="#L-680"><span class="linenos">680</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span>
</span><span id="L-681"><a href="#L-681"><span class="linenos">681</span></a>        <span class="c1"># maps layers to act funcs</span>
</span><span id="L-682"><a href="#L-682"><span class="linenos">682</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_func_map</span><span class="p">(</span><span class="n">act_funcs</span><span class="p">)</span>
</span><span id="L-683"><a href="#L-683"><span class="linenos">683</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The image was successfully imported from &quot;</span> <span class="o">+</span> <span class="n">data_path</span><span class="p">)</span>
</span><span id="L-684"><a href="#L-684"><span class="linenos">684</span></a>
</span><span id="L-685"><a href="#L-685"><span class="linenos">685</span></a>    <span class="c1"># LEGACY METHODS (can be used but are outperformed by the current algorithms)</span>
</span><span id="L-686"><a href="#L-686"><span class="linenos">686</span></a>    <span class="k">def</span> <span class="nf">_legacy_finite_diff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
</span><span id="L-687"><a href="#L-687"><span class="linenos">687</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-688"><a href="#L-688"><span class="linenos">688</span></a><span class="sd">        The function uses the finite difference method to approximate the partial derivatives of each </span>
</span><span id="L-689"><a href="#L-689"><span class="linenos">689</span></a><span class="sd">        parameter and construct the gradient of the cost function, and then updates the parameter</span>
</span><span id="L-690"><a href="#L-690"><span class="linenos">690</span></a><span class="sd">        values accordingly.</span>
</span><span id="L-691"><a href="#L-691"><span class="linenos">691</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-692"><a href="#L-692"><span class="linenos">692</span></a>        <span class="c1"># computes the initial cost of the model</span>
</span><span id="L-693"><a href="#L-693"><span class="linenos">693</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">()</span>
</span><span id="L-694"><a href="#L-694"><span class="linenos">694</span></a>        <span class="c1"># dictionary that will store </span>
</span><span id="L-695"><a href="#L-695"><span class="linenos">695</span></a>        <span class="c1"># updated parameter values</span>
</span><span id="L-696"><a href="#L-696"><span class="linenos">696</span></a>        <span class="n">upd_vals</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="L-697"><a href="#L-697"><span class="linenos">697</span></a>        <span class="c1"># iterates through each layer of parameters</span>
</span><span id="L-698"><a href="#L-698"><span class="linenos">698</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
</span><span id="L-699"><a href="#L-699"><span class="linenos">699</span></a>            <span class="n">mx_lst</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-700"><a href="#L-700"><span class="linenos">700</span></a>            <span class="k">for</span> <span class="n">mx_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span><span id="L-701"><a href="#L-701"><span class="linenos">701</span></a>                <span class="n">param_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="n">mx_idx</span><span class="p">]</span>
</span><span id="L-702"><a href="#L-702"><span class="linenos">702</span></a>                <span class="c1"># creates a copy of the current values</span>
</span><span id="L-703"><a href="#L-703"><span class="linenos">703</span></a>                <span class="n">new_vals</span> <span class="o">=</span> <span class="n">param_vals</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="L-704"><a href="#L-704"><span class="linenos">704</span></a>                <span class="c1"># iterates through each parameter value</span>
</span><span id="L-705"><a href="#L-705"><span class="linenos">705</span></a>                <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">([</span><span class="n">param_vals</span><span class="p">,</span> <span class="n">new_vals</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span> <span class="k">as</span> <span class="n">it</span><span class="p">:</span>
</span><span id="L-706"><a href="#L-706"><span class="linenos">706</span></a>                    <span class="k">for</span> <span class="n">param_val</span><span class="p">,</span> <span class="n">new_val</span> <span class="ow">in</span> <span class="n">it</span><span class="p">:</span>
</span><span id="L-707"><a href="#L-707"><span class="linenos">707</span></a>                        <span class="c1"># saves the original value</span>
</span><span id="L-708"><a href="#L-708"><span class="linenos">708</span></a>                        <span class="n">saved</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">param_val</span><span class="p">)</span>
</span><span id="L-709"><a href="#L-709"><span class="linenos">709</span></a>                        <span class="c1"># tweaks the parameter by &lt;_eps&gt;</span>
</span><span id="L-710"><a href="#L-710"><span class="linenos">710</span></a>                        <span class="n">param_val</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span>
</span><span id="L-711"><a href="#L-711"><span class="linenos">711</span></a>                        <span class="c1"># computes the derivate of the cost function</span>
</span><span id="L-712"><a href="#L-712"><span class="linenos">712</span></a>                        <span class="n">grad_val</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span>
</span><span id="L-713"><a href="#L-713"><span class="linenos">713</span></a>                        <span class="c1"># updates the parameter value (in the copy)</span>
</span><span id="L-714"><a href="#L-714"><span class="linenos">714</span></a>                        <span class="n">new_val</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="n">grad_val</span>
</span><span id="L-715"><a href="#L-715"><span class="linenos">715</span></a>                        <span class="c1"># restores the original value</span>
</span><span id="L-716"><a href="#L-716"><span class="linenos">716</span></a>                        <span class="n">param_val</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved</span>
</span><span id="L-717"><a href="#L-717"><span class="linenos">717</span></a>                <span class="c1"># adds the matrix with updated values to the list-layer</span>
</span><span id="L-718"><a href="#L-718"><span class="linenos">718</span></a>                <span class="n">mx_lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_vals</span><span class="p">)</span>
</span><span id="L-719"><a href="#L-719"><span class="linenos">719</span></a>            <span class="c1"># adds the layer to the final updated dictionary</span>
</span><span id="L-720"><a href="#L-720"><span class="linenos">720</span></a>            <span class="n">upd_vals</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">mx_lst</span>
</span><span id="L-721"><a href="#L-721"><span class="linenos">721</span></a>        <span class="c1"># the updated dictionary becomes the new parameter dictionary</span>
</span><span id="L-722"><a href="#L-722"><span class="linenos">722</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="n">upd_vals</span>
</span><span id="L-723"><a href="#L-723"><span class="linenos">723</span></a>    <span class="k">def</span> <span class="nf">_legacy_gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
</span><span id="L-724"><a href="#L-724"><span class="linenos">724</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-725"><a href="#L-725"><span class="linenos">725</span></a><span class="sd">        The function implements a backpropagation algorithm for training a neural network by iterating </span>
</span><span id="L-726"><a href="#L-726"><span class="linenos">726</span></a><span class="sd">        through each layer and instantly updating the weights and biases based on the computed derivatives.</span>
</span><span id="L-727"><a href="#L-727"><span class="linenos">727</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-728"><a href="#L-728"><span class="linenos">728</span></a>        <span class="n">total_err</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-729"><a href="#L-729"><span class="linenos">729</span></a>        <span class="c1"># iterates through each data sample</span>
</span><span id="L-730"><a href="#L-730"><span class="linenos">730</span></a>        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="L-731"><a href="#L-731"><span class="linenos">731</span></a>            <span class="c1"># splits training data array </span>
</span><span id="L-732"><a href="#L-732"><span class="linenos">732</span></a>            <span class="c1"># into input/expected output subarrays</span>
</span><span id="L-733"><a href="#L-733"><span class="linenos">733</span></a>            <span class="nb">input</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="L-734"><a href="#L-734"><span class="linenos">734</span></a>            <span class="n">expected</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">:</span> <span class="p">]])</span>
</span><span id="L-735"><a href="#L-735"><span class="linenos">735</span></a>
</span><span id="L-736"><a href="#L-736"><span class="linenos">736</span></a>            <span class="c1"># computes activations for each layer</span>
</span><span id="L-737"><a href="#L-737"><span class="linenos">737</span></a>            <span class="c1"># and saves the last layer activations</span>
</span><span id="L-738"><a href="#L-738"><span class="linenos">738</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="L-739"><a href="#L-739"><span class="linenos">739</span></a>
</span><span id="L-740"><a href="#L-740"><span class="linenos">740</span></a>            <span class="c1"># verifies that there is no mismatch between </span>
</span><span id="L-741"><a href="#L-741"><span class="linenos">741</span></a>            <span class="c1"># the sizes of the expected and actual output arrays</span>
</span><span id="L-742"><a href="#L-742"><span class="linenos">742</span></a>            <span class="k">assert</span> <span class="n">expected</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-743"><a href="#L-743"><span class="linenos">743</span></a>
</span><span id="L-744"><a href="#L-744"><span class="linenos">744</span></a>            <span class="c1"># initializes a list that holds partial derivatives of</span>
</span><span id="L-745"><a href="#L-745"><span class="linenos">745</span></a>            <span class="c1"># activations of the current layer to the cost of the next layer</span>
</span><span id="L-746"><a href="#L-746"><span class="linenos">746</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-747"><a href="#L-747"><span class="linenos">747</span></a>            <span class="c1"># computes activation derivative values for the last layer</span>
</span><span id="L-748"><a href="#L-748"><span class="linenos">748</span></a>            <span class="c1"># as a difference between expected and actual outputs of MLP</span>
</span><span id="L-749"><a href="#L-749"><span class="linenos">749</span></a>            <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">expected</span><span class="p">])</span> <span class="k">as</span> <span class="n">it</span><span class="p">:</span>
</span><span id="L-750"><a href="#L-750"><span class="linenos">750</span></a>                <span class="k">for</span> <span class="n">out_val</span><span class="p">,</span> <span class="n">exp_val</span> <span class="ow">in</span> <span class="n">it</span><span class="p">:</span>
</span><span id="L-751"><a href="#L-751"><span class="linenos">751</span></a>                    <span class="n">da_next</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_val</span> <span class="o">-</span> <span class="n">exp_val</span><span class="p">)</span>
</span><span id="L-752"><a href="#L-752"><span class="linenos">752</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">da_next</span><span class="p">)</span>
</span><span id="L-753"><a href="#L-753"><span class="linenos">753</span></a>            
</span><span id="L-754"><a href="#L-754"><span class="linenos">754</span></a>            <span class="c1"># sums up error values of each activation in the last layer</span>
</span><span id="L-755"><a href="#L-755"><span class="linenos">755</span></a>            <span class="k">for</span> <span class="n">err_val</span> <span class="ow">in</span> <span class="n">da_next</span><span class="p">:</span>
</span><span id="L-756"><a href="#L-756"><span class="linenos">756</span></a>                <span class="n">total_err</span> <span class="o">+=</span> <span class="n">err_val</span> <span class="o">**</span> <span class="mi">2</span>
</span><span id="L-757"><a href="#L-757"><span class="linenos">757</span></a>        
</span><span id="L-758"><a href="#L-758"><span class="linenos">758</span></a>            <span class="c1"># iterates in reverse through each layer</span>
</span><span id="L-759"><a href="#L-759"><span class="linenos">759</span></a>            <span class="c1"># starting from the last one</span>
</span><span id="L-760"><a href="#L-760"><span class="linenos">760</span></a>            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
</span><span id="L-761"><a href="#L-761"><span class="linenos">761</span></a>                <span class="n">db_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-762"><a href="#L-762"><span class="linenos">762</span></a>                <span class="n">dw_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-763"><a href="#L-763"><span class="linenos">763</span></a>                <span class="n">da_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-764"><a href="#L-764"><span class="linenos">764</span></a>                <span class="c1"># gets the biases for the current layer</span>
</span><span id="L-765"><a href="#L-765"><span class="linenos">765</span></a>                <span class="n">layer_bss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-766"><a href="#L-766"><span class="linenos">766</span></a>                <span class="c1"># gets the weights for the current layer</span>
</span><span id="L-767"><a href="#L-767"><span class="linenos">767</span></a>                <span class="n">layer_wghts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-768"><a href="#L-768"><span class="linenos">768</span></a>                <span class="c1"># iterates through each activation</span>
</span><span id="L-769"><a href="#L-769"><span class="linenos">769</span></a>                <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="L-770"><a href="#L-770"><span class="linenos">770</span></a>                    <span class="n">der_expr</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">da_next</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">act</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">act</span><span class="p">)</span>
</span><span id="L-771"><a href="#L-771"><span class="linenos">771</span></a>                    <span class="c1"># computes bias derivative</span>
</span><span id="L-772"><a href="#L-772"><span class="linenos">772</span></a>                    <span class="n">db</span> <span class="o">=</span> <span class="n">der_expr</span>
</span><span id="L-773"><a href="#L-773"><span class="linenos">773</span></a>                    <span class="n">db_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
</span><span id="L-774"><a href="#L-774"><span class="linenos">774</span></a>                    <span class="c1"># computes weights derivatives</span>
</span><span id="L-775"><a href="#L-775"><span class="linenos">775</span></a>                    <span class="n">dw_act</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-776"><a href="#L-776"><span class="linenos">776</span></a>                    <span class="k">for</span> <span class="n">prev_act</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-777"><a href="#L-777"><span class="linenos">777</span></a>                        <span class="n">dw</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">*</span> <span class="n">prev_act</span>
</span><span id="L-778"><a href="#L-778"><span class="linenos">778</span></a>                        <span class="n">dw_act</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dw</span><span class="p">)</span>
</span><span id="L-779"><a href="#L-779"><span class="linenos">779</span></a>                    <span class="n">dw_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dw_act</span><span class="p">)</span>
</span><span id="L-780"><a href="#L-780"><span class="linenos">780</span></a>                    <span class="c1"># computes activation derivatives for the previous layer</span>
</span><span id="L-781"><a href="#L-781"><span class="linenos">781</span></a>                    <span class="n">da_act</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-782"><a href="#L-782"><span class="linenos">782</span></a>                    <span class="n">curr_wghts</span> <span class="o">=</span> <span class="n">layer_wghts</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
</span><span id="L-783"><a href="#L-783"><span class="linenos">783</span></a>                    <span class="k">for</span> <span class="n">wght</span> <span class="ow">in</span> <span class="n">curr_wghts</span><span class="p">:</span>
</span><span id="L-784"><a href="#L-784"><span class="linenos">784</span></a>                        <span class="n">da</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">*</span> <span class="n">wght</span>
</span><span id="L-785"><a href="#L-785"><span class="linenos">785</span></a>                        <span class="n">da_act</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da</span><span class="p">)</span>
</span><span id="L-786"><a href="#L-786"><span class="linenos">786</span></a>                    <span class="n">da_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da_act</span><span class="p">)</span>
</span><span id="L-787"><a href="#L-787"><span class="linenos">787</span></a>
</span><span id="L-788"><a href="#L-788"><span class="linenos">788</span></a>                <span class="c1"># APPLYING THE GRADIENT</span>
</span><span id="L-789"><a href="#L-789"><span class="linenos">789</span></a>                <span class="c1"># substracts bias gradient</span>
</span><span id="L-790"><a href="#L-790"><span class="linenos">790</span></a>                <span class="n">bs_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">db_layer</span><span class="p">)</span>
</span><span id="L-791"><a href="#L-791"><span class="linenos">791</span></a>                <span class="n">layer_bss</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="n">bs_grad</span>
</span><span id="L-792"><a href="#L-792"><span class="linenos">792</span></a>                <span class="c1"># substracts weight gradient</span>
</span><span id="L-793"><a href="#L-793"><span class="linenos">793</span></a>                <span class="n">wght_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dw_layer</span><span class="p">)</span>
</span><span id="L-794"><a href="#L-794"><span class="linenos">794</span></a>                <span class="n">wght_grad</span> <span class="o">=</span> <span class="n">wght_grad</span><span class="o">.</span><span class="n">T</span>
</span><span id="L-795"><a href="#L-795"><span class="linenos">795</span></a>                <span class="n">layer_wghts</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="n">wght_grad</span>
</span><span id="L-796"><a href="#L-796"><span class="linenos">796</span></a>                
</span><span id="L-797"><a href="#L-797"><span class="linenos">797</span></a>                <span class="c1"># computes a sum of activation derivatives</span>
</span><span id="L-798"><a href="#L-798"><span class="linenos">798</span></a>                <span class="c1"># for each activation of the previous layer</span>
</span><span id="L-799"><a href="#L-799"><span class="linenos">799</span></a>                <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">da_layer</span><span class="p">)</span>
</span><span id="L-800"><a href="#L-800"><span class="linenos">800</span></a>                <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">da_next</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-801"><a href="#L-801"><span class="linenos">801</span></a>
</span><span id="L-802"><a href="#L-802"><span class="linenos">802</span></a>        <span class="c1"># updates the current cost of the model </span>
</span><span id="L-803"><a href="#L-803"><span class="linenos">803</span></a>        <span class="c1"># as an average error value per 1 sample</span>
</span><span id="L-804"><a href="#L-804"><span class="linenos">804</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="n">total_err</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>
</span><span id="L-805"><a href="#L-805"><span class="linenos">805</span></a>    <span class="k">def</span> <span class="nf">_legacy_backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">da_next</span><span class="p">):</span>
</span><span id="L-806"><a href="#L-806"><span class="linenos">806</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-807"><a href="#L-807"><span class="linenos">807</span></a><span class="sd">        The function implements backpropagation by iterating through the layers in reverse order, computing</span>
</span><span id="L-808"><a href="#L-808"><span class="linenos">808</span></a><span class="sd">        the derivatives of the biases and weights, and updating the gradient for later use in updating the</span>
</span><span id="L-809"><a href="#L-809"><span class="linenos">809</span></a><span class="sd">        parameters of the neural network. On average, this backpropagation algorithm reaches 1% of cost 30 </span>
</span><span id="L-810"><a href="#L-810"><span class="linenos">810</span></a><span class="sd">        times faster than the &#39;_legacy_finite_diff&#39; method.</span>
</span><span id="L-811"><a href="#L-811"><span class="linenos">811</span></a><span class="sd">        </span>
</span><span id="L-812"><a href="#L-812"><span class="linenos">812</span></a><span class="sd">        :param da_next: An array of computed error values for the current output</span>
</span><span id="L-813"><a href="#L-813"><span class="linenos">813</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-814"><a href="#L-814"><span class="linenos">814</span></a>        <span class="c1"># iterates through each layer backwards starting from the last one</span>
</span><span id="L-815"><a href="#L-815"><span class="linenos">815</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
</span><span id="L-816"><a href="#L-816"><span class="linenos">816</span></a>            <span class="n">db_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-817"><a href="#L-817"><span class="linenos">817</span></a>            <span class="n">dw_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-818"><a href="#L-818"><span class="linenos">818</span></a>            <span class="n">da_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-819"><a href="#L-819"><span class="linenos">819</span></a>            <span class="c1"># gets the weights for the current layer</span>
</span><span id="L-820"><a href="#L-820"><span class="linenos">820</span></a>            <span class="n">layer_wghts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-821"><a href="#L-821"><span class="linenos">821</span></a>            <span class="c1"># iterates through each activation</span>
</span><span id="L-822"><a href="#L-822"><span class="linenos">822</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="L-823"><a href="#L-823"><span class="linenos">823</span></a>                <span class="n">der_expr</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">da_next</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">act</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">act</span><span class="p">)</span>
</span><span id="L-824"><a href="#L-824"><span class="linenos">824</span></a>                <span class="c1"># computes bias derivative</span>
</span><span id="L-825"><a href="#L-825"><span class="linenos">825</span></a>                <span class="n">db</span> <span class="o">=</span> <span class="n">der_expr</span>
</span><span id="L-826"><a href="#L-826"><span class="linenos">826</span></a>                <span class="n">db_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
</span><span id="L-827"><a href="#L-827"><span class="linenos">827</span></a>                <span class="c1"># computes weights derivatives</span>
</span><span id="L-828"><a href="#L-828"><span class="linenos">828</span></a>                <span class="n">dw_act</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-829"><a href="#L-829"><span class="linenos">829</span></a>                <span class="k">for</span> <span class="n">prev_act</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-830"><a href="#L-830"><span class="linenos">830</span></a>                    <span class="n">dw</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">*</span> <span class="n">prev_act</span>
</span><span id="L-831"><a href="#L-831"><span class="linenos">831</span></a>                    <span class="n">dw_act</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dw</span><span class="p">)</span>
</span><span id="L-832"><a href="#L-832"><span class="linenos">832</span></a>                <span class="n">dw_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dw_act</span><span class="p">)</span>
</span><span id="L-833"><a href="#L-833"><span class="linenos">833</span></a>                <span class="c1"># computes activation derivatives for the previous layer</span>
</span><span id="L-834"><a href="#L-834"><span class="linenos">834</span></a>                <span class="n">da_act</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-835"><a href="#L-835"><span class="linenos">835</span></a>                <span class="n">curr_wghts</span> <span class="o">=</span> <span class="n">layer_wghts</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
</span><span id="L-836"><a href="#L-836"><span class="linenos">836</span></a>                <span class="k">for</span> <span class="n">wght</span> <span class="ow">in</span> <span class="n">curr_wghts</span><span class="p">:</span>
</span><span id="L-837"><a href="#L-837"><span class="linenos">837</span></a>                    <span class="n">da</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">*</span> <span class="n">wght</span>
</span><span id="L-838"><a href="#L-838"><span class="linenos">838</span></a>                    <span class="n">da_act</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da</span><span class="p">)</span>
</span><span id="L-839"><a href="#L-839"><span class="linenos">839</span></a>                <span class="n">da_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da_act</span><span class="p">)</span>
</span><span id="L-840"><a href="#L-840"><span class="linenos">840</span></a>
</span><span id="L-841"><a href="#L-841"><span class="linenos">841</span></a>            <span class="c1"># computes the gradient for current layer</span>
</span><span id="L-842"><a href="#L-842"><span class="linenos">842</span></a>            <span class="n">b_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">db_layer</span><span class="p">)</span>                
</span><span id="L-843"><a href="#L-843"><span class="linenos">843</span></a>            <span class="n">w_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dw_layer</span><span class="p">)</span>
</span><span id="L-844"><a href="#L-844"><span class="linenos">844</span></a>            <span class="n">w_grad</span> <span class="o">=</span> <span class="n">w_grad</span><span class="o">.</span><span class="n">T</span>
</span><span id="L-845"><a href="#L-845"><span class="linenos">845</span></a>            <span class="n">grad_mxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_grad</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">]</span>
</span><span id="L-846"><a href="#L-846"><span class="linenos">846</span></a>            <span class="c1"># adds up the gradient values of the current sample</span>
</span><span id="L-847"><a href="#L-847"><span class="linenos">847</span></a>            <span class="c1"># to the gradient sum of previously computed samples</span>
</span><span id="L-848"><a href="#L-848"><span class="linenos">848</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">]):</span>
</span><span id="L-849"><a href="#L-849"><span class="linenos">849</span></a>                <span class="n">mx</span> <span class="o">+=</span> <span class="n">grad_mxs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span><span id="L-850"><a href="#L-850"><span class="linenos">850</span></a>            
</span><span id="L-851"><a href="#L-851"><span class="linenos">851</span></a>            <span class="c1"># computes a sum of activation derivatives</span>
</span><span id="L-852"><a href="#L-852"><span class="linenos">852</span></a>            <span class="c1"># for each activation of the previous layer</span>
</span><span id="L-853"><a href="#L-853"><span class="linenos">853</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">da_layer</span><span class="p">)</span>
</span><span id="L-854"><a href="#L-854"><span class="linenos">854</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">da_next</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-855"><a href="#L-855"><span class="linenos">855</span></a>
</span><span id="L-856"><a href="#L-856"><span class="linenos">856</span></a><span class="k">def</span> <span class="nf">adder_truth_table</span><span class="p">(</span><span class="n">num_bits</span><span class="p">):</span>
</span><span id="L-857"><a href="#L-857"><span class="linenos">857</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot; @private</span>
</span><span id="L-858"><a href="#L-858"><span class="linenos">858</span></a><span class="sd">    The function generates a truth table for a given number of bits for a binary adder.</span>
</span><span id="L-859"><a href="#L-859"><span class="linenos">859</span></a><span class="sd">    </span>
</span><span id="L-860"><a href="#L-860"><span class="linenos">860</span></a><span class="sd">    :param num_bits: Represents the number of bits in the adder. It determines the size of the truth </span>
</span><span id="L-861"><a href="#L-861"><span class="linenos">861</span></a><span class="sd">    table that will be generated</span>
</span><span id="L-862"><a href="#L-862"><span class="linenos">862</span></a><span class="sd">    :return: a numpy array that represents the truth table for a `num_bits` adder.</span>
</span><span id="L-863"><a href="#L-863"><span class="linenos">863</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-864"><a href="#L-864"><span class="linenos">864</span></a>    <span class="n">truth_table</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-865"><a href="#L-865"><span class="linenos">865</span></a>    <span class="n">decimal_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">num_bits</span><span class="p">)</span>
</span><span id="L-866"><a href="#L-866"><span class="linenos">866</span></a>    <span class="n">all_pairs</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations_with_replacement</span><span class="p">(</span><span class="n">decimal_range</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-867"><a href="#L-867"><span class="linenos">867</span></a>    <span class="k">for</span> <span class="n">a_pair</span> <span class="ow">in</span> <span class="n">all_pairs</span><span class="p">:</span>
</span><span id="L-868"><a href="#L-868"><span class="linenos">868</span></a>        <span class="n">a</span> <span class="o">=</span> <span class="n">a_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-869"><a href="#L-869"><span class="linenos">869</span></a>        <span class="n">b</span> <span class="o">=</span> <span class="n">a_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-870"><a href="#L-870"><span class="linenos">870</span></a>        <span class="n">a_sum</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</span><span id="L-871"><a href="#L-871"><span class="linenos">871</span></a>        <span class="n">a_str</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="n">num_bits</span><span class="p">)</span>
</span><span id="L-872"><a href="#L-872"><span class="linenos">872</span></a>        <span class="n">b_str</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="n">b</span><span class="p">)[</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="n">num_bits</span><span class="p">)</span>
</span><span id="L-873"><a href="#L-873"><span class="linenos">873</span></a>        <span class="n">sum_str</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="n">a_sum</span><span class="p">)[</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="n">num_bits</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-874"><a href="#L-874"><span class="linenos">874</span></a>        <span class="n">sample</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">char</span><span class="p">)</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="p">(</span><span class="n">a_str</span> <span class="o">+</span> <span class="n">b_str</span> <span class="o">+</span> <span class="n">sum_str</span><span class="p">))</span>
</span><span id="L-875"><a href="#L-875"><span class="linenos">875</span></a>        <span class="n">truth_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</span><span id="L-876"><a href="#L-876"><span class="linenos">876</span></a>
</span><span id="L-877"><a href="#L-877"><span class="linenos">877</span></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">truth_table</span><span class="p">)</span>
</span><span id="L-878"><a href="#L-878"><span class="linenos">878</span></a>
</span><span id="L-879"><a href="#L-879"><span class="linenos">879</span></a><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span><span id="L-880"><a href="#L-880"><span class="linenos">880</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-881"><a href="#L-881"><span class="linenos">881</span></a><span class="sd">    The main function is used to test the neural network implementation.</span>
</span><span id="L-882"><a href="#L-882"><span class="linenos">882</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-883"><a href="#L-883"><span class="linenos">883</span></a>    <span class="n">multiprocessing</span><span class="o">.</span><span class="n">freeze_support</span><span class="p">()</span>
</span><span id="L-884"><a href="#L-884"><span class="linenos">884</span></a>    <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-885"><a href="#L-885"><span class="linenos">885</span></a>    <span class="n">rand_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-886"><a href="#L-886"><span class="linenos">886</span></a>
</span><span id="L-887"><a href="#L-887"><span class="linenos">887</span></a>    <span class="n">n_bit</span> <span class="o">=</span> <span class="mi">4</span>
</span><span id="L-888"><a href="#L-888"><span class="linenos">888</span></a>    <span class="n">adder_train</span> <span class="o">=</span> <span class="n">adder_truth_table</span><span class="p">(</span><span class="n">n_bit</span><span class="p">)</span>
</span><span id="L-889"><a href="#L-889"><span class="linenos">889</span></a>    <span class="n">act_funcs</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">}</span>
</span><span id="L-890"><a href="#L-890"><span class="linenos">890</span></a>    <span class="n">adder_layout</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">n_bit</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">n_bit</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">n_bit</span><span class="p">,</span> <span class="n">n_bit</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">act_funcs</span><span class="p">)</span>
</span><span id="L-891"><a href="#L-891"><span class="linenos">891</span></a>
</span><span id="L-892"><a href="#L-892"><span class="linenos">892</span></a>    <span class="n">adder_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">adder_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">adder_train</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n_bit</span><span class="si">}</span><span class="s2">-bit Adder&quot;</span><span class="p">)</span>
</span><span id="L-893"><a href="#L-893"><span class="linenos">893</span></a>    <span class="n">adder_mlp</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">adder_train</span><span class="p">),</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plot_dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-894"><a href="#L-894"><span class="linenos">894</span></a>
</span><span id="L-895"><a href="#L-895"><span class="linenos">895</span></a><span class="k">def</span> <span class="nf">legacy_main</span><span class="p">():</span>
</span><span id="L-896"><a href="#L-896"><span class="linenos">896</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot; @private</span>
</span><span id="L-897"><a href="#L-897"><span class="linenos">897</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-898"><a href="#L-898"><span class="linenos">898</span></a>    <span class="n">rand_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-899"><a href="#L-899"><span class="linenos">899</span></a>    <span class="c1"># &lt;&lt;&lt; OR, AND, NAND and XOR &gt;&gt;&gt;</span>
</span><span id="L-900"><a href="#L-900"><span class="linenos">900</span></a>    <span class="n">or_train</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-901"><a href="#L-901"><span class="linenos">901</span></a>                <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-902"><a href="#L-902"><span class="linenos">902</span></a>                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-903"><a href="#L-903"><span class="linenos">903</span></a>                <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="L-904"><a href="#L-904"><span class="linenos">904</span></a>    <span class="n">and_train</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-905"><a href="#L-905"><span class="linenos">905</span></a>                 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-906"><a href="#L-906"><span class="linenos">906</span></a>                 <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-907"><a href="#L-907"><span class="linenos">907</span></a>                 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="L-908"><a href="#L-908"><span class="linenos">908</span></a>    <span class="n">nand_train</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-909"><a href="#L-909"><span class="linenos">909</span></a>                  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-910"><a href="#L-910"><span class="linenos">910</span></a>                  <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-911"><a href="#L-911"><span class="linenos">911</span></a>                  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</span><span id="L-912"><a href="#L-912"><span class="linenos">912</span></a>    <span class="n">xor_train</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-913"><a href="#L-913"><span class="linenos">913</span></a>                 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-914"><a href="#L-914"><span class="linenos">914</span></a>                 <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-915"><a href="#L-915"><span class="linenos">915</span></a>                 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</span><span id="L-916"><a href="#L-916"><span class="linenos">916</span></a>    <span class="c1"># activation function for each layer of MLP</span>
</span><span id="L-917"><a href="#L-917"><span class="linenos">917</span></a>    <span class="n">act_funcs</span> <span class="o">=</span> <span class="p">{</span><span class="n">layer</span><span class="p">:</span><span class="s2">&quot;sigmoid&quot;</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>
</span><span id="L-918"><a href="#L-918"><span class="linenos">918</span></a>    <span class="c1"># layout of a fully connected MLP of type: [2xinp ---&gt; 2nx1n ---&gt; out]</span>
</span><span id="L-919"><a href="#L-919"><span class="linenos">919</span></a>    <span class="n">xor_layout</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">act_funcs</span><span class="p">)</span>
</span><span id="L-920"><a href="#L-920"><span class="linenos">920</span></a>    <span class="n">xor_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">xor_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xor_train</span><span class="p">),</span> <span class="s2">&quot;XOR gate&quot;</span><span class="p">)</span>
</span><span id="L-921"><a href="#L-921"><span class="linenos">921</span></a>
</span><span id="L-922"><a href="#L-922"><span class="linenos">922</span></a>    <span class="c1"># &lt;&lt;&lt; MUX and DMUX &gt;&gt;&gt;</span>
</span><span id="L-923"><a href="#L-923"><span class="linenos">923</span></a>    <span class="n">dmux_train</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-924"><a href="#L-924"><span class="linenos">924</span></a>                  <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-925"><a href="#L-925"><span class="linenos">925</span></a>                  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-926"><a href="#L-926"><span class="linenos">926</span></a>                  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="L-927"><a href="#L-927"><span class="linenos">927</span></a>    <span class="n">dmux_layout</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">act_funcs</span><span class="p">)</span>
</span><span id="L-928"><a href="#L-928"><span class="linenos">928</span></a>    
</span><span id="L-929"><a href="#L-929"><span class="linenos">929</span></a>    <span class="n">mux_train</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-930"><a href="#L-930"><span class="linenos">930</span></a>                 <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-931"><a href="#L-931"><span class="linenos">931</span></a>                 <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-932"><a href="#L-932"><span class="linenos">932</span></a>                 <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-933"><a href="#L-933"><span class="linenos">933</span></a>                 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-934"><a href="#L-934"><span class="linenos">934</span></a>                 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-935"><a href="#L-935"><span class="linenos">935</span></a>                 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-936"><a href="#L-936"><span class="linenos">936</span></a>                 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-937"><a href="#L-937"><span class="linenos">937</span></a>                 <span class="p">)</span>
</span><span id="L-938"><a href="#L-938"><span class="linenos">938</span></a>    <span class="n">mux_layout</span> <span class="o">=</span> <span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">act_funcs</span><span class="p">)</span>
</span><span id="L-939"><a href="#L-939"><span class="linenos">939</span></a>    
</span><span id="L-940"><a href="#L-940"><span class="linenos">940</span></a>    <span class="n">dmux_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">dmux_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dmux_train</span><span class="p">),</span> <span class="s2">&quot;Dmux gate&quot;</span><span class="p">)</span>    
</span><span id="L-941"><a href="#L-941"><span class="linenos">941</span></a>    <span class="n">mux_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">mux_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mux_train</span><span class="p">),</span> <span class="s2">&quot;Mux gate&quot;</span><span class="p">)</span>
</span><span id="L-942"><a href="#L-942"><span class="linenos">942</span></a>    
</span><span id="L-943"><a href="#L-943"><span class="linenos">943</span></a>    <span class="c1"># &lt;&lt;&lt; ADDERS &gt;&gt;&gt; </span>
</span><span id="L-944"><a href="#L-944"><span class="linenos">944</span></a>    <span class="n">half_add_train</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-945"><a href="#L-945"><span class="linenos">945</span></a>                      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-946"><a href="#L-946"><span class="linenos">946</span></a>                      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-947"><a href="#L-947"><span class="linenos">947</span></a>                      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="L-948"><a href="#L-948"><span class="linenos">948</span></a>    <span class="n">half_add_layout</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">act_funcs</span><span class="p">)</span>
</span><span id="L-949"><a href="#L-949"><span class="linenos">949</span></a>
</span><span id="L-950"><a href="#L-950"><span class="linenos">950</span></a>    <span class="n">full_add_train</span> <span class="o">=</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-951"><a href="#L-951"><span class="linenos">951</span></a>                      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-952"><a href="#L-952"><span class="linenos">952</span></a>                      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-953"><a href="#L-953"><span class="linenos">953</span></a>                      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
</span><span id="L-954"><a href="#L-954"><span class="linenos">954</span></a>                      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-955"><a href="#L-955"><span class="linenos">955</span></a>                      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-956"><a href="#L-956"><span class="linenos">956</span></a>                      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span><span id="L-957"><a href="#L-957"><span class="linenos">957</span></a>                      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="L-958"><a href="#L-958"><span class="linenos">958</span></a>    <span class="n">full_add_layout</span> <span class="o">=</span> <span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">act_funcs</span><span class="p">)</span>
</span><span id="L-959"><a href="#L-959"><span class="linenos">959</span></a>
</span><span id="L-960"><a href="#L-960"><span class="linenos">960</span></a>    <span class="n">half_add_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">half_add_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">half_add_train</span><span class="p">),</span> <span class="s2">&quot;Half-adder&quot;</span><span class="p">)</span>
</span><span id="L-961"><a href="#L-961"><span class="linenos">961</span></a>    <span class="n">full_add_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">full_add_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">full_add_train</span><span class="p">),</span> <span class="s2">&quot;Full-adder&quot;</span><span class="p">)</span>
</span><span id="L-962"><a href="#L-962"><span class="linenos">962</span></a>
</span><span id="L-963"><a href="#L-963"><span class="linenos">963</span></a><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span id="L-964"><a href="#L-964"><span class="linenos">964</span></a>    <span class="n">main</span><span class="p">()</span>
</span></pre></div>


            </section>
                <section id="MLP">
                            <input id="MLP-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">MLP</span>:

                <label class="view-source-button" for="MLP-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP-15"><a href="#MLP-15"><span class="linenos"> 15</span></a><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
</span><span id="MLP-16"><a href="#MLP-16"><span class="linenos"> 16</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-17"><a href="#MLP-17"><span class="linenos"> 17</span></a><span class="sd">    A class implementation of a multilayer perceptron with a customizable size and configuration.</span>
</span><span id="MLP-18"><a href="#MLP-18"><span class="linenos"> 18</span></a><span class="sd">    It employs only the core Python and numpy functions. The performance optimization is done via numpy vectorization.</span>
</span><span id="MLP-19"><a href="#MLP-19"><span class="linenos"> 19</span></a><span class="sd">    Multiple activation and loss functions are supported and can be easily extended.</span>
</span><span id="MLP-20"><a href="#MLP-20"><span class="linenos"> 20</span></a><span class="sd">    Provides two plotting methods (static / real-time).</span>
</span><span id="MLP-21"><a href="#MLP-21"><span class="linenos"> 21</span></a><span class="sd">    Allows for storing the image of the network after the training in a file, as well as importing other saved images.</span>
</span><span id="MLP-22"><a href="#MLP-22"><span class="linenos"> 22</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="MLP-23"><a href="#MLP-23"><span class="linenos"> 23</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mlp_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span><span id="MLP-24"><a href="#MLP-24"><span class="linenos"> 24</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-25"><a href="#MLP-25"><span class="linenos"> 25</span></a><span class="sd">        The function is the constructor for a `MLP` class, which initializes the parameters and the state </span>
</span><span id="MLP-26"><a href="#MLP-26"><span class="linenos"> 26</span></a><span class="sd">        of the newly created neural network object.</span>
</span><span id="MLP-27"><a href="#MLP-27"><span class="linenos"> 27</span></a><span class="sd">        The `MLP` object state is defined by mapping between each layer and the corresponding matrices that </span>
</span><span id="MLP-28"><a href="#MLP-28"><span class="linenos"> 28</span></a><span class="sd">        contain values for weights and biases plus the name of the activation function. It is represented as </span>
</span><span id="MLP-29"><a href="#MLP-29"><span class="linenos"> 29</span></a><span class="sd">        the dictionary with the following structure:</span>
</span><span id="MLP-30"><a href="#MLP-30"><span class="linenos"> 30</span></a><span class="sd">        `{layer: ([w, b], &#39;act_func&#39;)}`, where:</span>
</span><span id="MLP-31"><a href="#MLP-31"><span class="linenos"> 31</span></a><span class="sd">            - `w` is the matrix of weights for the layer;</span>
</span><span id="MLP-32"><a href="#MLP-32"><span class="linenos"> 32</span></a><span class="sd">            - `b` is the matrix of biases for the layer;</span>
</span><span id="MLP-33"><a href="#MLP-33"><span class="linenos"> 33</span></a><span class="sd">            - `&#39;act_func&#39;` is the activation function for the layer;</span>
</span><span id="MLP-34"><a href="#MLP-34"><span class="linenos"> 34</span></a><span class="sd">            - `layer` is the number of the layer.</span>
</span><span id="MLP-35"><a href="#MLP-35"><span class="linenos"> 35</span></a>
</span><span id="MLP-36"><a href="#MLP-36"><span class="linenos"> 36</span></a><span class="sd">        :param mlp_layout: A layout of the neural network. Defined as a tuple of type:</span>
</span><span id="MLP-37"><a href="#MLP-37"><span class="linenos"> 37</span></a><span class="sd">        `((inp, l1, l2, ... , ln), act_map)`, where:</span>
</span><span id="MLP-38"><a href="#MLP-38"><span class="linenos"> 38</span></a><span class="sd">            - `inp` denotes the size of the input;</span>
</span><span id="MLP-39"><a href="#MLP-39"><span class="linenos"> 39</span></a><span class="sd">            - `ln` denotes the number of neurons in the n-th layer.</span>
</span><span id="MLP-40"><a href="#MLP-40"><span class="linenos"> 40</span></a><span class="sd">            - `act_map` denotes types of activation funcs for each layer.</span>
</span><span id="MLP-41"><a href="#MLP-41"><span class="linenos"> 41</span></a><span class="sd">        :param rand_range: A tuple containing the lower and upper limits for random initialization of </span>
</span><span id="MLP-42"><a href="#MLP-42"><span class="linenos"> 42</span></a><span class="sd">        parameter matrices</span>
</span><span id="MLP-43"><a href="#MLP-43"><span class="linenos"> 43</span></a><span class="sd">        :param train_data: A sequence of training data samples of type:</span>
</span><span id="MLP-44"><a href="#MLP-44"><span class="linenos"> 44</span></a><span class="sd">        `((a1, b1, c1, ...), (a2, b2, c2, ...), (an, bn, cn, ...))`, where:</span>
</span><span id="MLP-45"><a href="#MLP-45"><span class="linenos"> 45</span></a><span class="sd">            - each sample contains `j` input and `k` output values, with `j` defined by `mlp_layout[0][0]`</span>
</span><span id="MLP-46"><a href="#MLP-46"><span class="linenos"> 46</span></a><span class="sd">        :param name: The name of the neural network that identifies it</span>
</span><span id="MLP-47"><a href="#MLP-47"><span class="linenos"> 47</span></a><span class="sd">        :param rate: Is used to define the speed of learning in the neural network. It determines how quickly </span>
</span><span id="MLP-48"><a href="#MLP-48"><span class="linenos"> 48</span></a><span class="sd">        the network adjusts its weights and biases during the training process. Defaults to `1` (optional)</span>
</span><span id="MLP-49"><a href="#MLP-49"><span class="linenos"> 49</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-50"><a href="#MLP-50"><span class="linenos"> 50</span></a>        <span class="c1"># to store shape of each parameter mx</span>
</span><span id="MLP-51"><a href="#MLP-51"><span class="linenos"> 51</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mx_size_map</span><span class="p">(</span><span class="n">mlp_layout</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span id="MLP-52"><a href="#MLP-52"><span class="linenos"> 52</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">)</span>
</span><span id="MLP-53"><a href="#MLP-53"><span class="linenos"> 53</span></a>        <span class="c1"># to store activation funcs for each layer</span>
</span><span id="MLP-54"><a href="#MLP-54"><span class="linenos"> 54</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span> <span class="o">=</span> <span class="n">mlp_layout</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP-55"><a href="#MLP-55"><span class="linenos"> 55</span></a>        <span class="c1"># to map act. funcs and derivatives to each layer</span>
</span><span id="MLP-56"><a href="#MLP-56"><span class="linenos"> 56</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_func_map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span><span class="p">)</span>
</span><span id="MLP-57"><a href="#MLP-57"><span class="linenos"> 57</span></a>        <span class="c1"># to store training dataset</span>
</span><span id="MLP-58"><a href="#MLP-58"><span class="linenos"> 58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_train</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="MLP-59"><a href="#MLP-59"><span class="linenos"> 59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span> <span class="o">=</span> <span class="n">mlp_layout</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-60"><a href="#MLP-60"><span class="linenos"> 60</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MLP-61"><a href="#MLP-61"><span class="linenos"> 61</span></a>        <span class="c1"># to store the state of MLP</span>
</span><span id="MLP-62"><a href="#MLP-62"><span class="linenos"> 62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP-63"><a href="#MLP-63"><span class="linenos"> 63</span></a>        <span class="c1"># to store activations</span>
</span><span id="MLP-64"><a href="#MLP-64"><span class="linenos"> 64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP-65"><a href="#MLP-65"><span class="linenos"> 65</span></a>        <span class="c1"># to store the gradient</span>
</span><span id="MLP-66"><a href="#MLP-66"><span class="linenos"> 66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP-67"><a href="#MLP-67"><span class="linenos"> 67</span></a>        <span class="c1"># constant for finite difference computation</span>
</span><span id="MLP-68"><a href="#MLP-68"><span class="linenos"> 68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="mf">1e-1</span>
</span><span id="MLP-69"><a href="#MLP-69"><span class="linenos"> 69</span></a>        <span class="c1"># to define the speed of learning</span>
</span><span id="MLP-70"><a href="#MLP-70"><span class="linenos"> 70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">rate</span>
</span><span id="MLP-71"><a href="#MLP-71"><span class="linenos"> 71</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
</span><span id="MLP-72"><a href="#MLP-72"><span class="linenos"> 72</span></a>        <span class="c1"># initializes the parameters and the gradient</span>
</span><span id="MLP-73"><a href="#MLP-73"><span class="linenos"> 73</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="MLP-74"><a href="#MLP-74"><span class="linenos"> 74</span></a>            <span class="c1"># gets the sizes of parameter matrixes for each layer</span>
</span><span id="MLP-75"><a href="#MLP-75"><span class="linenos"> 75</span></a>            <span class="n">w_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-76"><a href="#MLP-76"><span class="linenos"> 76</span></a>            <span class="n">b_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP-77"><a href="#MLP-77"><span class="linenos"> 77</span></a>            <span class="c1"># creates two random matrixes for weights and biases per each layer</span>
</span><span id="MLP-78"><a href="#MLP-78"><span class="linenos"> 78</span></a>            <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">rand_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rand_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">w_shape</span><span class="p">)</span>
</span><span id="MLP-79"><a href="#MLP-79"><span class="linenos"> 79</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">rand_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rand_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">b_shape</span><span class="p">)</span>
</span><span id="MLP-80"><a href="#MLP-80"><span class="linenos"> 80</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>
</span><span id="MLP-81"><a href="#MLP-81"><span class="linenos"> 81</span></a>            <span class="c1"># initializes gradient table with empty matrixes of the same shape</span>
</span><span id="MLP-82"><a href="#MLP-82"><span class="linenos"> 82</span></a>            <span class="n">w_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">w_shape</span><span class="p">)</span>
</span><span id="MLP-83"><a href="#MLP-83"><span class="linenos"> 83</span></a>            <span class="n">b_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">b_shape</span><span class="p">)</span>
</span><span id="MLP-84"><a href="#MLP-84"><span class="linenos"> 84</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_grad</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">]</span>
</span><span id="MLP-85"><a href="#MLP-85"><span class="linenos"> 85</span></a>
</span><span id="MLP-86"><a href="#MLP-86"><span class="linenos"> 86</span></a>    <span class="c1"># helper methods for __init__</span>
</span><span id="MLP-87"><a href="#MLP-87"><span class="linenos"> 87</span></a>    <span class="k">def</span> <span class="nf">_mx_size_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
</span><span id="MLP-88"><a href="#MLP-88"><span class="linenos"> 88</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP-89"><a href="#MLP-89"><span class="linenos"> 89</span></a><span class="sd">        The function creates a mapping between each layer in a neural network and the sizes of</span>
</span><span id="MLP-90"><a href="#MLP-90"><span class="linenos"> 90</span></a><span class="sd">        the corresponding weight and bias matrices.</span>
</span><span id="MLP-91"><a href="#MLP-91"><span class="linenos"> 91</span></a><span class="sd">        </span>
</span><span id="MLP-92"><a href="#MLP-92"><span class="linenos"> 92</span></a><span class="sd">        :param params: The `params` parameter is a list that represents the number of neurons in each layer</span>
</span><span id="MLP-93"><a href="#MLP-93"><span class="linenos"> 93</span></a><span class="sd">        of a neural network. For example, if `params = (10, 20, 30)`, it means that the neural network has 3</span>
</span><span id="MLP-94"><a href="#MLP-94"><span class="linenos"> 94</span></a><span class="sd">        layers with 10 neurons in the input layer, 20 in the inner layer, and 30 in the output layer</span>
</span><span id="MLP-95"><a href="#MLP-95"><span class="linenos"> 95</span></a><span class="sd">        :return: a dictionary where the keys are the layer numbers and the values are tuples. Each tuple</span>
</span><span id="MLP-96"><a href="#MLP-96"><span class="linenos"> 96</span></a><span class="sd">        contains the size of the weight matrix and the size of the bias matrix for that layer:</span>
</span><span id="MLP-97"><a href="#MLP-97"><span class="linenos"> 97</span></a><span class="sd">                `{1:(w1_size, b1_size), </span>
</span><span id="MLP-98"><a href="#MLP-98"><span class="linenos"> 98</span></a><span class="sd">                  2:(w2_size, b2_size),</span>
</span><span id="MLP-99"><a href="#MLP-99"><span class="linenos"> 99</span></a><span class="sd">                  n:(wn_size, bn_size)}`.</span>
</span><span id="MLP-100"><a href="#MLP-100"><span class="linenos">100</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-101"><a href="#MLP-101"><span class="linenos">101</span></a>        <span class="n">mlp_layout</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP-102"><a href="#MLP-102"><span class="linenos">102</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)):</span>
</span><span id="MLP-103"><a href="#MLP-103"><span class="linenos">103</span></a>            <span class="n">mlp_layout</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="p">]),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="p">]))</span>
</span><span id="MLP-104"><a href="#MLP-104"><span class="linenos">104</span></a>
</span><span id="MLP-105"><a href="#MLP-105"><span class="linenos">105</span></a>        <span class="k">return</span> <span class="n">mlp_layout</span>
</span><span id="MLP-106"><a href="#MLP-106"><span class="linenos">106</span></a>    <span class="k">def</span> <span class="nf">_act_func_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">act_map</span><span class="p">):</span>
</span><span id="MLP-107"><a href="#MLP-107"><span class="linenos">107</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP-108"><a href="#MLP-108"><span class="linenos">108</span></a><span class="sd">        The function converts a provided map of activation functions for each layer into a</span>
</span><span id="MLP-109"><a href="#MLP-109"><span class="linenos">109</span></a><span class="sd">        dictionary with direct references to the functions and their derivatives for each layer.</span>
</span><span id="MLP-110"><a href="#MLP-110"><span class="linenos">110</span></a><span class="sd">        </span>
</span><span id="MLP-111"><a href="#MLP-111"><span class="linenos">111</span></a><span class="sd">        :param act_map: The `act_map` parameter is a dictionary that maps layer numbers to activation</span>
</span><span id="MLP-112"><a href="#MLP-112"><span class="linenos">112</span></a><span class="sd">        function names. Each key-value pair in the dictionary represents a layer in the neural network,</span>
</span><span id="MLP-113"><a href="#MLP-113"><span class="linenos">113</span></a><span class="sd">        where the key is the layer number (an integer) and the value is the name of the activation function</span>
</span><span id="MLP-114"><a href="#MLP-114"><span class="linenos">114</span></a><span class="sd">        (a string)</span>
</span><span id="MLP-115"><a href="#MLP-115"><span class="linenos">115</span></a><span class="sd">        :return: a new dictionary where each layer number is mapped to a tuple containing the activation</span>
</span><span id="MLP-116"><a href="#MLP-116"><span class="linenos">116</span></a><span class="sd">        function and its derivative for that layer.</span>
</span><span id="MLP-117"><a href="#MLP-117"><span class="linenos">117</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-118"><a href="#MLP-118"><span class="linenos">118</span></a>        <span class="c1"># validate the input</span>
</span><span id="MLP-119"><a href="#MLP-119"><span class="linenos">119</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">act_map</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span><span class="p">,</span> <span class="s2">&quot;Invalid activation map (length)&quot;</span>
</span><span id="MLP-120"><a href="#MLP-120"><span class="linenos">120</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="s2">&quot;Invalid layer number(s)&quot;</span>
</span><span id="MLP-121"><a href="#MLP-121"><span class="linenos">121</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">func_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="s2">&quot;Invalid function name(s)&quot;</span>
</span><span id="MLP-122"><a href="#MLP-122"><span class="linenos">122</span></a>
</span><span id="MLP-123"><a href="#MLP-123"><span class="linenos">123</span></a>        <span class="c1"># return a new dict with funcs and their derivatives mapped to each layer</span>
</span><span id="MLP-124"><a href="#MLP-124"><span class="linenos">124</span></a>        <span class="k">return</span> <span class="p">{</span><span class="n">layer</span><span class="p">:</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">func_name</span><span class="p">),</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">func_name</span> <span class="o">+</span> <span class="s2">&quot;_der&quot;</span><span class="p">))</span> <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span><span id="MLP-125"><a href="#MLP-125"><span class="linenos">125</span></a>
</span><span id="MLP-126"><a href="#MLP-126"><span class="linenos">126</span></a>    <span class="c1"># methods for printing to the console</span>
</span><span id="MLP-127"><a href="#MLP-127"><span class="linenos">127</span></a>    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">with_full_report</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="MLP-128"><a href="#MLP-128"><span class="linenos">128</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-129"><a href="#MLP-129"><span class="linenos">129</span></a><span class="sd">        The function returns a string representation of the current state of the model, including</span>
</span><span id="MLP-130"><a href="#MLP-130"><span class="linenos">130</span></a><span class="sd">        the model name, cost, parameters, and performance on training samples.</span>
</span><span id="MLP-131"><a href="#MLP-131"><span class="linenos">131</span></a><span class="sd">        </span>
</span><span id="MLP-132"><a href="#MLP-132"><span class="linenos">132</span></a><span class="sd">        :param with_full_report: The (optional) `with_full_report` parameter is a boolean flag that determines whether</span>
</span><span id="MLP-133"><a href="#MLP-133"><span class="linenos">133</span></a><span class="sd">        to include additional information in the string representation of the model&#39;s state. If</span>
</span><span id="MLP-134"><a href="#MLP-134"><span class="linenos">134</span></a><span class="sd">        `with_full_report` is set to `True`, the string will include the values of parameters for each</span>
</span><span id="MLP-135"><a href="#MLP-135"><span class="linenos">135</span></a><span class="sd">        neuron and the actual output of the model on all training samples. Defaults to `False`</span>
</span><span id="MLP-136"><a href="#MLP-136"><span class="linenos">136</span></a><span class="sd">        :return: a string that represents the current state of the model.</span>
</span><span id="MLP-137"><a href="#MLP-137"><span class="linenos">137</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-138"><a href="#MLP-138"><span class="linenos">138</span></a>        <span class="n">state_str</span>  <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;1) Model: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="MLP-139"><a href="#MLP-139"><span class="linenos">139</span></a>        <span class="n">state_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;2) Cost: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="MLP-140"><a href="#MLP-140"><span class="linenos">140</span></a>
</span><span id="MLP-141"><a href="#MLP-141"><span class="linenos">141</span></a>        <span class="k">if</span> <span class="n">with_full_report</span><span class="p">:</span>
</span><span id="MLP-142"><a href="#MLP-142"><span class="linenos">142</span></a>            <span class="c1"># prints out parameter matrixes (weights and biases)</span>
</span><span id="MLP-143"><a href="#MLP-143"><span class="linenos">143</span></a>            <span class="c1"># for each of the layers of the MLP</span>
</span><span id="MLP-144"><a href="#MLP-144"><span class="linenos">144</span></a>            <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;3) State of MLP parameters:</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="MLP-145"><a href="#MLP-145"><span class="linenos">145</span></a>            <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">param_lst</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP-146"><a href="#MLP-146"><span class="linenos">146</span></a>                <span class="n">state_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Layer: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="MLP-147"><a href="#MLP-147"><span class="linenos">147</span></a>                <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param_mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_lst</span><span class="p">):</span>
</span><span id="MLP-148"><a href="#MLP-148"><span class="linenos">148</span></a>                    <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MLP-149"><a href="#MLP-149"><span class="linenos">149</span></a>                        <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;&lt;&lt;&lt; Weights &gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="MLP-150"><a href="#MLP-150"><span class="linenos">150</span></a>                    <span class="k">else</span><span class="p">:</span>
</span><span id="MLP-151"><a href="#MLP-151"><span class="linenos">151</span></a>                        <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt;&lt;&lt; Biases &gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="MLP-152"><a href="#MLP-152"><span class="linenos">152</span></a>                    <span class="n">state_str</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">param_mx</span><span class="p">)</span>
</span><span id="MLP-153"><a href="#MLP-153"><span class="linenos">153</span></a>                <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">---------------</span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="MLP-154"><a href="#MLP-154"><span class="linenos">154</span></a>
</span><span id="MLP-155"><a href="#MLP-155"><span class="linenos">155</span></a>            <span class="c1"># prints out mapping between input values and</span>
</span><span id="MLP-156"><a href="#MLP-156"><span class="linenos">156</span></a>            <span class="c1"># the actual output of the MLP for all data samples</span>
</span><span id="MLP-157"><a href="#MLP-157"><span class="linenos">157</span></a>            <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;4) Total MLP performance:&quot;</span>
</span><span id="MLP-158"><a href="#MLP-158"><span class="linenos">158</span></a>            <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="MLP-159"><a href="#MLP-159"><span class="linenos">159</span></a>                <span class="n">state_str</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(&quot;</span>
</span><span id="MLP-160"><a href="#MLP-160"><span class="linenos">160</span></a>                <span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="MLP-161"><a href="#MLP-161"><span class="linenos">161</span></a>                <span class="k">for</span> <span class="n">inp_val</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">:</span>
</span><span id="MLP-162"><a href="#MLP-162"><span class="linenos">162</span></a>                    <span class="n">state_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">inp_val</span><span class="si">}</span><span class="s2">, &quot;</span>
</span><span id="MLP-163"><a href="#MLP-163"><span class="linenos">163</span></a>                
</span><span id="MLP-164"><a href="#MLP-164"><span class="linenos">164</span></a>                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="MLP-165"><a href="#MLP-165"><span class="linenos">165</span></a>                <span class="n">state_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">)&quot;</span>
</span><span id="MLP-166"><a href="#MLP-166"><span class="linenos">166</span></a>
</span><span id="MLP-167"><a href="#MLP-167"><span class="linenos">167</span></a>        <span class="k">return</span> <span class="n">state_str</span>
</span><span id="MLP-168"><a href="#MLP-168"><span class="linenos">168</span></a>    <span class="k">def</span> <span class="nf">_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="MLP-169"><a href="#MLP-169"><span class="linenos">169</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-170"><a href="#MLP-170"><span class="linenos">170</span></a><span class="sd">        The function computes the current cost value of the neural network by calculating the mean</span>
</span><span id="MLP-171"><a href="#MLP-171"><span class="linenos">171</span></a><span class="sd">        squared error (MSE) between the network&#39;s output and the expected output for each training data sample.</span>
</span><span id="MLP-172"><a href="#MLP-172"><span class="linenos">172</span></a><span class="sd">        :return: the current cost value of the neural network.</span>
</span><span id="MLP-173"><a href="#MLP-173"><span class="linenos">173</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-174"><a href="#MLP-174"><span class="linenos">174</span></a>        <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP-175"><a href="#MLP-175"><span class="linenos">175</span></a>        <span class="c1"># iterates through each entry of the training data</span>
</span><span id="MLP-176"><a href="#MLP-176"><span class="linenos">176</span></a>        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="MLP-177"><a href="#MLP-177"><span class="linenos">177</span></a>            <span class="c1"># splits training data array </span>
</span><span id="MLP-178"><a href="#MLP-178"><span class="linenos">178</span></a>            <span class="c1"># into input/expected output subarrays</span>
</span><span id="MLP-179"><a href="#MLP-179"><span class="linenos">179</span></a>            <span class="nb">input</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="MLP-180"><a href="#MLP-180"><span class="linenos">180</span></a>            <span class="n">expected</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">:</span> <span class="p">]])</span>
</span><span id="MLP-181"><a href="#MLP-181"><span class="linenos">181</span></a>
</span><span id="MLP-182"><a href="#MLP-182"><span class="linenos">182</span></a>            <span class="c1"># feeds input subarray into the MLP</span>
</span><span id="MLP-183"><a href="#MLP-183"><span class="linenos">183</span></a>            <span class="c1"># collects the output data array</span>
</span><span id="MLP-184"><a href="#MLP-184"><span class="linenos">184</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="MLP-185"><a href="#MLP-185"><span class="linenos">185</span></a>
</span><span id="MLP-186"><a href="#MLP-186"><span class="linenos">186</span></a>            <span class="c1"># verifies that there is no mismatch between </span>
</span><span id="MLP-187"><a href="#MLP-187"><span class="linenos">187</span></a>            <span class="c1"># the sizes of the expected and actual output arrays</span>
</span><span id="MLP-188"><a href="#MLP-188"><span class="linenos">188</span></a>            <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">shape</span>
</span><span id="MLP-189"><a href="#MLP-189"><span class="linenos">189</span></a>            
</span><span id="MLP-190"><a href="#MLP-190"><span class="linenos">190</span></a>            <span class="c1"># iterates through both arrays and </span>
</span><span id="MLP-191"><a href="#MLP-191"><span class="linenos">191</span></a>            <span class="c1"># computes corresponding error values</span>
</span><span id="MLP-192"><a href="#MLP-192"><span class="linenos">192</span></a>            <span class="n">err_vals</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
</span><span id="MLP-193"><a href="#MLP-193"><span class="linenos">193</span></a>
</span><span id="MLP-194"><a href="#MLP-194"><span class="linenos">194</span></a>            <span class="c1"># adds the accumulated error to the total cost</span>
</span><span id="MLP-195"><a href="#MLP-195"><span class="linenos">195</span></a>            <span class="n">result</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">err_vals</span><span class="p">)</span>
</span><span id="MLP-196"><a href="#MLP-196"><span class="linenos">196</span></a>
</span><span id="MLP-197"><a href="#MLP-197"><span class="linenos">197</span></a>        <span class="c1"># computes the mean error value per 1 sample of training data</span>
</span><span id="MLP-198"><a href="#MLP-198"><span class="linenos">198</span></a>        <span class="n">result</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>
</span><span id="MLP-199"><a href="#MLP-199"><span class="linenos">199</span></a>
</span><span id="MLP-200"><a href="#MLP-200"><span class="linenos">200</span></a>        <span class="k">return</span> <span class="n">result</span>
</span><span id="MLP-201"><a href="#MLP-201"><span class="linenos">201</span></a>
</span><span id="MLP-202"><a href="#MLP-202"><span class="linenos">202</span></a>    <span class="c1"># methods for computing activations</span>
</span><span id="MLP-203"><a href="#MLP-203"><span class="linenos">203</span></a>    <span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-204"><a href="#MLP-204"><span class="linenos">204</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-205"><a href="#MLP-205"><span class="linenos">205</span></a><span class="sd">        The function computes the sigmoid of a given input.</span>
</span><span id="MLP-206"><a href="#MLP-206"><span class="linenos">206</span></a><span class="sd">        </span>
</span><span id="MLP-207"><a href="#MLP-207"><span class="linenos">207</span></a><span class="sd">        :param x: The parameter `x` is the input value for which we want to compute the sigmoid function</span>
</span><span id="MLP-208"><a href="#MLP-208"><span class="linenos">208</span></a><span class="sd">        :return: the value of sigmoid(x).</span>
</span><span id="MLP-209"><a href="#MLP-209"><span class="linenos">209</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-210"><a href="#MLP-210"><span class="linenos">210</span></a>        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</span><span id="MLP-211"><a href="#MLP-211"><span class="linenos">211</span></a>    <span class="k">def</span> <span class="nf">_relu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-212"><a href="#MLP-212"><span class="linenos">212</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-213"><a href="#MLP-213"><span class="linenos">213</span></a><span class="sd">        The function computes the ReLU (Rectified Linear Unit) of a given input.</span>
</span><span id="MLP-214"><a href="#MLP-214"><span class="linenos">214</span></a><span class="sd">        </span>
</span><span id="MLP-215"><a href="#MLP-215"><span class="linenos">215</span></a><span class="sd">        :param x: The parameter `x` is a scalar or an array-like object representing the input to the ReLU</span>
</span><span id="MLP-216"><a href="#MLP-216"><span class="linenos">216</span></a><span class="sd">        function</span>
</span><span id="MLP-217"><a href="#MLP-217"><span class="linenos">217</span></a><span class="sd">        :return: the maximum value between 0 and the input value x.</span>
</span><span id="MLP-218"><a href="#MLP-218"><span class="linenos">218</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-219"><a href="#MLP-219"><span class="linenos">219</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span><span id="MLP-220"><a href="#MLP-220"><span class="linenos">220</span></a>    <span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-221"><a href="#MLP-221"><span class="linenos">221</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-222"><a href="#MLP-222"><span class="linenos">222</span></a><span class="sd">        The function computes the softmax in a stable manner of a given input.</span>
</span><span id="MLP-223"><a href="#MLP-223"><span class="linenos">223</span></a><span class="sd">        </span>
</span><span id="MLP-224"><a href="#MLP-224"><span class="linenos">224</span></a><span class="sd">        :param x: The parameter `x` is a numpy array representing the input values for which we want to</span>
</span><span id="MLP-225"><a href="#MLP-225"><span class="linenos">225</span></a><span class="sd">        compute the softmax function</span>
</span><span id="MLP-226"><a href="#MLP-226"><span class="linenos">226</span></a><span class="sd">        :return: the softmax of the input array x.</span>
</span><span id="MLP-227"><a href="#MLP-227"><span class="linenos">227</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-228"><a href="#MLP-228"><span class="linenos">228</span></a>        <span class="n">norm_x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MLP-229"><a href="#MLP-229"><span class="linenos">229</span></a>        <span class="n">exp_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">norm_x</span><span class="p">)</span>
</span><span id="MLP-230"><a href="#MLP-230"><span class="linenos">230</span></a>        <span class="n">result</span> <span class="o">=</span> <span class="n">exp_z</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_z</span><span class="p">)</span>
</span><span id="MLP-231"><a href="#MLP-231"><span class="linenos">231</span></a>
</span><span id="MLP-232"><a href="#MLP-232"><span class="linenos">232</span></a>        <span class="k">return</span> <span class="n">result</span>
</span><span id="MLP-233"><a href="#MLP-233"><span class="linenos">233</span></a>
</span><span id="MLP-234"><a href="#MLP-234"><span class="linenos">234</span></a>    <span class="c1"># methods for computing activation derivatives</span>
</span><span id="MLP-235"><a href="#MLP-235"><span class="linenos">235</span></a>    <span class="k">def</span> <span class="nf">_sigmoid_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-236"><a href="#MLP-236"><span class="linenos">236</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-237"><a href="#MLP-237"><span class="linenos">237</span></a><span class="sd">        Computes the derivative values for f(x)=1/(1+e**(-x)).</span>
</span><span id="MLP-238"><a href="#MLP-238"><span class="linenos">238</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-239"><a href="#MLP-239"><span class="linenos">239</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
</span><span id="MLP-240"><a href="#MLP-240"><span class="linenos">240</span></a>    <span class="k">def</span> <span class="nf">_relu_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-241"><a href="#MLP-241"><span class="linenos">241</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-242"><a href="#MLP-242"><span class="linenos">242</span></a><span class="sd">        Computes the derivative values for f(x)=max(0, x).</span>
</span><span id="MLP-243"><a href="#MLP-243"><span class="linenos">243</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-244"><a href="#MLP-244"><span class="linenos">244</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="MLP-245"><a href="#MLP-245"><span class="linenos">245</span></a>    <span class="k">def</span> <span class="nf">_softmax_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-246"><a href="#MLP-246"><span class="linenos">246</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-247"><a href="#MLP-247"><span class="linenos">247</span></a><span class="sd">        Computes the derivative values for f(x)=softmax([x]).</span>
</span><span id="MLP-248"><a href="#MLP-248"><span class="linenos">248</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-249"><a href="#MLP-249"><span class="linenos">249</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigmoid_der</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MLP-250"><a href="#MLP-250"><span class="linenos">250</span></a>
</span><span id="MLP-251"><a href="#MLP-251"><span class="linenos">251</span></a>    <span class="c1"># methods for computing loss derivatives</span>
</span><span id="MLP-252"><a href="#MLP-252"><span class="linenos">252</span></a>    <span class="k">def</span> <span class="nf">_abs_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-253"><a href="#MLP-253"><span class="linenos">253</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-254"><a href="#MLP-254"><span class="linenos">254</span></a><span class="sd">        Computes the derivative values for f(x) = |x|.</span>
</span><span id="MLP-255"><a href="#MLP-255"><span class="linenos">255</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-256"><a href="#MLP-256"><span class="linenos">256</span></a>        <span class="c1"># creates an array of ones with the same shape as input arr</span>
</span><span id="MLP-257"><a href="#MLP-257"><span class="linenos">257</span></a>        <span class="n">der_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MLP-258"><a href="#MLP-258"><span class="linenos">258</span></a>        <span class="c1"># replaces elements less than or equal to zero with -1</span>
</span><span id="MLP-259"><a href="#MLP-259"><span class="linenos">259</span></a>        <span class="n">der_arr</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span><span id="MLP-260"><a href="#MLP-260"><span class="linenos">260</span></a>        <span class="k">return</span> <span class="n">der_arr</span>
</span><span id="MLP-261"><a href="#MLP-261"><span class="linenos">261</span></a>    <span class="k">def</span> <span class="nf">_sqr_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-262"><a href="#MLP-262"><span class="linenos">262</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-263"><a href="#MLP-263"><span class="linenos">263</span></a><span class="sd">        Computes the derivative values for f(x)=x**2.</span>
</span><span id="MLP-264"><a href="#MLP-264"><span class="linenos">264</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-265"><a href="#MLP-265"><span class="linenos">265</span></a>        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
</span><span id="MLP-266"><a href="#MLP-266"><span class="linenos">266</span></a>    <span class="k">def</span> <span class="nf">_half_sqr_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-267"><a href="#MLP-267"><span class="linenos">267</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-268"><a href="#MLP-268"><span class="linenos">268</span></a><span class="sd">        Computes the derivative values for f(x)=(x**2)/2.</span>
</span><span id="MLP-269"><a href="#MLP-269"><span class="linenos">269</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-270"><a href="#MLP-270"><span class="linenos">270</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="MLP-271"><a href="#MLP-271"><span class="linenos">271</span></a>    <span class="k">def</span> <span class="nf">_lncosh_der</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="MLP-272"><a href="#MLP-272"><span class="linenos">272</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-273"><a href="#MLP-273"><span class="linenos">273</span></a><span class="sd">        Computes the derivative values for f(x)=ln(cosh(x)).</span>
</span><span id="MLP-274"><a href="#MLP-274"><span class="linenos">274</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-275"><a href="#MLP-275"><span class="linenos">275</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="MLP-276"><a href="#MLP-276"><span class="linenos">276</span></a>
</span><span id="MLP-277"><a href="#MLP-277"><span class="linenos">277</span></a>    <span class="c1"># core methods for the learning process</span>
</span><span id="MLP-278"><a href="#MLP-278"><span class="linenos">278</span></a>    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_arr</span><span class="p">):</span>
</span><span id="MLP-279"><a href="#MLP-279"><span class="linenos">279</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP-280"><a href="#MLP-280"><span class="linenos">280</span></a><span class="sd">        The function takes an input array and passes it through each layer of a neural network,</span>
</span><span id="MLP-281"><a href="#MLP-281"><span class="linenos">281</span></a><span class="sd">        applying weights, biases, and activation functions to produce the final output.</span>
</span><span id="MLP-282"><a href="#MLP-282"><span class="linenos">282</span></a><span class="sd">        </span>
</span><span id="MLP-283"><a href="#MLP-283"><span class="linenos">283</span></a><span class="sd">        :param input_arr: The input_arr is a numpy array that represents the input to the neural network. It</span>
</span><span id="MLP-284"><a href="#MLP-284"><span class="linenos">284</span></a><span class="sd">        is the input that will be forwarded through each layer of the network</span>
</span><span id="MLP-285"><a href="#MLP-285"><span class="linenos">285</span></a><span class="sd">        :return: the activations of the last layer of the neural network.</span>
</span><span id="MLP-286"><a href="#MLP-286"><span class="linenos">286</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-287"><a href="#MLP-287"><span class="linenos">287</span></a>        <span class="c1"># initializes input as the 0-th layer of activations</span>
</span><span id="MLP-288"><a href="#MLP-288"><span class="linenos">288</span></a>        <span class="n">a0</span> <span class="o">=</span> <span class="n">input_arr</span>
</span><span id="MLP-289"><a href="#MLP-289"><span class="linenos">289</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a0</span>
</span><span id="MLP-290"><a href="#MLP-290"><span class="linenos">290</span></a>        <span class="c1"># iterates through each layer of the MLP</span>
</span><span id="MLP-291"><a href="#MLP-291"><span class="linenos">291</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">param_lst</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP-292"><a href="#MLP-292"><span class="linenos">292</span></a>            <span class="c1"># gets activation func for current layer</span>
</span><span id="MLP-293"><a href="#MLP-293"><span class="linenos">293</span></a>            <span class="n">act_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-294"><a href="#MLP-294"><span class="linenos">294</span></a>            <span class="c1"># retrieves values of weights</span>
</span><span id="MLP-295"><a href="#MLP-295"><span class="linenos">295</span></a>            <span class="c1"># and biases of the layer</span>
</span><span id="MLP-296"><a href="#MLP-296"><span class="linenos">296</span></a>            <span class="n">w</span> <span class="o">=</span> <span class="n">param_lst</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-297"><a href="#MLP-297"><span class="linenos">297</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">param_lst</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP-298"><a href="#MLP-298"><span class="linenos">298</span></a>            <span class="c1"># applies weights</span>
</span><span id="MLP-299"><a href="#MLP-299"><span class="linenos">299</span></a>            <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</span><span id="MLP-300"><a href="#MLP-300"><span class="linenos">300</span></a>            <span class="c1"># applies biases</span>
</span><span id="MLP-301"><a href="#MLP-301"><span class="linenos">301</span></a>            <span class="n">a1</span> <span class="o">+=</span> <span class="n">b</span>
</span><span id="MLP-302"><a href="#MLP-302"><span class="linenos">302</span></a>            <span class="c1"># applies the activation function</span>
</span><span id="MLP-303"><a href="#MLP-303"><span class="linenos">303</span></a>            <span class="n">a1</span> <span class="o">=</span> <span class="n">act_func</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
</span><span id="MLP-304"><a href="#MLP-304"><span class="linenos">304</span></a>            <span class="c1"># saves the resulting activations</span>
</span><span id="MLP-305"><a href="#MLP-305"><span class="linenos">305</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">a1</span>
</span><span id="MLP-306"><a href="#MLP-306"><span class="linenos">306</span></a>            <span class="c1"># assigns the activations as</span>
</span><span id="MLP-307"><a href="#MLP-307"><span class="linenos">307</span></a>            <span class="c1"># an input for the next layer</span>
</span><span id="MLP-308"><a href="#MLP-308"><span class="linenos">308</span></a>            <span class="n">a0</span> <span class="o">=</span> <span class="n">a1</span>
</span><span id="MLP-309"><a href="#MLP-309"><span class="linenos">309</span></a>        <span class="c1"># returns activations of the last layer</span>
</span><span id="MLP-310"><a href="#MLP-310"><span class="linenos">310</span></a>        <span class="k">return</span> <span class="n">a0</span>
</span><span id="MLP-311"><a href="#MLP-311"><span class="linenos">311</span></a>    <span class="k">def</span> <span class="nf">_stochastic_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="p">):</span>
</span><span id="MLP-312"><a href="#MLP-312"><span class="linenos">312</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP-313"><a href="#MLP-313"><span class="linenos">313</span></a><span class="sd">        The function implements the stochastic gradient descent algorithm for a neural</span>
</span><span id="MLP-314"><a href="#MLP-314"><span class="linenos">314</span></a><span class="sd">        network, updating the parameters based on the computed gradients.</span>
</span><span id="MLP-315"><a href="#MLP-315"><span class="linenos">315</span></a><span class="sd">        It does the following operations:</span>
</span><span id="MLP-316"><a href="#MLP-316"><span class="linenos">316</span></a><span class="sd">            Keeps track of the total accumulated error value and the count of current samples.</span>
</span><span id="MLP-317"><a href="#MLP-317"><span class="linenos">317</span></a><span class="sd">            Shuffles the training data randomly and iterates through each data sample.</span>
</span><span id="MLP-318"><a href="#MLP-318"><span class="linenos">318</span></a><span class="sd">            For each sample, it </span>
</span><span id="MLP-319"><a href="#MLP-319"><span class="linenos">319</span></a><span class="sd">                1. splits the training data array into input and expected output subarrays;</span>
</span><span id="MLP-320"><a href="#MLP-320"><span class="linenos">320</span></a><span class="sd">                2. computes activations for each layer, and saves the last layer activations (actual output);</span>
</span><span id="MLP-321"><a href="#MLP-321"><span class="linenos">321</span></a><span class="sd">                3. sums up squares of error values of each activation in the last layer;</span>
</span><span id="MLP-322"><a href="#MLP-322"><span class="linenos">322</span></a><span class="sd">                4. performs backpropagation and computes the gradient.</span>
</span><span id="MLP-323"><a href="#MLP-323"><span class="linenos">323</span></a><span class="sd">            If the batch size is reached, it applies the accumulated gradient values to the parameters.</span>
</span><span id="MLP-324"><a href="#MLP-324"><span class="linenos">324</span></a><span class="sd">            Finally, it updates the current cost of the model as an average error value per one sample.</span>
</span><span id="MLP-325"><a href="#MLP-325"><span class="linenos">325</span></a><span class="sd">        </span>
</span><span id="MLP-326"><a href="#MLP-326"><span class="linenos">326</span></a><span class="sd">        :param batch_ratio: The `batch_ratio` parameter is a float value that represents the ratio of the</span>
</span><span id="MLP-327"><a href="#MLP-327"><span class="linenos">327</span></a><span class="sd">        total number of training samples that should be used in each batch. For example, if `batch_ratio` is</span>
</span><span id="MLP-328"><a href="#MLP-328"><span class="linenos">328</span></a><span class="sd">        set to `0.5`, it means that each batch will contain `50%` of the total training samples.</span>
</span><span id="MLP-329"><a href="#MLP-329"><span class="linenos">329</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-330"><a href="#MLP-330"><span class="linenos">330</span></a>        <span class="c1"># keeps the total accumulated error value</span>
</span><span id="MLP-331"><a href="#MLP-331"><span class="linenos">331</span></a>        <span class="n">total_err</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP-332"><a href="#MLP-332"><span class="linenos">332</span></a>        <span class="c1"># keeps the count of current samples</span>
</span><span id="MLP-333"><a href="#MLP-333"><span class="linenos">333</span></a>        <span class="n">sample_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP-334"><a href="#MLP-334"><span class="linenos">334</span></a>        <span class="c1"># shuffles the training data randomly</span>
</span><span id="MLP-335"><a href="#MLP-335"><span class="linenos">335</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>
</span><span id="MLP-336"><a href="#MLP-336"><span class="linenos">336</span></a>        <span class="c1"># iterates through each data sample</span>
</span><span id="MLP-337"><a href="#MLP-337"><span class="linenos">337</span></a>        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="MLP-338"><a href="#MLP-338"><span class="linenos">338</span></a>            <span class="c1"># splits training data array </span>
</span><span id="MLP-339"><a href="#MLP-339"><span class="linenos">339</span></a>            <span class="c1"># into input/expected output subarrays</span>
</span><span id="MLP-340"><a href="#MLP-340"><span class="linenos">340</span></a>            <span class="nb">input</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="MLP-341"><a href="#MLP-341"><span class="linenos">341</span></a>            <span class="n">expected</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">:</span> <span class="p">]])</span>
</span><span id="MLP-342"><a href="#MLP-342"><span class="linenos">342</span></a>
</span><span id="MLP-343"><a href="#MLP-343"><span class="linenos">343</span></a>            <span class="c1"># computes activations for each layer</span>
</span><span id="MLP-344"><a href="#MLP-344"><span class="linenos">344</span></a>            <span class="c1"># and saves the last layer activations</span>
</span><span id="MLP-345"><a href="#MLP-345"><span class="linenos">345</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="MLP-346"><a href="#MLP-346"><span class="linenos">346</span></a>
</span><span id="MLP-347"><a href="#MLP-347"><span class="linenos">347</span></a>            <span class="c1"># verifies that there is no mismatch between </span>
</span><span id="MLP-348"><a href="#MLP-348"><span class="linenos">348</span></a>            <span class="c1"># the sizes of the expected and actual output arrays</span>
</span><span id="MLP-349"><a href="#MLP-349"><span class="linenos">349</span></a>            <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">shape</span>
</span><span id="MLP-350"><a href="#MLP-350"><span class="linenos">350</span></a>
</span><span id="MLP-351"><a href="#MLP-351"><span class="linenos">351</span></a>            <span class="c1"># computes activation derivative values for the last layer</span>
</span><span id="MLP-352"><a href="#MLP-352"><span class="linenos">352</span></a>            <span class="c1"># as a difference between expected and actual outputs of MLP</span>
</span><span id="MLP-353"><a href="#MLP-353"><span class="linenos">353</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-354"><a href="#MLP-354"><span class="linenos">354</span></a>
</span><span id="MLP-355"><a href="#MLP-355"><span class="linenos">355</span></a>            <span class="c1"># sums up squares of error values </span>
</span><span id="MLP-356"><a href="#MLP-356"><span class="linenos">356</span></a>            <span class="c1"># of each activation in the last layer</span>
</span><span id="MLP-357"><a href="#MLP-357"><span class="linenos">357</span></a>            <span class="n">total_err</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">da_next</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MLP-358"><a href="#MLP-358"><span class="linenos">358</span></a>
</span><span id="MLP-359"><a href="#MLP-359"><span class="linenos">359</span></a>            <span class="c1"># performs backpropagation and computes the gradient</span>
</span><span id="MLP-360"><a href="#MLP-360"><span class="linenos">360</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_backprop</span><span class="p">(</span><span class="n">da_next</span><span class="p">)</span>
</span><span id="MLP-361"><a href="#MLP-361"><span class="linenos">361</span></a>
</span><span id="MLP-362"><a href="#MLP-362"><span class="linenos">362</span></a>            <span class="n">sample_counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="MLP-363"><a href="#MLP-363"><span class="linenos">363</span></a>            <span class="c1"># checks if the batch size is reached</span>
</span><span id="MLP-364"><a href="#MLP-364"><span class="linenos">364</span></a>            <span class="k">if</span> <span class="n">sample_counter</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">batch_ratio</span><span class="p">:</span>
</span><span id="MLP-365"><a href="#MLP-365"><span class="linenos">365</span></a>                <span class="c1"># applies accumulated gradient values to the parameters</span>
</span><span id="MLP-366"><a href="#MLP-366"><span class="linenos">366</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_grad</span><span class="p">(</span><span class="n">sample_counter</span><span class="p">)</span>
</span><span id="MLP-367"><a href="#MLP-367"><span class="linenos">367</span></a>                <span class="n">sample_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP-368"><a href="#MLP-368"><span class="linenos">368</span></a>
</span><span id="MLP-369"><a href="#MLP-369"><span class="linenos">369</span></a>        <span class="c1"># applies the gradient of the samples</span>
</span><span id="MLP-370"><a href="#MLP-370"><span class="linenos">370</span></a>        <span class="c1"># that didn&#39;t reach the size of the batch</span>
</span><span id="MLP-371"><a href="#MLP-371"><span class="linenos">371</span></a>        <span class="k">if</span> <span class="n">sample_counter</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MLP-372"><a href="#MLP-372"><span class="linenos">372</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_apply_grad</span><span class="p">(</span><span class="n">sample_counter</span><span class="p">)</span>
</span><span id="MLP-373"><a href="#MLP-373"><span class="linenos">373</span></a>
</span><span id="MLP-374"><a href="#MLP-374"><span class="linenos">374</span></a>        <span class="c1"># updates the current cost of the model </span>
</span><span id="MLP-375"><a href="#MLP-375"><span class="linenos">375</span></a>        <span class="c1"># as an average error value per 1 sample</span>
</span><span id="MLP-376"><a href="#MLP-376"><span class="linenos">376</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="n">total_err</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>    
</span><span id="MLP-377"><a href="#MLP-377"><span class="linenos">377</span></a>    <span class="k">def</span> <span class="nf">_backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">da_next</span><span class="p">):</span>
</span><span id="MLP-378"><a href="#MLP-378"><span class="linenos">378</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP-379"><a href="#MLP-379"><span class="linenos">379</span></a><span class="sd">        The function performs backpropagation to compute and accumulate the gradients of the weights and biases for later application to the parameters of a neural network.</span>
</span><span id="MLP-380"><a href="#MLP-380"><span class="linenos">380</span></a><span class="sd">        </span>
</span><span id="MLP-381"><a href="#MLP-381"><span class="linenos">381</span></a><span class="sd">        :param da_next: da_next is the derivative of the cost function with respect to the activations of</span>
</span><span id="MLP-382"><a href="#MLP-382"><span class="linenos">382</span></a><span class="sd">        the next layer. It represents the backpropagated error from the next layer</span>
</span><span id="MLP-383"><a href="#MLP-383"><span class="linenos">383</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-384"><a href="#MLP-384"><span class="linenos">384</span></a>        <span class="c1"># iterates through each layer backwards starting from the last one</span>
</span><span id="MLP-385"><a href="#MLP-385"><span class="linenos">385</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
</span><span id="MLP-386"><a href="#MLP-386"><span class="linenos">386</span></a>
</span><span id="MLP-387"><a href="#MLP-387"><span class="linenos">387</span></a>            <span class="c1"># gets the weights for the current layer</span>
</span><span id="MLP-388"><a href="#MLP-388"><span class="linenos">388</span></a>            <span class="n">layer_wghts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-389"><a href="#MLP-389"><span class="linenos">389</span></a>            
</span><span id="MLP-390"><a href="#MLP-390"><span class="linenos">390</span></a>            <span class="c1"># gets the derivative of activation func for the current layer</span>
</span><span id="MLP-391"><a href="#MLP-391"><span class="linenos">391</span></a>            <span class="n">der_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP-392"><a href="#MLP-392"><span class="linenos">392</span></a>
</span><span id="MLP-393"><a href="#MLP-393"><span class="linenos">393</span></a>            <span class="c1"># computes the derivative expression vectorized for all activations</span>
</span><span id="MLP-394"><a href="#MLP-394"><span class="linenos">394</span></a>            <span class="n">der_expr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lncosh_der</span><span class="p">(</span><span class="n">da_next</span><span class="p">)</span> <span class="o">*</span> <span class="n">der_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</span><span id="MLP-395"><a href="#MLP-395"><span class="linenos">395</span></a>
</span><span id="MLP-396"><a href="#MLP-396"><span class="linenos">396</span></a>            <span class="c1"># computes bias derivatives for the entire layer</span>
</span><span id="MLP-397"><a href="#MLP-397"><span class="linenos">397</span></a>            <span class="n">db</span> <span class="o">=</span> <span class="n">der_expr</span>
</span><span id="MLP-398"><a href="#MLP-398"><span class="linenos">398</span></a>
</span><span id="MLP-399"><a href="#MLP-399"><span class="linenos">399</span></a>            <span class="c1"># computes weights derivatives for the entire layer</span>
</span><span id="MLP-400"><a href="#MLP-400"><span class="linenos">400</span></a>            <span class="n">dw</span> <span class="o">=</span> <span class="n">der_expr</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-401"><a href="#MLP-401"><span class="linenos">401</span></a>            <span class="n">dw</span> <span class="o">=</span> <span class="n">dw</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</span><span id="MLP-402"><a href="#MLP-402"><span class="linenos">402</span></a>
</span><span id="MLP-403"><a href="#MLP-403"><span class="linenos">403</span></a>            <span class="c1"># computes the gradient for current layer</span>
</span><span id="MLP-404"><a href="#MLP-404"><span class="linenos">404</span></a>            <span class="n">grad_mxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dw</span><span class="p">,</span> <span class="n">db</span><span class="p">]</span>
</span><span id="MLP-405"><a href="#MLP-405"><span class="linenos">405</span></a>
</span><span id="MLP-406"><a href="#MLP-406"><span class="linenos">406</span></a>            <span class="c1"># adds up the gradient values of the current sample</span>
</span><span id="MLP-407"><a href="#MLP-407"><span class="linenos">407</span></a>            <span class="c1"># to the gradient sum of previously computed samples</span>
</span><span id="MLP-408"><a href="#MLP-408"><span class="linenos">408</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">]):</span>
</span><span id="MLP-409"><a href="#MLP-409"><span class="linenos">409</span></a>                <span class="n">mx</span> <span class="o">+=</span> <span class="n">grad_mxs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span><span id="MLP-410"><a href="#MLP-410"><span class="linenos">410</span></a>
</span><span id="MLP-411"><a href="#MLP-411"><span class="linenos">411</span></a>            <span class="c1"># computes activation derivatives for the previous layer</span>
</span><span id="MLP-412"><a href="#MLP-412"><span class="linenos">412</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">@</span> <span class="n">layer_wghts</span><span class="o">.</span><span class="n">T</span>
</span><span id="MLP-413"><a href="#MLP-413"><span class="linenos">413</span></a>    <span class="k">def</span> <span class="nf">_apply_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
</span><span id="MLP-414"><a href="#MLP-414"><span class="linenos">414</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP-415"><a href="#MLP-415"><span class="linenos">415</span></a><span class="sd">        The function applies the stored gradient values to the parameters of a neural network.</span>
</span><span id="MLP-416"><a href="#MLP-416"><span class="linenos">416</span></a><span class="sd">        </span>
</span><span id="MLP-417"><a href="#MLP-417"><span class="linenos">417</span></a><span class="sd">        :param num_samples: The `num_samples` parameter represents the number of samples used to compute the</span>
</span><span id="MLP-418"><a href="#MLP-418"><span class="linenos">418</span></a><span class="sd">        gradient. It is used to normalize the gradient update step by dividing it by the number of samples.</span>
</span><span id="MLP-419"><a href="#MLP-419"><span class="linenos">419</span></a><span class="sd">        This helps to ensure that the gradient update is not too large or too small, regardless of the</span>
</span><span id="MLP-420"><a href="#MLP-420"><span class="linenos">420</span></a><span class="sd">        number of samples used in the batch.</span>
</span><span id="MLP-421"><a href="#MLP-421"><span class="linenos">421</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-422"><a href="#MLP-422"><span class="linenos">422</span></a>        <span class="c1"># iterates through parameters</span>
</span><span id="MLP-423"><a href="#MLP-423"><span class="linenos">423</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">param_mxs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP-424"><a href="#MLP-424"><span class="linenos">424</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param_mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_mxs</span><span class="p">):</span>
</span><span id="MLP-425"><a href="#MLP-425"><span class="linenos">425</span></a>                <span class="c1"># gets the gradient values</span>
</span><span id="MLP-426"><a href="#MLP-426"><span class="linenos">426</span></a>                <span class="n">grad_mx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</span><span id="MLP-427"><a href="#MLP-427"><span class="linenos">427</span></a>                <span class="c1"># subtracts the gradient from the parameters</span>
</span><span id="MLP-428"><a href="#MLP-428"><span class="linenos">428</span></a>                <span class="n">param_mx</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_mx</span> <span class="o">/</span> <span class="n">num_samples</span><span class="p">)</span>
</span><span id="MLP-429"><a href="#MLP-429"><span class="linenos">429</span></a>                <span class="c1"># resets the gradient to zero for next training cycle</span>
</span><span id="MLP-430"><a href="#MLP-430"><span class="linenos">430</span></a>                <span class="n">grad_mx</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP-431"><a href="#MLP-431"><span class="linenos">431</span></a>
</span><span id="MLP-432"><a href="#MLP-432"><span class="linenos">432</span></a>    <span class="c1"># API method for setting up the learning parameters</span>
</span><span id="MLP-433"><a href="#MLP-433"><span class="linenos">433</span></a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">_stochastic_descent</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plot_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">plot_dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">upd_interval</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">with_full_report</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_cost</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="MLP-434"><a href="#MLP-434"><span class="linenos">434</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-435"><a href="#MLP-435"><span class="linenos">435</span></a><span class="sd">        The function trains a neural network for a specified number of epochs using a chosen</span>
</span><span id="MLP-436"><a href="#MLP-436"><span class="linenos">436</span></a><span class="sd">        algorithm, reports the state of the model, plots the cost function, and computes the total run time.</span>
</span><span id="MLP-437"><a href="#MLP-437"><span class="linenos">437</span></a><span class="sd">        </span>
</span><span id="MLP-438"><a href="#MLP-438"><span class="linenos">438</span></a><span class="sd">        :param num_epochs: The number of epochs, which is the number of times the neural network will be</span>
</span><span id="MLP-439"><a href="#MLP-439"><span class="linenos">439</span></a><span class="sd">        trained on the dataset</span>
</span><span id="MLP-440"><a href="#MLP-440"><span class="linenos">440</span></a><span class="sd">        :param algorithm: Determines the learning algorithm to be used for training the neural network. </span>
</span><span id="MLP-441"><a href="#MLP-441"><span class="linenos">441</span></a><span class="sd">        The default value is `stochastic_descent`, but you can pass any other function that implements a </span>
</span><span id="MLP-442"><a href="#MLP-442"><span class="linenos">442</span></a><span class="sd">        different learning algorithm</span>
</span><span id="MLP-443"><a href="#MLP-443"><span class="linenos">443</span></a><span class="sd">        :param rate: The learning rate of the neural network. It determines how quickly the parameters of the </span>
</span><span id="MLP-444"><a href="#MLP-444"><span class="linenos">444</span></a><span class="sd">        network are updated during training. A higher learning rate can result in faster convergence, but it </span>
</span><span id="MLP-445"><a href="#MLP-445"><span class="linenos">445</span></a><span class="sd">        may also cause the network to overshoot the optimal solution. Defaults to `1` (optional)</span>
</span><span id="MLP-446"><a href="#MLP-446"><span class="linenos">446</span></a><span class="sd">        :param batch_ratio: Determines the ratio of the training data used in each iteration of the learning </span>
</span><span id="MLP-447"><a href="#MLP-447"><span class="linenos">447</span></a><span class="sd">        algorithm</span>
</span><span id="MLP-448"><a href="#MLP-448"><span class="linenos">448</span></a><span class="sd">        :param threshold: Is used to determine when to stop the learning process. If the current cost of the </span>
</span><span id="MLP-449"><a href="#MLP-449"><span class="linenos">449</span></a><span class="sd">        model falls below the threshold value, the learning process will stop</span>
</span><span id="MLP-450"><a href="#MLP-450"><span class="linenos">450</span></a><span class="sd">        :param stop: Determines whether the learning process should stop when the threshold cost is reached. </span>
</span><span id="MLP-451"><a href="#MLP-451"><span class="linenos">451</span></a><span class="sd">        If set to `True`, the learning process will stop when the cost falls below the specified threshold. </span>
</span><span id="MLP-452"><a href="#MLP-452"><span class="linenos">452</span></a><span class="sd">        Otherwise, the learning process will continue until all epochs are completed, regardless of the cost. </span>
</span><span id="MLP-453"><a href="#MLP-453"><span class="linenos">453</span></a><span class="sd">        Defaults to `True` (optional)</span>
</span><span id="MLP-454"><a href="#MLP-454"><span class="linenos">454</span></a><span class="sd">        :param plot_static: Determines whether to plot the graph of the cost function after training the neural </span>
</span><span id="MLP-455"><a href="#MLP-455"><span class="linenos">455</span></a><span class="sd">        network. If set to `True`, the graph will be plotted using the `static_plot` method. Defaults to `False` </span>
</span><span id="MLP-456"><a href="#MLP-456"><span class="linenos">456</span></a><span class="sd">        (optional)</span>
</span><span id="MLP-457"><a href="#MLP-457"><span class="linenos">457</span></a><span class="sd">        :param plot_dynamic: Determines whether or not to plot the cost function in real-time during the learning </span>
</span><span id="MLP-458"><a href="#MLP-458"><span class="linenos">458</span></a><span class="sd">        process. If set to `True`, a separate process will be created to handle the plotting. Defaults to `False` </span>
</span><span id="MLP-459"><a href="#MLP-459"><span class="linenos">459</span></a><span class="sd">        (optional)</span>
</span><span id="MLP-460"><a href="#MLP-460"><span class="linenos">460</span></a><span class="sd">        :param upd_interval: Determines the number of epochs after which the dynamic cost plot is updated during </span>
</span><span id="MLP-461"><a href="#MLP-461"><span class="linenos">461</span></a><span class="sd">        the learning process. Defaults to `20` (optional)</span>
</span><span id="MLP-462"><a href="#MLP-462"><span class="linenos">462</span></a><span class="sd">        :param with_full_report: Determines whether to include a full report of the neural network&#39;s state after </span>
</span><span id="MLP-463"><a href="#MLP-463"><span class="linenos">463</span></a><span class="sd">        training. If set to `True`, the full report will be printed to the console. Otherwise, only a summary </span>
</span><span id="MLP-464"><a href="#MLP-464"><span class="linenos">464</span></a><span class="sd">        of the neural network&#39;s state will be printed. Defaults to `False` (optional)</span>
</span><span id="MLP-465"><a href="#MLP-465"><span class="linenos">465</span></a><span class="sd">        :param return_cost: Determines whether the function should return the list of cost values. If set to </span>
</span><span id="MLP-466"><a href="#MLP-466"><span class="linenos">466</span></a><span class="sd">        `True`, the function will return a list of cost values for each epoch. Defaults to `False` (optional)</span>
</span><span id="MLP-467"><a href="#MLP-467"><span class="linenos">467</span></a><span class="sd">        :return: If the `return_cost` parameter is set to `True`, it returns a list of cost values. Otherwise it </span>
</span><span id="MLP-468"><a href="#MLP-468"><span class="linenos">468</span></a><span class="sd">        returns `None`</span>
</span><span id="MLP-469"><a href="#MLP-469"><span class="linenos">469</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-470"><a href="#MLP-470"><span class="linenos">470</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">plot_dynamic</span> <span class="ow">and</span> <span class="n">plot_static</span><span class="p">),</span> <span class="s2">&quot;Incorrect plotting mode setup&quot;</span>
</span><span id="MLP-471"><a href="#MLP-471"><span class="linenos">471</span></a>        <span class="c1"># initializes threshold flag</span>
</span><span id="MLP-472"><a href="#MLP-472"><span class="linenos">472</span></a>        <span class="n">thresh_flag</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="MLP-473"><a href="#MLP-473"><span class="linenos">473</span></a>        <span class="c1"># sets the learning rate</span>
</span><span id="MLP-474"><a href="#MLP-474"><span class="linenos">474</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">rate</span>
</span><span id="MLP-475"><a href="#MLP-475"><span class="linenos">475</span></a>
</span><span id="MLP-476"><a href="#MLP-476"><span class="linenos">476</span></a>        <span class="c1"># reports the initial state of the MLP</span>
</span><span id="MLP-477"><a href="#MLP-477"><span class="linenos">477</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt;&lt;&lt; INITIAL STATE &gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="MLP-478"><a href="#MLP-478"><span class="linenos">478</span></a>        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">(</span><span class="n">with_full_report</span><span class="p">))</span>
</span><span id="MLP-479"><a href="#MLP-479"><span class="linenos">479</span></a>
</span><span id="MLP-480"><a href="#MLP-480"><span class="linenos">480</span></a>        <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="MLP-481"><a href="#MLP-481"><span class="linenos">481</span></a>            <span class="c1"># initializes table for cost/epoch</span>
</span><span id="MLP-482"><a href="#MLP-482"><span class="linenos">482</span></a>            <span class="n">cost_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-483"><a href="#MLP-483"><span class="linenos">483</span></a>        <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>  
</span><span id="MLP-484"><a href="#MLP-484"><span class="linenos">484</span></a>            <span class="c1"># initializes lists for plotting</span>
</span><span id="MLP-485"><a href="#MLP-485"><span class="linenos">485</span></a>            <span class="n">x_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-486"><a href="#MLP-486"><span class="linenos">486</span></a>            <span class="n">y_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-487"><a href="#MLP-487"><span class="linenos">487</span></a>        <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="MLP-488"><a href="#MLP-488"><span class="linenos">488</span></a>            <span class="c1"># creates a queue for communication with the plotting process</span>
</span><span id="MLP-489"><a href="#MLP-489"><span class="linenos">489</span></a>            <span class="n">queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">()</span>
</span><span id="MLP-490"><a href="#MLP-490"><span class="linenos">490</span></a>            <span class="c1"># creates the plotting process</span>
</span><span id="MLP-491"><a href="#MLP-491"><span class="linenos">491</span></a>            <span class="n">plotting_process</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_plot</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">queue</span><span class="p">,))</span>
</span><span id="MLP-492"><a href="#MLP-492"><span class="linenos">492</span></a>            <span class="n">plotting_process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span id="MLP-493"><a href="#MLP-493"><span class="linenos">493</span></a>
</span><span id="MLP-494"><a href="#MLP-494"><span class="linenos">494</span></a>        <span class="c1"># initializes the progress bar</span>
</span><span id="MLP-495"><a href="#MLP-495"><span class="linenos">495</span></a>        <span class="n">EMPTY</span><span class="p">,</span> <span class="n">COMPLETE</span> <span class="o">=</span> <span class="s1">&#39; - &#39;</span><span class="p">,</span> <span class="s1">&#39; # &#39;</span>
</span><span id="MLP-496"><a href="#MLP-496"><span class="linenos">496</span></a>        <span class="n">BAR_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>
</span><span id="MLP-497"><a href="#MLP-497"><span class="linenos">497</span></a>        <span class="n">CURSOR_UP</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[1A&#39;</span>
</span><span id="MLP-498"><a href="#MLP-498"><span class="linenos">498</span></a>        <span class="n">CLEAR</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\x1b</span><span class="s1">[2K&#39;</span>
</span><span id="MLP-499"><a href="#MLP-499"><span class="linenos">499</span></a>        <span class="n">CLEAR_LINE</span> <span class="o">=</span> <span class="n">CURSOR_UP</span> <span class="o">+</span> <span class="n">CLEAR</span>
</span><span id="MLP-500"><a href="#MLP-500"><span class="linenos">500</span></a>        <span class="n">counter</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="MLP-501"><a href="#MLP-501"><span class="linenos">501</span></a>        <span class="n">ratio</span> <span class="o">=</span> <span class="n">num_epochs</span> <span class="o">/</span> <span class="n">BAR_LENGTH</span>
</span><span id="MLP-502"><a href="#MLP-502"><span class="linenos">502</span></a>        <span class="n">progress</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">EMPTY</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">BAR_LENGTH</span><span class="si">}</span><span class="s2">]&quot;</span>
</span><span id="MLP-503"><a href="#MLP-503"><span class="linenos">503</span></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt;&lt;&lt; LEARNING PROGRESS &gt;&gt;&gt;</span><span class="se">\n</span><span class="si">{</span><span class="n">progress</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="MLP-504"><a href="#MLP-504"><span class="linenos">504</span></a>
</span><span id="MLP-505"><a href="#MLP-505"><span class="linenos">505</span></a>        <span class="n">start_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="MLP-506"><a href="#MLP-506"><span class="linenos">506</span></a>        <span class="c1"># repeats the learning procedure &lt;num_epochs&gt; times</span>
</span><span id="MLP-507"><a href="#MLP-507"><span class="linenos">507</span></a>        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span><span id="MLP-508"><a href="#MLP-508"><span class="linenos">508</span></a>            <span class="c1"># executes the selected algorithm</span>
</span><span id="MLP-509"><a href="#MLP-509"><span class="linenos">509</span></a>            <span class="n">algorithm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="p">)</span>
</span><span id="MLP-510"><a href="#MLP-510"><span class="linenos">510</span></a>            <span class="c1"># update the progress bar if needed</span>
</span><span id="MLP-511"><a href="#MLP-511"><span class="linenos">511</span></a>            <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">ratio</span><span class="o">*</span><span class="n">counter</span><span class="p">:</span>
</span><span id="MLP-512"><a href="#MLP-512"><span class="linenos">512</span></a>                <span class="nb">print</span><span class="p">(</span><span class="n">CLEAR_LINE</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</span><span id="MLP-513"><a href="#MLP-513"><span class="linenos">513</span></a>                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">COMPLETE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">counter</span><span class="si">}{</span><span class="n">EMPTY</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">BAR_LENGTH</span><span class="o">-</span><span class="n">counter</span><span class="p">)</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</span><span id="MLP-514"><a href="#MLP-514"><span class="linenos">514</span></a>                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="MLP-515"><a href="#MLP-515"><span class="linenos">515</span></a>
</span><span id="MLP-516"><a href="#MLP-516"><span class="linenos">516</span></a>            <span class="c1"># gets the current cost of the model</span>
</span><span id="MLP-517"><a href="#MLP-517"><span class="linenos">517</span></a>            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span>
</span><span id="MLP-518"><a href="#MLP-518"><span class="linenos">518</span></a>            <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="MLP-519"><a href="#MLP-519"><span class="linenos">519</span></a>                <span class="c1"># saves the values for the cost table</span>
</span><span id="MLP-520"><a href="#MLP-520"><span class="linenos">520</span></a>                <span class="n">cost_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="MLP-521"><a href="#MLP-521"><span class="linenos">521</span></a>            <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>
</span><span id="MLP-522"><a href="#MLP-522"><span class="linenos">522</span></a>                <span class="c1"># saves the values for the plot</span>
</span><span id="MLP-523"><a href="#MLP-523"><span class="linenos">523</span></a>                <span class="n">x_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="MLP-524"><a href="#MLP-524"><span class="linenos">524</span></a>                <span class="n">y_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="MLP-525"><a href="#MLP-525"><span class="linenos">525</span></a>            <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="MLP-526"><a href="#MLP-526"><span class="linenos">526</span></a>                <span class="c1"># updates the cost plot every &lt;upd_interval&gt; cycles</span>
</span><span id="MLP-527"><a href="#MLP-527"><span class="linenos">527</span></a>                <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">upd_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MLP-528"><a href="#MLP-528"><span class="linenos">528</span></a>                    <span class="c1"># sends data to the plotting process</span>
</span><span id="MLP-529"><a href="#MLP-529"><span class="linenos">529</span></a>                    <span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>
</span><span id="MLP-530"><a href="#MLP-530"><span class="linenos">530</span></a>
</span><span id="MLP-531"><a href="#MLP-531"><span class="linenos">531</span></a>            <span class="c1"># detects the time when the threshold cost is reached</span>
</span><span id="MLP-532"><a href="#MLP-532"><span class="linenos">532</span></a>            <span class="k">if</span> <span class="n">cost</span> <span class="o">&lt;</span> <span class="n">threshold</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">thresh_flag</span><span class="p">:</span>
</span><span id="MLP-533"><a href="#MLP-533"><span class="linenos">533</span></a>                <span class="n">thresh_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="MLP-534"><a href="#MLP-534"><span class="linenos">534</span></a>                <span class="n">thresh_iter</span> <span class="o">=</span> <span class="n">epoch</span>
</span><span id="MLP-535"><a href="#MLP-535"><span class="linenos">535</span></a>                <span class="n">thresh_flag</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="MLP-536"><a href="#MLP-536"><span class="linenos">536</span></a>                <span class="c1"># stops learning</span>
</span><span id="MLP-537"><a href="#MLP-537"><span class="linenos">537</span></a>                <span class="k">if</span> <span class="n">stop</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
</span><span id="MLP-538"><a href="#MLP-538"><span class="linenos">538</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="n">CLEAR_LINE</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</span><span id="MLP-539"><a href="#MLP-539"><span class="linenos">539</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">COMPLETE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">BAR_LENGTH</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</span><span id="MLP-540"><a href="#MLP-540"><span class="linenos">540</span></a>                    <span class="k">break</span>
</span><span id="MLP-541"><a href="#MLP-541"><span class="linenos">541</span></a>        
</span><span id="MLP-542"><a href="#MLP-542"><span class="linenos">542</span></a>        <span class="n">end_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="MLP-543"><a href="#MLP-543"><span class="linenos">543</span></a>
</span><span id="MLP-544"><a href="#MLP-544"><span class="linenos">544</span></a>        <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="MLP-545"><a href="#MLP-545"><span class="linenos">545</span></a>            <span class="c1"># signals the plot process to terminate</span>
</span><span id="MLP-546"><a href="#MLP-546"><span class="linenos">546</span></a>            <span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</span><span id="MLP-547"><a href="#MLP-547"><span class="linenos">547</span></a>
</span><span id="MLP-548"><a href="#MLP-548"><span class="linenos">548</span></a>        <span class="c1"># reports the final state of the model</span>
</span><span id="MLP-549"><a href="#MLP-549"><span class="linenos">549</span></a>        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">(</span><span class="n">with_full_report</span><span class="p">))</span>
</span><span id="MLP-550"><a href="#MLP-550"><span class="linenos">550</span></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Time elapsed: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">end_t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_t</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">s.&quot;</span><span class="p">)</span>
</span><span id="MLP-551"><a href="#MLP-551"><span class="linenos">551</span></a>        <span class="c1"># prints how much time was needed to pass the cost threshold</span>
</span><span id="MLP-552"><a href="#MLP-552"><span class="linenos">552</span></a>        <span class="k">if</span> <span class="n">thresh_flag</span><span class="p">:</span>
</span><span id="MLP-553"><a href="#MLP-553"><span class="linenos">553</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">threshold</span><span class="si">}</span><span class="s2">% of cost was reached in </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">thresh_t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_t</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">s (</span><span class="si">{</span><span class="n">thresh_iter</span><span class="si">}</span><span class="s2"> iterations).&quot;</span><span class="p">)</span>
</span><span id="MLP-554"><a href="#MLP-554"><span class="linenos">554</span></a>
</span><span id="MLP-555"><a href="#MLP-555"><span class="linenos">555</span></a>        <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>
</span><span id="MLP-556"><a href="#MLP-556"><span class="linenos">556</span></a>            <span class="c1"># plots the cost</span>
</span><span id="MLP-557"><a href="#MLP-557"><span class="linenos">557</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_static_plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
</span><span id="MLP-558"><a href="#MLP-558"><span class="linenos">558</span></a>
</span><span id="MLP-559"><a href="#MLP-559"><span class="linenos">559</span></a>        <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="MLP-560"><a href="#MLP-560"><span class="linenos">560</span></a>            <span class="k">return</span> <span class="n">cost_vals</span>
</span><span id="MLP-561"><a href="#MLP-561"><span class="linenos">561</span></a>
</span><span id="MLP-562"><a href="#MLP-562"><span class="linenos">562</span></a>    <span class="c1"># methods for plotting the cost</span>
</span><span id="MLP-563"><a href="#MLP-563"><span class="linenos">563</span></a>    <span class="k">def</span> <span class="nf">_static_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
</span><span id="MLP-564"><a href="#MLP-564"><span class="linenos">564</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP-565"><a href="#MLP-565"><span class="linenos">565</span></a><span class="sd">        The function plots the cost of a model against the number of training iterations.</span>
</span><span id="MLP-566"><a href="#MLP-566"><span class="linenos">566</span></a><span class="sd">        </span>
</span><span id="MLP-567"><a href="#MLP-567"><span class="linenos">567</span></a><span class="sd">        :param x_vals: Is a list or array of values representing the number of learning iterations. </span>
</span><span id="MLP-568"><a href="#MLP-568"><span class="linenos">568</span></a><span class="sd">        These values will be plotted on the x-axis of the graph</span>
</span><span id="MLP-569"><a href="#MLP-569"><span class="linenos">569</span></a><span class="sd">        :param y_vals: Represents the values of the cost function for each corresponding value </span>
</span><span id="MLP-570"><a href="#MLP-570"><span class="linenos">570</span></a><span class="sd">        in the `x_vals` parameter. These values are used to plot the cost vs. number of training </span>
</span><span id="MLP-571"><a href="#MLP-571"><span class="linenos">571</span></a><span class="sd">        iterations</span>
</span><span id="MLP-572"><a href="#MLP-572"><span class="linenos">572</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-573"><a href="#MLP-573"><span class="linenos">573</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</span><span id="MLP-574"><a href="#MLP-574"><span class="linenos">574</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of learning iterations&#39;</span><span class="p">)</span>
</span><span id="MLP-575"><a href="#MLP-575"><span class="linenos">575</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost of the model&#39;</span><span class="p">)</span>
</span><span id="MLP-576"><a href="#MLP-576"><span class="linenos">576</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="MLP-577"><a href="#MLP-577"><span class="linenos">577</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span id="MLP-578"><a href="#MLP-578"><span class="linenos">578</span></a>    <span class="k">def</span> <span class="nf">_dynamic_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span><span id="MLP-579"><a href="#MLP-579"><span class="linenos">579</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP-580"><a href="#MLP-580"><span class="linenos">580</span></a><span class="sd">        The function plots the cost function over the number of learning iterations in real-time as the data comes in through </span>
</span><span id="MLP-581"><a href="#MLP-581"><span class="linenos">581</span></a><span class="sd">        a queue. It updates the plot with each new piece of data received through the queue, allowing for live visualization </span>
</span><span id="MLP-582"><a href="#MLP-582"><span class="linenos">582</span></a><span class="sd">        of the training process. The plot remains open and dynamically updates until a `None` value is received through the </span>
</span><span id="MLP-583"><a href="#MLP-583"><span class="linenos">583</span></a><span class="sd">        queue.</span>
</span><span id="MLP-584"><a href="#MLP-584"><span class="linenos">584</span></a>
</span><span id="MLP-585"><a href="#MLP-585"><span class="linenos">585</span></a><span class="sd">        :param args: The first element of `args` is expected to be a queue that provides tuples of `(iteration, cost)`, where iteration </span>
</span><span id="MLP-586"><a href="#MLP-586"><span class="linenos">586</span></a><span class="sd">        is an integer representing the iteration number, and cost is a float representing the cost value at that iteration</span>
</span><span id="MLP-587"><a href="#MLP-587"><span class="linenos">587</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-588"><a href="#MLP-588"><span class="linenos">588</span></a>        <span class="c1"># gets the queue</span>
</span><span id="MLP-589"><a href="#MLP-589"><span class="linenos">589</span></a>        <span class="n">queue</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-590"><a href="#MLP-590"><span class="linenos">590</span></a>        <span class="c1"># small constant for scaling of axes</span>
</span><span id="MLP-591"><a href="#MLP-591"><span class="linenos">591</span></a>        <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>
</span><span id="MLP-592"><a href="#MLP-592"><span class="linenos">592</span></a>        <span class="c1"># initializes the plot</span>
</span><span id="MLP-593"><a href="#MLP-593"><span class="linenos">593</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span id="MLP-594"><a href="#MLP-594"><span class="linenos">594</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">)</span>
</span><span id="MLP-595"><a href="#MLP-595"><span class="linenos">595</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span>
</span><span id="MLP-596"><a href="#MLP-596"><span class="linenos">596</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="MLP-597"><a href="#MLP-597"><span class="linenos">597</span></a>        
</span><span id="MLP-598"><a href="#MLP-598"><span class="linenos">598</span></a>        <span class="n">x_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-599"><a href="#MLP-599"><span class="linenos">599</span></a>        <span class="n">y_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-600"><a href="#MLP-600"><span class="linenos">600</span></a>
</span><span id="MLP-601"><a href="#MLP-601"><span class="linenos">601</span></a>        <span class="c1"># creates the line object</span>
</span><span id="MLP-602"><a href="#MLP-602"><span class="linenos">602</span></a>        <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</span><span id="MLP-603"><a href="#MLP-603"><span class="linenos">603</span></a>        <span class="n">cost_text</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
</span><span id="MLP-604"><a href="#MLP-604"><span class="linenos">604</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="MLP-605"><a href="#MLP-605"><span class="linenos">605</span></a>
</span><span id="MLP-606"><a href="#MLP-606"><span class="linenos">606</span></a>        <span class="k">try</span><span class="p">:</span>
</span><span id="MLP-607"><a href="#MLP-607"><span class="linenos">607</span></a>            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span><span id="MLP-608"><a href="#MLP-608"><span class="linenos">608</span></a>                <span class="c1"># checks if there&#39;s data in the queue</span>
</span><span id="MLP-609"><a href="#MLP-609"><span class="linenos">609</span></a>                <span class="k">if</span> <span class="ow">not</span> <span class="n">queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
</span><span id="MLP-610"><a href="#MLP-610"><span class="linenos">610</span></a>                    <span class="c1"># gets values from the queue</span>
</span><span id="MLP-611"><a href="#MLP-611"><span class="linenos">611</span></a>                    <span class="n">data</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span><span id="MLP-612"><a href="#MLP-612"><span class="linenos">612</span></a>                    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MLP-613"><a href="#MLP-613"><span class="linenos">613</span></a>                        <span class="k">break</span>
</span><span id="MLP-614"><a href="#MLP-614"><span class="linenos">614</span></a>                    
</span><span id="MLP-615"><a href="#MLP-615"><span class="linenos">615</span></a>                    <span class="c1"># stores the values for plotting</span>
</span><span id="MLP-616"><a href="#MLP-616"><span class="linenos">616</span></a>                    <span class="n">iteration</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">data</span>
</span><span id="MLP-617"><a href="#MLP-617"><span class="linenos">617</span></a>                    <span class="n">x_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
</span><span id="MLP-618"><a href="#MLP-618"><span class="linenos">618</span></a>                    <span class="n">y_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="MLP-619"><a href="#MLP-619"><span class="linenos">619</span></a>
</span><span id="MLP-620"><a href="#MLP-620"><span class="linenos">620</span></a>                    <span class="c1"># updates the xdata and ydata of the line</span>
</span><span id="MLP-621"><a href="#MLP-621"><span class="linenos">621</span></a>                    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
</span><span id="MLP-622"><a href="#MLP-622"><span class="linenos">622</span></a>
</span><span id="MLP-623"><a href="#MLP-623"><span class="linenos">623</span></a>                    <span class="c1"># sets appropriate plot limits</span>
</span><span id="MLP-624"><a href="#MLP-624"><span class="linenos">624</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span><span class="o">-</span><span class="n">eps</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span>
</span><span id="MLP-625"><a href="#MLP-625"><span class="linenos">625</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y_vals</span><span class="p">)</span><span class="o">-</span><span class="n">eps</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_vals</span><span class="p">)</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span>
</span><span id="MLP-626"><a href="#MLP-626"><span class="linenos">626</span></a>
</span><span id="MLP-627"><a href="#MLP-627"><span class="linenos">627</span></a>                    <span class="c1"># updates the cost text with the current value of y</span>
</span><span id="MLP-628"><a href="#MLP-628"><span class="linenos">628</span></a>                    <span class="n">cost_text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Current accuracy: </span><span class="si">{</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">cost</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</span><span id="MLP-629"><a href="#MLP-629"><span class="linenos">629</span></a>
</span><span id="MLP-630"><a href="#MLP-630"><span class="linenos">630</span></a>                    <span class="c1"># draws the updated plot</span>
</span><span id="MLP-631"><a href="#MLP-631"><span class="linenos">631</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</span><span id="MLP-632"><a href="#MLP-632"><span class="linenos">632</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># pauses for a short duration to display the plot</span>
</span><span id="MLP-633"><a href="#MLP-633"><span class="linenos">633</span></a>
</span><span id="MLP-634"><a href="#MLP-634"><span class="linenos">634</span></a>        <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
</span><span id="MLP-635"><a href="#MLP-635"><span class="linenos">635</span></a>            <span class="k">pass</span>
</span><span id="MLP-636"><a href="#MLP-636"><span class="linenos">636</span></a>        
</span><span id="MLP-637"><a href="#MLP-637"><span class="linenos">637</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span id="MLP-638"><a href="#MLP-638"><span class="linenos">638</span></a>
</span><span id="MLP-639"><a href="#MLP-639"><span class="linenos">639</span></a>    <span class="c1"># methods for storing a MLP object state in memory</span>
</span><span id="MLP-640"><a href="#MLP-640"><span class="linenos">640</span></a>    <span class="k">def</span> <span class="nf">export_mlp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">):</span>
</span><span id="MLP-641"><a href="#MLP-641"><span class="linenos">641</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-642"><a href="#MLP-642"><span class="linenos">642</span></a><span class="sd">        The function creates a dictionary, that contains the current parameters and activation</span>
</span><span id="MLP-643"><a href="#MLP-643"><span class="linenos">643</span></a><span class="sd">        functions of a neural network, and saves it to a file, which can be later used by the method </span>
</span><span id="MLP-644"><a href="#MLP-644"><span class="linenos">644</span></a><span class="sd">        `import_mlp` to restore the state of the neural network.</span>
</span><span id="MLP-645"><a href="#MLP-645"><span class="linenos">645</span></a><span class="sd">        </span>
</span><span id="MLP-646"><a href="#MLP-646"><span class="linenos">646</span></a><span class="sd">        :param data_path: Is a string that represents the path where the neural network image will be saved</span>
</span><span id="MLP-647"><a href="#MLP-647"><span class="linenos">647</span></a><span class="sd">        as a numpy file</span>
</span><span id="MLP-648"><a href="#MLP-648"><span class="linenos">648</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-649"><a href="#MLP-649"><span class="linenos">649</span></a>        <span class="n">mlp_state</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP-650"><a href="#MLP-650"><span class="linenos">650</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP-651"><a href="#MLP-651"><span class="linenos">651</span></a>            <span class="n">act_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>
</span><span id="MLP-652"><a href="#MLP-652"><span class="linenos">652</span></a>            <span class="n">mlp_state</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">act_func</span><span class="p">)</span>
</span><span id="MLP-653"><a href="#MLP-653"><span class="linenos">653</span></a>
</span><span id="MLP-654"><a href="#MLP-654"><span class="linenos">654</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">mlp_state</span><span class="p">)</span>
</span><span id="MLP-655"><a href="#MLP-655"><span class="linenos">655</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The image was successfully exported to &quot;</span> <span class="o">+</span> <span class="n">data_path</span><span class="p">)</span>
</span><span id="MLP-656"><a href="#MLP-656"><span class="linenos">656</span></a>    <span class="k">def</span> <span class="nf">import_mlp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">):</span>
</span><span id="MLP-657"><a href="#MLP-657"><span class="linenos">657</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-658"><a href="#MLP-658"><span class="linenos">658</span></a><span class="sd">        The function imports new parameters for a neural network and updates the corresponding class instance </span>
</span><span id="MLP-659"><a href="#MLP-659"><span class="linenos">659</span></a><span class="sd">        attributes. It collaborates with the method `export_mlp` to streamline the storage of neural networks.</span>
</span><span id="MLP-660"><a href="#MLP-660"><span class="linenos">660</span></a><span class="sd">        </span>
</span><span id="MLP-661"><a href="#MLP-661"><span class="linenos">661</span></a><span class="sd">        :param data_path: Is a string that represents the path to the file from which the new parameters for </span>
</span><span id="MLP-662"><a href="#MLP-662"><span class="linenos">662</span></a><span class="sd">        the neural network will be imported</span>
</span><span id="MLP-663"><a href="#MLP-663"><span class="linenos">663</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-664"><a href="#MLP-664"><span class="linenos">664</span></a>        <span class="c1"># read new parameters from the file</span>
</span><span id="MLP-665"><a href="#MLP-665"><span class="linenos">665</span></a>        <span class="n">new_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="s1">&#39;TRUE&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="MLP-666"><a href="#MLP-666"><span class="linenos">666</span></a>        <span class="c1"># creates temp dicts to store new configuration</span>
</span><span id="MLP-667"><a href="#MLP-667"><span class="linenos">667</span></a>        <span class="n">act_funcs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP-668"><a href="#MLP-668"><span class="linenos">668</span></a>        <span class="n">new_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP-669"><a href="#MLP-669"><span class="linenos">669</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">new_state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP-670"><a href="#MLP-670"><span class="linenos">670</span></a>            <span class="c1"># checks if there is act func</span>
</span><span id="MLP-671"><a href="#MLP-671"><span class="linenos">671</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="MLP-672"><a href="#MLP-672"><span class="linenos">672</span></a>                <span class="n">new_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-673"><a href="#MLP-673"><span class="linenos">673</span></a>                <span class="c1"># retrieves act func name</span>
</span><span id="MLP-674"><a href="#MLP-674"><span class="linenos">674</span></a>                <span class="n">act_funcs</span> <span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP-675"><a href="#MLP-675"><span class="linenos">675</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="MLP-676"><a href="#MLP-676"><span class="linenos">676</span></a>                <span class="n">new_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MLP-677"><a href="#MLP-677"><span class="linenos">677</span></a>                <span class="c1"># assigns the default act func</span>
</span><span id="MLP-678"><a href="#MLP-678"><span class="linenos">678</span></a>                <span class="n">act_funcs</span> <span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span>
</span><span id="MLP-679"><a href="#MLP-679"><span class="linenos">679</span></a>        <span class="c1"># updated weights and biases</span>
</span><span id="MLP-680"><a href="#MLP-680"><span class="linenos">680</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">new_params</span><span class="p">)</span>
</span><span id="MLP-681"><a href="#MLP-681"><span class="linenos">681</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span>
</span><span id="MLP-682"><a href="#MLP-682"><span class="linenos">682</span></a>        <span class="c1"># maps layers to act funcs</span>
</span><span id="MLP-683"><a href="#MLP-683"><span class="linenos">683</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_func_map</span><span class="p">(</span><span class="n">act_funcs</span><span class="p">)</span>
</span><span id="MLP-684"><a href="#MLP-684"><span class="linenos">684</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The image was successfully imported from &quot;</span> <span class="o">+</span> <span class="n">data_path</span><span class="p">)</span>
</span><span id="MLP-685"><a href="#MLP-685"><span class="linenos">685</span></a>
</span><span id="MLP-686"><a href="#MLP-686"><span class="linenos">686</span></a>    <span class="c1"># LEGACY METHODS (can be used but are outperformed by the current algorithms)</span>
</span><span id="MLP-687"><a href="#MLP-687"><span class="linenos">687</span></a>    <span class="k">def</span> <span class="nf">_legacy_finite_diff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
</span><span id="MLP-688"><a href="#MLP-688"><span class="linenos">688</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-689"><a href="#MLP-689"><span class="linenos">689</span></a><span class="sd">        The function uses the finite difference method to approximate the partial derivatives of each </span>
</span><span id="MLP-690"><a href="#MLP-690"><span class="linenos">690</span></a><span class="sd">        parameter and construct the gradient of the cost function, and then updates the parameter</span>
</span><span id="MLP-691"><a href="#MLP-691"><span class="linenos">691</span></a><span class="sd">        values accordingly.</span>
</span><span id="MLP-692"><a href="#MLP-692"><span class="linenos">692</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-693"><a href="#MLP-693"><span class="linenos">693</span></a>        <span class="c1"># computes the initial cost of the model</span>
</span><span id="MLP-694"><a href="#MLP-694"><span class="linenos">694</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">()</span>
</span><span id="MLP-695"><a href="#MLP-695"><span class="linenos">695</span></a>        <span class="c1"># dictionary that will store </span>
</span><span id="MLP-696"><a href="#MLP-696"><span class="linenos">696</span></a>        <span class="c1"># updated parameter values</span>
</span><span id="MLP-697"><a href="#MLP-697"><span class="linenos">697</span></a>        <span class="n">upd_vals</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP-698"><a href="#MLP-698"><span class="linenos">698</span></a>        <span class="c1"># iterates through each layer of parameters</span>
</span><span id="MLP-699"><a href="#MLP-699"><span class="linenos">699</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
</span><span id="MLP-700"><a href="#MLP-700"><span class="linenos">700</span></a>            <span class="n">mx_lst</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-701"><a href="#MLP-701"><span class="linenos">701</span></a>            <span class="k">for</span> <span class="n">mx_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span><span id="MLP-702"><a href="#MLP-702"><span class="linenos">702</span></a>                <span class="n">param_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="n">mx_idx</span><span class="p">]</span>
</span><span id="MLP-703"><a href="#MLP-703"><span class="linenos">703</span></a>                <span class="c1"># creates a copy of the current values</span>
</span><span id="MLP-704"><a href="#MLP-704"><span class="linenos">704</span></a>                <span class="n">new_vals</span> <span class="o">=</span> <span class="n">param_vals</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="MLP-705"><a href="#MLP-705"><span class="linenos">705</span></a>                <span class="c1"># iterates through each parameter value</span>
</span><span id="MLP-706"><a href="#MLP-706"><span class="linenos">706</span></a>                <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">([</span><span class="n">param_vals</span><span class="p">,</span> <span class="n">new_vals</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span> <span class="k">as</span> <span class="n">it</span><span class="p">:</span>
</span><span id="MLP-707"><a href="#MLP-707"><span class="linenos">707</span></a>                    <span class="k">for</span> <span class="n">param_val</span><span class="p">,</span> <span class="n">new_val</span> <span class="ow">in</span> <span class="n">it</span><span class="p">:</span>
</span><span id="MLP-708"><a href="#MLP-708"><span class="linenos">708</span></a>                        <span class="c1"># saves the original value</span>
</span><span id="MLP-709"><a href="#MLP-709"><span class="linenos">709</span></a>                        <span class="n">saved</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">param_val</span><span class="p">)</span>
</span><span id="MLP-710"><a href="#MLP-710"><span class="linenos">710</span></a>                        <span class="c1"># tweaks the parameter by &lt;_eps&gt;</span>
</span><span id="MLP-711"><a href="#MLP-711"><span class="linenos">711</span></a>                        <span class="n">param_val</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span>
</span><span id="MLP-712"><a href="#MLP-712"><span class="linenos">712</span></a>                        <span class="c1"># computes the derivate of the cost function</span>
</span><span id="MLP-713"><a href="#MLP-713"><span class="linenos">713</span></a>                        <span class="n">grad_val</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cost</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span>
</span><span id="MLP-714"><a href="#MLP-714"><span class="linenos">714</span></a>                        <span class="c1"># updates the parameter value (in the copy)</span>
</span><span id="MLP-715"><a href="#MLP-715"><span class="linenos">715</span></a>                        <span class="n">new_val</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="n">grad_val</span>
</span><span id="MLP-716"><a href="#MLP-716"><span class="linenos">716</span></a>                        <span class="c1"># restores the original value</span>
</span><span id="MLP-717"><a href="#MLP-717"><span class="linenos">717</span></a>                        <span class="n">param_val</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">saved</span>
</span><span id="MLP-718"><a href="#MLP-718"><span class="linenos">718</span></a>                <span class="c1"># adds the matrix with updated values to the list-layer</span>
</span><span id="MLP-719"><a href="#MLP-719"><span class="linenos">719</span></a>                <span class="n">mx_lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_vals</span><span class="p">)</span>
</span><span id="MLP-720"><a href="#MLP-720"><span class="linenos">720</span></a>            <span class="c1"># adds the layer to the final updated dictionary</span>
</span><span id="MLP-721"><a href="#MLP-721"><span class="linenos">721</span></a>            <span class="n">upd_vals</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">mx_lst</span>
</span><span id="MLP-722"><a href="#MLP-722"><span class="linenos">722</span></a>        <span class="c1"># the updated dictionary becomes the new parameter dictionary</span>
</span><span id="MLP-723"><a href="#MLP-723"><span class="linenos">723</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="n">upd_vals</span>
</span><span id="MLP-724"><a href="#MLP-724"><span class="linenos">724</span></a>    <span class="k">def</span> <span class="nf">_legacy_gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
</span><span id="MLP-725"><a href="#MLP-725"><span class="linenos">725</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-726"><a href="#MLP-726"><span class="linenos">726</span></a><span class="sd">        The function implements a backpropagation algorithm for training a neural network by iterating </span>
</span><span id="MLP-727"><a href="#MLP-727"><span class="linenos">727</span></a><span class="sd">        through each layer and instantly updating the weights and biases based on the computed derivatives.</span>
</span><span id="MLP-728"><a href="#MLP-728"><span class="linenos">728</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-729"><a href="#MLP-729"><span class="linenos">729</span></a>        <span class="n">total_err</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP-730"><a href="#MLP-730"><span class="linenos">730</span></a>        <span class="c1"># iterates through each data sample</span>
</span><span id="MLP-731"><a href="#MLP-731"><span class="linenos">731</span></a>        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="MLP-732"><a href="#MLP-732"><span class="linenos">732</span></a>            <span class="c1"># splits training data array </span>
</span><span id="MLP-733"><a href="#MLP-733"><span class="linenos">733</span></a>            <span class="c1"># into input/expected output subarrays</span>
</span><span id="MLP-734"><a href="#MLP-734"><span class="linenos">734</span></a>            <span class="nb">input</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="MLP-735"><a href="#MLP-735"><span class="linenos">735</span></a>            <span class="n">expected</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">:</span> <span class="p">]])</span>
</span><span id="MLP-736"><a href="#MLP-736"><span class="linenos">736</span></a>
</span><span id="MLP-737"><a href="#MLP-737"><span class="linenos">737</span></a>            <span class="c1"># computes activations for each layer</span>
</span><span id="MLP-738"><a href="#MLP-738"><span class="linenos">738</span></a>            <span class="c1"># and saves the last layer activations</span>
</span><span id="MLP-739"><a href="#MLP-739"><span class="linenos">739</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="MLP-740"><a href="#MLP-740"><span class="linenos">740</span></a>
</span><span id="MLP-741"><a href="#MLP-741"><span class="linenos">741</span></a>            <span class="c1"># verifies that there is no mismatch between </span>
</span><span id="MLP-742"><a href="#MLP-742"><span class="linenos">742</span></a>            <span class="c1"># the sizes of the expected and actual output arrays</span>
</span><span id="MLP-743"><a href="#MLP-743"><span class="linenos">743</span></a>            <span class="k">assert</span> <span class="n">expected</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span>
</span><span id="MLP-744"><a href="#MLP-744"><span class="linenos">744</span></a>
</span><span id="MLP-745"><a href="#MLP-745"><span class="linenos">745</span></a>            <span class="c1"># initializes a list that holds partial derivatives of</span>
</span><span id="MLP-746"><a href="#MLP-746"><span class="linenos">746</span></a>            <span class="c1"># activations of the current layer to the cost of the next layer</span>
</span><span id="MLP-747"><a href="#MLP-747"><span class="linenos">747</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-748"><a href="#MLP-748"><span class="linenos">748</span></a>            <span class="c1"># computes activation derivative values for the last layer</span>
</span><span id="MLP-749"><a href="#MLP-749"><span class="linenos">749</span></a>            <span class="c1"># as a difference between expected and actual outputs of MLP</span>
</span><span id="MLP-750"><a href="#MLP-750"><span class="linenos">750</span></a>            <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">expected</span><span class="p">])</span> <span class="k">as</span> <span class="n">it</span><span class="p">:</span>
</span><span id="MLP-751"><a href="#MLP-751"><span class="linenos">751</span></a>                <span class="k">for</span> <span class="n">out_val</span><span class="p">,</span> <span class="n">exp_val</span> <span class="ow">in</span> <span class="n">it</span><span class="p">:</span>
</span><span id="MLP-752"><a href="#MLP-752"><span class="linenos">752</span></a>                    <span class="n">da_next</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_val</span> <span class="o">-</span> <span class="n">exp_val</span><span class="p">)</span>
</span><span id="MLP-753"><a href="#MLP-753"><span class="linenos">753</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">da_next</span><span class="p">)</span>
</span><span id="MLP-754"><a href="#MLP-754"><span class="linenos">754</span></a>            
</span><span id="MLP-755"><a href="#MLP-755"><span class="linenos">755</span></a>            <span class="c1"># sums up error values of each activation in the last layer</span>
</span><span id="MLP-756"><a href="#MLP-756"><span class="linenos">756</span></a>            <span class="k">for</span> <span class="n">err_val</span> <span class="ow">in</span> <span class="n">da_next</span><span class="p">:</span>
</span><span id="MLP-757"><a href="#MLP-757"><span class="linenos">757</span></a>                <span class="n">total_err</span> <span class="o">+=</span> <span class="n">err_val</span> <span class="o">**</span> <span class="mi">2</span>
</span><span id="MLP-758"><a href="#MLP-758"><span class="linenos">758</span></a>        
</span><span id="MLP-759"><a href="#MLP-759"><span class="linenos">759</span></a>            <span class="c1"># iterates in reverse through each layer</span>
</span><span id="MLP-760"><a href="#MLP-760"><span class="linenos">760</span></a>            <span class="c1"># starting from the last one</span>
</span><span id="MLP-761"><a href="#MLP-761"><span class="linenos">761</span></a>            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
</span><span id="MLP-762"><a href="#MLP-762"><span class="linenos">762</span></a>                <span class="n">db_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-763"><a href="#MLP-763"><span class="linenos">763</span></a>                <span class="n">dw_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-764"><a href="#MLP-764"><span class="linenos">764</span></a>                <span class="n">da_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-765"><a href="#MLP-765"><span class="linenos">765</span></a>                <span class="c1"># gets the biases for the current layer</span>
</span><span id="MLP-766"><a href="#MLP-766"><span class="linenos">766</span></a>                <span class="n">layer_bss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP-767"><a href="#MLP-767"><span class="linenos">767</span></a>                <span class="c1"># gets the weights for the current layer</span>
</span><span id="MLP-768"><a href="#MLP-768"><span class="linenos">768</span></a>                <span class="n">layer_wghts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-769"><a href="#MLP-769"><span class="linenos">769</span></a>                <span class="c1"># iterates through each activation</span>
</span><span id="MLP-770"><a href="#MLP-770"><span class="linenos">770</span></a>                <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="MLP-771"><a href="#MLP-771"><span class="linenos">771</span></a>                    <span class="n">der_expr</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">da_next</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">act</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">act</span><span class="p">)</span>
</span><span id="MLP-772"><a href="#MLP-772"><span class="linenos">772</span></a>                    <span class="c1"># computes bias derivative</span>
</span><span id="MLP-773"><a href="#MLP-773"><span class="linenos">773</span></a>                    <span class="n">db</span> <span class="o">=</span> <span class="n">der_expr</span>
</span><span id="MLP-774"><a href="#MLP-774"><span class="linenos">774</span></a>                    <span class="n">db_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
</span><span id="MLP-775"><a href="#MLP-775"><span class="linenos">775</span></a>                    <span class="c1"># computes weights derivatives</span>
</span><span id="MLP-776"><a href="#MLP-776"><span class="linenos">776</span></a>                    <span class="n">dw_act</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-777"><a href="#MLP-777"><span class="linenos">777</span></a>                    <span class="k">for</span> <span class="n">prev_act</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="MLP-778"><a href="#MLP-778"><span class="linenos">778</span></a>                        <span class="n">dw</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">*</span> <span class="n">prev_act</span>
</span><span id="MLP-779"><a href="#MLP-779"><span class="linenos">779</span></a>                        <span class="n">dw_act</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dw</span><span class="p">)</span>
</span><span id="MLP-780"><a href="#MLP-780"><span class="linenos">780</span></a>                    <span class="n">dw_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dw_act</span><span class="p">)</span>
</span><span id="MLP-781"><a href="#MLP-781"><span class="linenos">781</span></a>                    <span class="c1"># computes activation derivatives for the previous layer</span>
</span><span id="MLP-782"><a href="#MLP-782"><span class="linenos">782</span></a>                    <span class="n">da_act</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-783"><a href="#MLP-783"><span class="linenos">783</span></a>                    <span class="n">curr_wghts</span> <span class="o">=</span> <span class="n">layer_wghts</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
</span><span id="MLP-784"><a href="#MLP-784"><span class="linenos">784</span></a>                    <span class="k">for</span> <span class="n">wght</span> <span class="ow">in</span> <span class="n">curr_wghts</span><span class="p">:</span>
</span><span id="MLP-785"><a href="#MLP-785"><span class="linenos">785</span></a>                        <span class="n">da</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">*</span> <span class="n">wght</span>
</span><span id="MLP-786"><a href="#MLP-786"><span class="linenos">786</span></a>                        <span class="n">da_act</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da</span><span class="p">)</span>
</span><span id="MLP-787"><a href="#MLP-787"><span class="linenos">787</span></a>                    <span class="n">da_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da_act</span><span class="p">)</span>
</span><span id="MLP-788"><a href="#MLP-788"><span class="linenos">788</span></a>
</span><span id="MLP-789"><a href="#MLP-789"><span class="linenos">789</span></a>                <span class="c1"># APPLYING THE GRADIENT</span>
</span><span id="MLP-790"><a href="#MLP-790"><span class="linenos">790</span></a>                <span class="c1"># substracts bias gradient</span>
</span><span id="MLP-791"><a href="#MLP-791"><span class="linenos">791</span></a>                <span class="n">bs_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">db_layer</span><span class="p">)</span>
</span><span id="MLP-792"><a href="#MLP-792"><span class="linenos">792</span></a>                <span class="n">layer_bss</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="n">bs_grad</span>
</span><span id="MLP-793"><a href="#MLP-793"><span class="linenos">793</span></a>                <span class="c1"># substracts weight gradient</span>
</span><span id="MLP-794"><a href="#MLP-794"><span class="linenos">794</span></a>                <span class="n">wght_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dw_layer</span><span class="p">)</span>
</span><span id="MLP-795"><a href="#MLP-795"><span class="linenos">795</span></a>                <span class="n">wght_grad</span> <span class="o">=</span> <span class="n">wght_grad</span><span class="o">.</span><span class="n">T</span>
</span><span id="MLP-796"><a href="#MLP-796"><span class="linenos">796</span></a>                <span class="n">layer_wghts</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="n">wght_grad</span>
</span><span id="MLP-797"><a href="#MLP-797"><span class="linenos">797</span></a>                
</span><span id="MLP-798"><a href="#MLP-798"><span class="linenos">798</span></a>                <span class="c1"># computes a sum of activation derivatives</span>
</span><span id="MLP-799"><a href="#MLP-799"><span class="linenos">799</span></a>                <span class="c1"># for each activation of the previous layer</span>
</span><span id="MLP-800"><a href="#MLP-800"><span class="linenos">800</span></a>                <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">da_layer</span><span class="p">)</span>
</span><span id="MLP-801"><a href="#MLP-801"><span class="linenos">801</span></a>                <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">da_next</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="MLP-802"><a href="#MLP-802"><span class="linenos">802</span></a>
</span><span id="MLP-803"><a href="#MLP-803"><span class="linenos">803</span></a>        <span class="c1"># updates the current cost of the model </span>
</span><span id="MLP-804"><a href="#MLP-804"><span class="linenos">804</span></a>        <span class="c1"># as an average error value per 1 sample</span>
</span><span id="MLP-805"><a href="#MLP-805"><span class="linenos">805</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="n">total_err</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>
</span><span id="MLP-806"><a href="#MLP-806"><span class="linenos">806</span></a>    <span class="k">def</span> <span class="nf">_legacy_backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">da_next</span><span class="p">):</span>
</span><span id="MLP-807"><a href="#MLP-807"><span class="linenos">807</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP-808"><a href="#MLP-808"><span class="linenos">808</span></a><span class="sd">        The function implements backpropagation by iterating through the layers in reverse order, computing</span>
</span><span id="MLP-809"><a href="#MLP-809"><span class="linenos">809</span></a><span class="sd">        the derivatives of the biases and weights, and updating the gradient for later use in updating the</span>
</span><span id="MLP-810"><a href="#MLP-810"><span class="linenos">810</span></a><span class="sd">        parameters of the neural network. On average, this backpropagation algorithm reaches 1% of cost 30 </span>
</span><span id="MLP-811"><a href="#MLP-811"><span class="linenos">811</span></a><span class="sd">        times faster than the &#39;_legacy_finite_diff&#39; method.</span>
</span><span id="MLP-812"><a href="#MLP-812"><span class="linenos">812</span></a><span class="sd">        </span>
</span><span id="MLP-813"><a href="#MLP-813"><span class="linenos">813</span></a><span class="sd">        :param da_next: An array of computed error values for the current output</span>
</span><span id="MLP-814"><a href="#MLP-814"><span class="linenos">814</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP-815"><a href="#MLP-815"><span class="linenos">815</span></a>        <span class="c1"># iterates through each layer backwards starting from the last one</span>
</span><span id="MLP-816"><a href="#MLP-816"><span class="linenos">816</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
</span><span id="MLP-817"><a href="#MLP-817"><span class="linenos">817</span></a>            <span class="n">db_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-818"><a href="#MLP-818"><span class="linenos">818</span></a>            <span class="n">dw_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-819"><a href="#MLP-819"><span class="linenos">819</span></a>            <span class="n">da_layer</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-820"><a href="#MLP-820"><span class="linenos">820</span></a>            <span class="c1"># gets the weights for the current layer</span>
</span><span id="MLP-821"><a href="#MLP-821"><span class="linenos">821</span></a>            <span class="n">layer_wghts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP-822"><a href="#MLP-822"><span class="linenos">822</span></a>            <span class="c1"># iterates through each activation</span>
</span><span id="MLP-823"><a href="#MLP-823"><span class="linenos">823</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="MLP-824"><a href="#MLP-824"><span class="linenos">824</span></a>                <span class="n">der_expr</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">da_next</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">act</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">act</span><span class="p">)</span>
</span><span id="MLP-825"><a href="#MLP-825"><span class="linenos">825</span></a>                <span class="c1"># computes bias derivative</span>
</span><span id="MLP-826"><a href="#MLP-826"><span class="linenos">826</span></a>                <span class="n">db</span> <span class="o">=</span> <span class="n">der_expr</span>
</span><span id="MLP-827"><a href="#MLP-827"><span class="linenos">827</span></a>                <span class="n">db_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
</span><span id="MLP-828"><a href="#MLP-828"><span class="linenos">828</span></a>                <span class="c1"># computes weights derivatives</span>
</span><span id="MLP-829"><a href="#MLP-829"><span class="linenos">829</span></a>                <span class="n">dw_act</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-830"><a href="#MLP-830"><span class="linenos">830</span></a>                <span class="k">for</span> <span class="n">prev_act</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="MLP-831"><a href="#MLP-831"><span class="linenos">831</span></a>                    <span class="n">dw</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">*</span> <span class="n">prev_act</span>
</span><span id="MLP-832"><a href="#MLP-832"><span class="linenos">832</span></a>                    <span class="n">dw_act</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dw</span><span class="p">)</span>
</span><span id="MLP-833"><a href="#MLP-833"><span class="linenos">833</span></a>                <span class="n">dw_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dw_act</span><span class="p">)</span>
</span><span id="MLP-834"><a href="#MLP-834"><span class="linenos">834</span></a>                <span class="c1"># computes activation derivatives for the previous layer</span>
</span><span id="MLP-835"><a href="#MLP-835"><span class="linenos">835</span></a>                <span class="n">da_act</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP-836"><a href="#MLP-836"><span class="linenos">836</span></a>                <span class="n">curr_wghts</span> <span class="o">=</span> <span class="n">layer_wghts</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
</span><span id="MLP-837"><a href="#MLP-837"><span class="linenos">837</span></a>                <span class="k">for</span> <span class="n">wght</span> <span class="ow">in</span> <span class="n">curr_wghts</span><span class="p">:</span>
</span><span id="MLP-838"><a href="#MLP-838"><span class="linenos">838</span></a>                    <span class="n">da</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">*</span> <span class="n">wght</span>
</span><span id="MLP-839"><a href="#MLP-839"><span class="linenos">839</span></a>                    <span class="n">da_act</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da</span><span class="p">)</span>
</span><span id="MLP-840"><a href="#MLP-840"><span class="linenos">840</span></a>                <span class="n">da_layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da_act</span><span class="p">)</span>
</span><span id="MLP-841"><a href="#MLP-841"><span class="linenos">841</span></a>
</span><span id="MLP-842"><a href="#MLP-842"><span class="linenos">842</span></a>            <span class="c1"># computes the gradient for current layer</span>
</span><span id="MLP-843"><a href="#MLP-843"><span class="linenos">843</span></a>            <span class="n">b_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">db_layer</span><span class="p">)</span>                
</span><span id="MLP-844"><a href="#MLP-844"><span class="linenos">844</span></a>            <span class="n">w_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dw_layer</span><span class="p">)</span>
</span><span id="MLP-845"><a href="#MLP-845"><span class="linenos">845</span></a>            <span class="n">w_grad</span> <span class="o">=</span> <span class="n">w_grad</span><span class="o">.</span><span class="n">T</span>
</span><span id="MLP-846"><a href="#MLP-846"><span class="linenos">846</span></a>            <span class="n">grad_mxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_grad</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">]</span>
</span><span id="MLP-847"><a href="#MLP-847"><span class="linenos">847</span></a>            <span class="c1"># adds up the gradient values of the current sample</span>
</span><span id="MLP-848"><a href="#MLP-848"><span class="linenos">848</span></a>            <span class="c1"># to the gradient sum of previously computed samples</span>
</span><span id="MLP-849"><a href="#MLP-849"><span class="linenos">849</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">]):</span>
</span><span id="MLP-850"><a href="#MLP-850"><span class="linenos">850</span></a>                <span class="n">mx</span> <span class="o">+=</span> <span class="n">grad_mxs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span><span id="MLP-851"><a href="#MLP-851"><span class="linenos">851</span></a>            
</span><span id="MLP-852"><a href="#MLP-852"><span class="linenos">852</span></a>            <span class="c1"># computes a sum of activation derivatives</span>
</span><span id="MLP-853"><a href="#MLP-853"><span class="linenos">853</span></a>            <span class="c1"># for each activation of the previous layer</span>
</span><span id="MLP-854"><a href="#MLP-854"><span class="linenos">854</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">da_layer</span><span class="p">)</span>
</span><span id="MLP-855"><a href="#MLP-855"><span class="linenos">855</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">da_next</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>A class implementation of a multilayer perceptron with a customizable size and configuration.
It employs only the core Python and numpy functions. The performance optimization is done via numpy vectorization.
Multiple activation and loss functions are supported and can be easily extended.
Provides two plotting methods (static / real-time).
Allows for storing the image of the network after the training in a file, as well as importing other saved images.</p>
</div>


                            <div id="MLP.__init__" class="classattr">
                                        <input id="MLP.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">MLP</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">mlp_layout</span>, </span><span class="param"><span class="n">rand_range</span>, </span><span class="param"><span class="n">train_data</span>, </span><span class="param"><span class="n">name</span>, </span><span class="param"><span class="n">rate</span><span class="o">=</span><span class="mi">1</span></span>)</span>

                <label class="view-source-button" for="MLP.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP.__init__-23"><a href="#MLP.__init__-23"><span class="linenos">23</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mlp_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span><span id="MLP.__init__-24"><a href="#MLP.__init__-24"><span class="linenos">24</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP.__init__-25"><a href="#MLP.__init__-25"><span class="linenos">25</span></a><span class="sd">        The function is the constructor for a `MLP` class, which initializes the parameters and the state </span>
</span><span id="MLP.__init__-26"><a href="#MLP.__init__-26"><span class="linenos">26</span></a><span class="sd">        of the newly created neural network object.</span>
</span><span id="MLP.__init__-27"><a href="#MLP.__init__-27"><span class="linenos">27</span></a><span class="sd">        The `MLP` object state is defined by mapping between each layer and the corresponding matrices that </span>
</span><span id="MLP.__init__-28"><a href="#MLP.__init__-28"><span class="linenos">28</span></a><span class="sd">        contain values for weights and biases plus the name of the activation function. It is represented as </span>
</span><span id="MLP.__init__-29"><a href="#MLP.__init__-29"><span class="linenos">29</span></a><span class="sd">        the dictionary with the following structure:</span>
</span><span id="MLP.__init__-30"><a href="#MLP.__init__-30"><span class="linenos">30</span></a><span class="sd">        `{layer: ([w, b], &#39;act_func&#39;)}`, where:</span>
</span><span id="MLP.__init__-31"><a href="#MLP.__init__-31"><span class="linenos">31</span></a><span class="sd">            - `w` is the matrix of weights for the layer;</span>
</span><span id="MLP.__init__-32"><a href="#MLP.__init__-32"><span class="linenos">32</span></a><span class="sd">            - `b` is the matrix of biases for the layer;</span>
</span><span id="MLP.__init__-33"><a href="#MLP.__init__-33"><span class="linenos">33</span></a><span class="sd">            - `&#39;act_func&#39;` is the activation function for the layer;</span>
</span><span id="MLP.__init__-34"><a href="#MLP.__init__-34"><span class="linenos">34</span></a><span class="sd">            - `layer` is the number of the layer.</span>
</span><span id="MLP.__init__-35"><a href="#MLP.__init__-35"><span class="linenos">35</span></a>
</span><span id="MLP.__init__-36"><a href="#MLP.__init__-36"><span class="linenos">36</span></a><span class="sd">        :param mlp_layout: A layout of the neural network. Defined as a tuple of type:</span>
</span><span id="MLP.__init__-37"><a href="#MLP.__init__-37"><span class="linenos">37</span></a><span class="sd">        `((inp, l1, l2, ... , ln), act_map)`, where:</span>
</span><span id="MLP.__init__-38"><a href="#MLP.__init__-38"><span class="linenos">38</span></a><span class="sd">            - `inp` denotes the size of the input;</span>
</span><span id="MLP.__init__-39"><a href="#MLP.__init__-39"><span class="linenos">39</span></a><span class="sd">            - `ln` denotes the number of neurons in the n-th layer.</span>
</span><span id="MLP.__init__-40"><a href="#MLP.__init__-40"><span class="linenos">40</span></a><span class="sd">            - `act_map` denotes types of activation funcs for each layer.</span>
</span><span id="MLP.__init__-41"><a href="#MLP.__init__-41"><span class="linenos">41</span></a><span class="sd">        :param rand_range: A tuple containing the lower and upper limits for random initialization of </span>
</span><span id="MLP.__init__-42"><a href="#MLP.__init__-42"><span class="linenos">42</span></a><span class="sd">        parameter matrices</span>
</span><span id="MLP.__init__-43"><a href="#MLP.__init__-43"><span class="linenos">43</span></a><span class="sd">        :param train_data: A sequence of training data samples of type:</span>
</span><span id="MLP.__init__-44"><a href="#MLP.__init__-44"><span class="linenos">44</span></a><span class="sd">        `((a1, b1, c1, ...), (a2, b2, c2, ...), (an, bn, cn, ...))`, where:</span>
</span><span id="MLP.__init__-45"><a href="#MLP.__init__-45"><span class="linenos">45</span></a><span class="sd">            - each sample contains `j` input and `k` output values, with `j` defined by `mlp_layout[0][0]`</span>
</span><span id="MLP.__init__-46"><a href="#MLP.__init__-46"><span class="linenos">46</span></a><span class="sd">        :param name: The name of the neural network that identifies it</span>
</span><span id="MLP.__init__-47"><a href="#MLP.__init__-47"><span class="linenos">47</span></a><span class="sd">        :param rate: Is used to define the speed of learning in the neural network. It determines how quickly </span>
</span><span id="MLP.__init__-48"><a href="#MLP.__init__-48"><span class="linenos">48</span></a><span class="sd">        the network adjusts its weights and biases during the training process. Defaults to `1` (optional)</span>
</span><span id="MLP.__init__-49"><a href="#MLP.__init__-49"><span class="linenos">49</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP.__init__-50"><a href="#MLP.__init__-50"><span class="linenos">50</span></a>        <span class="c1"># to store shape of each parameter mx</span>
</span><span id="MLP.__init__-51"><a href="#MLP.__init__-51"><span class="linenos">51</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mx_size_map</span><span class="p">(</span><span class="n">mlp_layout</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span id="MLP.__init__-52"><a href="#MLP.__init__-52"><span class="linenos">52</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">)</span>
</span><span id="MLP.__init__-53"><a href="#MLP.__init__-53"><span class="linenos">53</span></a>        <span class="c1"># to store activation funcs for each layer</span>
</span><span id="MLP.__init__-54"><a href="#MLP.__init__-54"><span class="linenos">54</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span> <span class="o">=</span> <span class="n">mlp_layout</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP.__init__-55"><a href="#MLP.__init__-55"><span class="linenos">55</span></a>        <span class="c1"># to map act. funcs and derivatives to each layer</span>
</span><span id="MLP.__init__-56"><a href="#MLP.__init__-56"><span class="linenos">56</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_func_map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span><span class="p">)</span>
</span><span id="MLP.__init__-57"><a href="#MLP.__init__-57"><span class="linenos">57</span></a>        <span class="c1"># to store training dataset</span>
</span><span id="MLP.__init__-58"><a href="#MLP.__init__-58"><span class="linenos">58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_train</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span id="MLP.__init__-59"><a href="#MLP.__init__-59"><span class="linenos">59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span> <span class="o">=</span> <span class="n">mlp_layout</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP.__init__-60"><a href="#MLP.__init__-60"><span class="linenos">60</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="MLP.__init__-61"><a href="#MLP.__init__-61"><span class="linenos">61</span></a>        <span class="c1"># to store the state of MLP</span>
</span><span id="MLP.__init__-62"><a href="#MLP.__init__-62"><span class="linenos">62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP.__init__-63"><a href="#MLP.__init__-63"><span class="linenos">63</span></a>        <span class="c1"># to store activations</span>
</span><span id="MLP.__init__-64"><a href="#MLP.__init__-64"><span class="linenos">64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP.__init__-65"><a href="#MLP.__init__-65"><span class="linenos">65</span></a>        <span class="c1"># to store the gradient</span>
</span><span id="MLP.__init__-66"><a href="#MLP.__init__-66"><span class="linenos">66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP.__init__-67"><a href="#MLP.__init__-67"><span class="linenos">67</span></a>        <span class="c1"># constant for finite difference computation</span>
</span><span id="MLP.__init__-68"><a href="#MLP.__init__-68"><span class="linenos">68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="mf">1e-1</span>
</span><span id="MLP.__init__-69"><a href="#MLP.__init__-69"><span class="linenos">69</span></a>        <span class="c1"># to define the speed of learning</span>
</span><span id="MLP.__init__-70"><a href="#MLP.__init__-70"><span class="linenos">70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">rate</span>
</span><span id="MLP.__init__-71"><a href="#MLP.__init__-71"><span class="linenos">71</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
</span><span id="MLP.__init__-72"><a href="#MLP.__init__-72"><span class="linenos">72</span></a>        <span class="c1"># initializes the parameters and the gradient</span>
</span><span id="MLP.__init__-73"><a href="#MLP.__init__-73"><span class="linenos">73</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="MLP.__init__-74"><a href="#MLP.__init__-74"><span class="linenos">74</span></a>            <span class="c1"># gets the sizes of parameter matrixes for each layer</span>
</span><span id="MLP.__init__-75"><a href="#MLP.__init__-75"><span class="linenos">75</span></a>            <span class="n">w_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP.__init__-76"><a href="#MLP.__init__-76"><span class="linenos">76</span></a>            <span class="n">b_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP.__init__-77"><a href="#MLP.__init__-77"><span class="linenos">77</span></a>            <span class="c1"># creates two random matrixes for weights and biases per each layer</span>
</span><span id="MLP.__init__-78"><a href="#MLP.__init__-78"><span class="linenos">78</span></a>            <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">rand_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rand_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">w_shape</span><span class="p">)</span>
</span><span id="MLP.__init__-79"><a href="#MLP.__init__-79"><span class="linenos">79</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">rand_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rand_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">b_shape</span><span class="p">)</span>
</span><span id="MLP.__init__-80"><a href="#MLP.__init__-80"><span class="linenos">80</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span>
</span><span id="MLP.__init__-81"><a href="#MLP.__init__-81"><span class="linenos">81</span></a>            <span class="c1"># initializes gradient table with empty matrixes of the same shape</span>
</span><span id="MLP.__init__-82"><a href="#MLP.__init__-82"><span class="linenos">82</span></a>            <span class="n">w_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">w_shape</span><span class="p">)</span>
</span><span id="MLP.__init__-83"><a href="#MLP.__init__-83"><span class="linenos">83</span></a>            <span class="n">b_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">b_shape</span><span class="p">)</span>
</span><span id="MLP.__init__-84"><a href="#MLP.__init__-84"><span class="linenos">84</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_grad</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>The function is the constructor for a <code><a href="#MLP">MLP</a></code> class, which initializes the parameters and the state 
of the newly created neural network object.
The <code><a href="#MLP">MLP</a></code> object state is defined by mapping between each layer and the corresponding matrices that 
contain values for weights and biases plus the name of the activation function. It is represented as 
the dictionary with the following structure:
<code>{layer: ([w, b], 'act_func')}</code>, where:
    - <code>w</code> is the matrix of weights for the layer;
    - <code>b</code> is the matrix of biases for the layer;
    - <code>'act_func'</code> is the activation function for the layer;
    - <code>layer</code> is the number of the layer.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>mlp_layout: A layout of the neural network. Defined as a tuple of type</strong>: 
<code>((inp, l1, l2, ... , ln), act_map)</code>, where:
<ul>
<li><code>inp</code> denotes the size of the input;</li>
<li><code>ln</code> denotes the number of neurons in the n-th layer.</li>
<li><code>act_map</code> denotes types of activation funcs for each layer.</li>
</ul></li>
<li><strong>rand_range</strong>:  A tuple containing the lower and upper limits for random initialization of 
parameter matrices</li>
<li><strong>train_data: A sequence of training data samples of type</strong>: 
<code>((a1, b1, c1, ...), (a2, b2, c2, ...), (an, bn, cn, ...))</code>, where:
<ul>
<li>each sample contains <code>j</code> input and <code>k</code> output values, with <code>j</code> defined by <code>mlp_layout[0][0]</code></li>
</ul></li>
<li><strong>name</strong>:  The name of the neural network that identifies it</li>
<li><strong>rate</strong>:  Is used to define the speed of learning in the neural network. It determines how quickly 
the network adjusts its weights and biases during the training process. Defaults to <code>1</code> (optional)</li>
</ul>
</div>


                            </div>
                            <div id="MLP._mx_size_map" class="classattr">
                                        <input id="MLP._mx_size_map-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">_mx_size_map</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">params</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP._mx_size_map-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP._mx_size_map"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP._mx_size_map-87"><a href="#MLP._mx_size_map-87"><span class="linenos"> 87</span></a>    <span class="k">def</span> <span class="nf">_mx_size_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
</span><span id="MLP._mx_size_map-88"><a href="#MLP._mx_size_map-88"><span class="linenos"> 88</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP._mx_size_map-89"><a href="#MLP._mx_size_map-89"><span class="linenos"> 89</span></a><span class="sd">        The function creates a mapping between each layer in a neural network and the sizes of</span>
</span><span id="MLP._mx_size_map-90"><a href="#MLP._mx_size_map-90"><span class="linenos"> 90</span></a><span class="sd">        the corresponding weight and bias matrices.</span>
</span><span id="MLP._mx_size_map-91"><a href="#MLP._mx_size_map-91"><span class="linenos"> 91</span></a><span class="sd">        </span>
</span><span id="MLP._mx_size_map-92"><a href="#MLP._mx_size_map-92"><span class="linenos"> 92</span></a><span class="sd">        :param params: The `params` parameter is a list that represents the number of neurons in each layer</span>
</span><span id="MLP._mx_size_map-93"><a href="#MLP._mx_size_map-93"><span class="linenos"> 93</span></a><span class="sd">        of a neural network. For example, if `params = (10, 20, 30)`, it means that the neural network has 3</span>
</span><span id="MLP._mx_size_map-94"><a href="#MLP._mx_size_map-94"><span class="linenos"> 94</span></a><span class="sd">        layers with 10 neurons in the input layer, 20 in the inner layer, and 30 in the output layer</span>
</span><span id="MLP._mx_size_map-95"><a href="#MLP._mx_size_map-95"><span class="linenos"> 95</span></a><span class="sd">        :return: a dictionary where the keys are the layer numbers and the values are tuples. Each tuple</span>
</span><span id="MLP._mx_size_map-96"><a href="#MLP._mx_size_map-96"><span class="linenos"> 96</span></a><span class="sd">        contains the size of the weight matrix and the size of the bias matrix for that layer:</span>
</span><span id="MLP._mx_size_map-97"><a href="#MLP._mx_size_map-97"><span class="linenos"> 97</span></a><span class="sd">                `{1:(w1_size, b1_size), </span>
</span><span id="MLP._mx_size_map-98"><a href="#MLP._mx_size_map-98"><span class="linenos"> 98</span></a><span class="sd">                  2:(w2_size, b2_size),</span>
</span><span id="MLP._mx_size_map-99"><a href="#MLP._mx_size_map-99"><span class="linenos"> 99</span></a><span class="sd">                  n:(wn_size, bn_size)}`.</span>
</span><span id="MLP._mx_size_map-100"><a href="#MLP._mx_size_map-100"><span class="linenos">100</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP._mx_size_map-101"><a href="#MLP._mx_size_map-101"><span class="linenos">101</span></a>        <span class="n">mlp_layout</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP._mx_size_map-102"><a href="#MLP._mx_size_map-102"><span class="linenos">102</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)):</span>
</span><span id="MLP._mx_size_map-103"><a href="#MLP._mx_size_map-103"><span class="linenos">103</span></a>            <span class="n">mlp_layout</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="p">]),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="p">]))</span>
</span><span id="MLP._mx_size_map-104"><a href="#MLP._mx_size_map-104"><span class="linenos">104</span></a>
</span><span id="MLP._mx_size_map-105"><a href="#MLP._mx_size_map-105"><span class="linenos">105</span></a>        <span class="k">return</span> <span class="n">mlp_layout</span>
</span></pre></div>


            <div class="docstring"><p>The function creates a mapping between each layer in a neural network and the sizes of
the corresponding weight and bias matrices.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>params</strong>:  The <code>params</code> parameter is a list that represents the number of neurons in each layer
of a neural network. For example, if <code>params = (10, 20, 30)</code>, it means that the neural network has 3
layers with 10 neurons in the input layer, 20 in the inner layer, and 30 in the output layer</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>a dictionary where the keys are the layer numbers and the values are tuples. Each tuple
  contains the size of the weight matrix and the size of the bias matrix for that layer:
          <code>{1:(w1_size, b1_size), 
          2:(w2_size, b2_size),
          n:(wn_size, bn_size)}</code>.</p>
</blockquote>
</div>


                            </div>
                            <div id="MLP._act_func_map" class="classattr">
                                        <input id="MLP._act_func_map-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">_act_func_map</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">act_map</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP._act_func_map-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP._act_func_map"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP._act_func_map-106"><a href="#MLP._act_func_map-106"><span class="linenos">106</span></a>    <span class="k">def</span> <span class="nf">_act_func_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">act_map</span><span class="p">):</span>
</span><span id="MLP._act_func_map-107"><a href="#MLP._act_func_map-107"><span class="linenos">107</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP._act_func_map-108"><a href="#MLP._act_func_map-108"><span class="linenos">108</span></a><span class="sd">        The function converts a provided map of activation functions for each layer into a</span>
</span><span id="MLP._act_func_map-109"><a href="#MLP._act_func_map-109"><span class="linenos">109</span></a><span class="sd">        dictionary with direct references to the functions and their derivatives for each layer.</span>
</span><span id="MLP._act_func_map-110"><a href="#MLP._act_func_map-110"><span class="linenos">110</span></a><span class="sd">        </span>
</span><span id="MLP._act_func_map-111"><a href="#MLP._act_func_map-111"><span class="linenos">111</span></a><span class="sd">        :param act_map: The `act_map` parameter is a dictionary that maps layer numbers to activation</span>
</span><span id="MLP._act_func_map-112"><a href="#MLP._act_func_map-112"><span class="linenos">112</span></a><span class="sd">        function names. Each key-value pair in the dictionary represents a layer in the neural network,</span>
</span><span id="MLP._act_func_map-113"><a href="#MLP._act_func_map-113"><span class="linenos">113</span></a><span class="sd">        where the key is the layer number (an integer) and the value is the name of the activation function</span>
</span><span id="MLP._act_func_map-114"><a href="#MLP._act_func_map-114"><span class="linenos">114</span></a><span class="sd">        (a string)</span>
</span><span id="MLP._act_func_map-115"><a href="#MLP._act_func_map-115"><span class="linenos">115</span></a><span class="sd">        :return: a new dictionary where each layer number is mapped to a tuple containing the activation</span>
</span><span id="MLP._act_func_map-116"><a href="#MLP._act_func_map-116"><span class="linenos">116</span></a><span class="sd">        function and its derivative for that layer.</span>
</span><span id="MLP._act_func_map-117"><a href="#MLP._act_func_map-117"><span class="linenos">117</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP._act_func_map-118"><a href="#MLP._act_func_map-118"><span class="linenos">118</span></a>        <span class="c1"># validate the input</span>
</span><span id="MLP._act_func_map-119"><a href="#MLP._act_func_map-119"><span class="linenos">119</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">act_map</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span><span class="p">,</span> <span class="s2">&quot;Invalid activation map (length)&quot;</span>
</span><span id="MLP._act_func_map-120"><a href="#MLP._act_func_map-120"><span class="linenos">120</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="s2">&quot;Invalid layer number(s)&quot;</span>
</span><span id="MLP._act_func_map-121"><a href="#MLP._act_func_map-121"><span class="linenos">121</span></a>        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">func_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="s2">&quot;Invalid function name(s)&quot;</span>
</span><span id="MLP._act_func_map-122"><a href="#MLP._act_func_map-122"><span class="linenos">122</span></a>
</span><span id="MLP._act_func_map-123"><a href="#MLP._act_func_map-123"><span class="linenos">123</span></a>        <span class="c1"># return a new dict with funcs and their derivatives mapped to each layer</span>
</span><span id="MLP._act_func_map-124"><a href="#MLP._act_func_map-124"><span class="linenos">124</span></a>        <span class="k">return</span> <span class="p">{</span><span class="n">layer</span><span class="p">:</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">func_name</span><span class="p">),</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">func_name</span> <span class="o">+</span> <span class="s2">&quot;_der&quot;</span><span class="p">))</span> <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">act_map</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span></pre></div>


            <div class="docstring"><p>The function converts a provided map of activation functions for each layer into a
dictionary with direct references to the functions and their derivatives for each layer.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>act_map</strong>:  The <code>act_map</code> parameter is a dictionary that maps layer numbers to activation
function names. Each key-value pair in the dictionary represents a layer in the neural network,
where the key is the layer number (an integer) and the value is the name of the activation function
(a string)</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>a new dictionary where each layer number is mapped to a tuple containing the activation
  function and its derivative for that layer.</p>
</blockquote>
</div>


                            </div>
                            <div id="MLP._forward" class="classattr">
                                        <input id="MLP._forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">_forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">input_arr</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP._forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP._forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP._forward-278"><a href="#MLP._forward-278"><span class="linenos">278</span></a>    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_arr</span><span class="p">):</span>
</span><span id="MLP._forward-279"><a href="#MLP._forward-279"><span class="linenos">279</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP._forward-280"><a href="#MLP._forward-280"><span class="linenos">280</span></a><span class="sd">        The function takes an input array and passes it through each layer of a neural network,</span>
</span><span id="MLP._forward-281"><a href="#MLP._forward-281"><span class="linenos">281</span></a><span class="sd">        applying weights, biases, and activation functions to produce the final output.</span>
</span><span id="MLP._forward-282"><a href="#MLP._forward-282"><span class="linenos">282</span></a><span class="sd">        </span>
</span><span id="MLP._forward-283"><a href="#MLP._forward-283"><span class="linenos">283</span></a><span class="sd">        :param input_arr: The input_arr is a numpy array that represents the input to the neural network. It</span>
</span><span id="MLP._forward-284"><a href="#MLP._forward-284"><span class="linenos">284</span></a><span class="sd">        is the input that will be forwarded through each layer of the network</span>
</span><span id="MLP._forward-285"><a href="#MLP._forward-285"><span class="linenos">285</span></a><span class="sd">        :return: the activations of the last layer of the neural network.</span>
</span><span id="MLP._forward-286"><a href="#MLP._forward-286"><span class="linenos">286</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP._forward-287"><a href="#MLP._forward-287"><span class="linenos">287</span></a>        <span class="c1"># initializes input as the 0-th layer of activations</span>
</span><span id="MLP._forward-288"><a href="#MLP._forward-288"><span class="linenos">288</span></a>        <span class="n">a0</span> <span class="o">=</span> <span class="n">input_arr</span>
</span><span id="MLP._forward-289"><a href="#MLP._forward-289"><span class="linenos">289</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a0</span>
</span><span id="MLP._forward-290"><a href="#MLP._forward-290"><span class="linenos">290</span></a>        <span class="c1"># iterates through each layer of the MLP</span>
</span><span id="MLP._forward-291"><a href="#MLP._forward-291"><span class="linenos">291</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">param_lst</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP._forward-292"><a href="#MLP._forward-292"><span class="linenos">292</span></a>            <span class="c1"># gets activation func for current layer</span>
</span><span id="MLP._forward-293"><a href="#MLP._forward-293"><span class="linenos">293</span></a>            <span class="n">act_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP._forward-294"><a href="#MLP._forward-294"><span class="linenos">294</span></a>            <span class="c1"># retrieves values of weights</span>
</span><span id="MLP._forward-295"><a href="#MLP._forward-295"><span class="linenos">295</span></a>            <span class="c1"># and biases of the layer</span>
</span><span id="MLP._forward-296"><a href="#MLP._forward-296"><span class="linenos">296</span></a>            <span class="n">w</span> <span class="o">=</span> <span class="n">param_lst</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP._forward-297"><a href="#MLP._forward-297"><span class="linenos">297</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">param_lst</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP._forward-298"><a href="#MLP._forward-298"><span class="linenos">298</span></a>            <span class="c1"># applies weights</span>
</span><span id="MLP._forward-299"><a href="#MLP._forward-299"><span class="linenos">299</span></a>            <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</span><span id="MLP._forward-300"><a href="#MLP._forward-300"><span class="linenos">300</span></a>            <span class="c1"># applies biases</span>
</span><span id="MLP._forward-301"><a href="#MLP._forward-301"><span class="linenos">301</span></a>            <span class="n">a1</span> <span class="o">+=</span> <span class="n">b</span>
</span><span id="MLP._forward-302"><a href="#MLP._forward-302"><span class="linenos">302</span></a>            <span class="c1"># applies the activation function</span>
</span><span id="MLP._forward-303"><a href="#MLP._forward-303"><span class="linenos">303</span></a>            <span class="n">a1</span> <span class="o">=</span> <span class="n">act_func</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
</span><span id="MLP._forward-304"><a href="#MLP._forward-304"><span class="linenos">304</span></a>            <span class="c1"># saves the resulting activations</span>
</span><span id="MLP._forward-305"><a href="#MLP._forward-305"><span class="linenos">305</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">a1</span>
</span><span id="MLP._forward-306"><a href="#MLP._forward-306"><span class="linenos">306</span></a>            <span class="c1"># assigns the activations as</span>
</span><span id="MLP._forward-307"><a href="#MLP._forward-307"><span class="linenos">307</span></a>            <span class="c1"># an input for the next layer</span>
</span><span id="MLP._forward-308"><a href="#MLP._forward-308"><span class="linenos">308</span></a>            <span class="n">a0</span> <span class="o">=</span> <span class="n">a1</span>
</span><span id="MLP._forward-309"><a href="#MLP._forward-309"><span class="linenos">309</span></a>        <span class="c1"># returns activations of the last layer</span>
</span><span id="MLP._forward-310"><a href="#MLP._forward-310"><span class="linenos">310</span></a>        <span class="k">return</span> <span class="n">a0</span>
</span></pre></div>


            <div class="docstring"><p>The function takes an input array and passes it through each layer of a neural network,
applying weights, biases, and activation functions to produce the final output.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>input_arr</strong>:  The input_arr is a numpy array that represents the input to the neural network. It
is the input that will be forwarded through each layer of the network</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>the activations of the last layer of the neural network.</p>
</blockquote>
</div>


                            </div>
                            <div id="MLP._stochastic_descent" class="classattr">
                                        <input id="MLP._stochastic_descent-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">_stochastic_descent</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">batch_ratio</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP._stochastic_descent-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP._stochastic_descent"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP._stochastic_descent-311"><a href="#MLP._stochastic_descent-311"><span class="linenos">311</span></a>    <span class="k">def</span> <span class="nf">_stochastic_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="p">):</span>
</span><span id="MLP._stochastic_descent-312"><a href="#MLP._stochastic_descent-312"><span class="linenos">312</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP._stochastic_descent-313"><a href="#MLP._stochastic_descent-313"><span class="linenos">313</span></a><span class="sd">        The function implements the stochastic gradient descent algorithm for a neural</span>
</span><span id="MLP._stochastic_descent-314"><a href="#MLP._stochastic_descent-314"><span class="linenos">314</span></a><span class="sd">        network, updating the parameters based on the computed gradients.</span>
</span><span id="MLP._stochastic_descent-315"><a href="#MLP._stochastic_descent-315"><span class="linenos">315</span></a><span class="sd">        It does the following operations:</span>
</span><span id="MLP._stochastic_descent-316"><a href="#MLP._stochastic_descent-316"><span class="linenos">316</span></a><span class="sd">            Keeps track of the total accumulated error value and the count of current samples.</span>
</span><span id="MLP._stochastic_descent-317"><a href="#MLP._stochastic_descent-317"><span class="linenos">317</span></a><span class="sd">            Shuffles the training data randomly and iterates through each data sample.</span>
</span><span id="MLP._stochastic_descent-318"><a href="#MLP._stochastic_descent-318"><span class="linenos">318</span></a><span class="sd">            For each sample, it </span>
</span><span id="MLP._stochastic_descent-319"><a href="#MLP._stochastic_descent-319"><span class="linenos">319</span></a><span class="sd">                1. splits the training data array into input and expected output subarrays;</span>
</span><span id="MLP._stochastic_descent-320"><a href="#MLP._stochastic_descent-320"><span class="linenos">320</span></a><span class="sd">                2. computes activations for each layer, and saves the last layer activations (actual output);</span>
</span><span id="MLP._stochastic_descent-321"><a href="#MLP._stochastic_descent-321"><span class="linenos">321</span></a><span class="sd">                3. sums up squares of error values of each activation in the last layer;</span>
</span><span id="MLP._stochastic_descent-322"><a href="#MLP._stochastic_descent-322"><span class="linenos">322</span></a><span class="sd">                4. performs backpropagation and computes the gradient.</span>
</span><span id="MLP._stochastic_descent-323"><a href="#MLP._stochastic_descent-323"><span class="linenos">323</span></a><span class="sd">            If the batch size is reached, it applies the accumulated gradient values to the parameters.</span>
</span><span id="MLP._stochastic_descent-324"><a href="#MLP._stochastic_descent-324"><span class="linenos">324</span></a><span class="sd">            Finally, it updates the current cost of the model as an average error value per one sample.</span>
</span><span id="MLP._stochastic_descent-325"><a href="#MLP._stochastic_descent-325"><span class="linenos">325</span></a><span class="sd">        </span>
</span><span id="MLP._stochastic_descent-326"><a href="#MLP._stochastic_descent-326"><span class="linenos">326</span></a><span class="sd">        :param batch_ratio: The `batch_ratio` parameter is a float value that represents the ratio of the</span>
</span><span id="MLP._stochastic_descent-327"><a href="#MLP._stochastic_descent-327"><span class="linenos">327</span></a><span class="sd">        total number of training samples that should be used in each batch. For example, if `batch_ratio` is</span>
</span><span id="MLP._stochastic_descent-328"><a href="#MLP._stochastic_descent-328"><span class="linenos">328</span></a><span class="sd">        set to `0.5`, it means that each batch will contain `50%` of the total training samples.</span>
</span><span id="MLP._stochastic_descent-329"><a href="#MLP._stochastic_descent-329"><span class="linenos">329</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP._stochastic_descent-330"><a href="#MLP._stochastic_descent-330"><span class="linenos">330</span></a>        <span class="c1"># keeps the total accumulated error value</span>
</span><span id="MLP._stochastic_descent-331"><a href="#MLP._stochastic_descent-331"><span class="linenos">331</span></a>        <span class="n">total_err</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP._stochastic_descent-332"><a href="#MLP._stochastic_descent-332"><span class="linenos">332</span></a>        <span class="c1"># keeps the count of current samples</span>
</span><span id="MLP._stochastic_descent-333"><a href="#MLP._stochastic_descent-333"><span class="linenos">333</span></a>        <span class="n">sample_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP._stochastic_descent-334"><a href="#MLP._stochastic_descent-334"><span class="linenos">334</span></a>        <span class="c1"># shuffles the training data randomly</span>
</span><span id="MLP._stochastic_descent-335"><a href="#MLP._stochastic_descent-335"><span class="linenos">335</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>
</span><span id="MLP._stochastic_descent-336"><a href="#MLP._stochastic_descent-336"><span class="linenos">336</span></a>        <span class="c1"># iterates through each data sample</span>
</span><span id="MLP._stochastic_descent-337"><a href="#MLP._stochastic_descent-337"><span class="linenos">337</span></a>        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">:</span>
</span><span id="MLP._stochastic_descent-338"><a href="#MLP._stochastic_descent-338"><span class="linenos">338</span></a>            <span class="c1"># splits training data array </span>
</span><span id="MLP._stochastic_descent-339"><a href="#MLP._stochastic_descent-339"><span class="linenos">339</span></a>            <span class="c1"># into input/expected output subarrays</span>
</span><span id="MLP._stochastic_descent-340"><a href="#MLP._stochastic_descent-340"><span class="linenos">340</span></a>            <span class="nb">input</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">]])</span>
</span><span id="MLP._stochastic_descent-341"><a href="#MLP._stochastic_descent-341"><span class="linenos">341</span></a>            <span class="n">expected</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_inp_size</span><span class="p">:</span> <span class="p">]])</span>
</span><span id="MLP._stochastic_descent-342"><a href="#MLP._stochastic_descent-342"><span class="linenos">342</span></a>
</span><span id="MLP._stochastic_descent-343"><a href="#MLP._stochastic_descent-343"><span class="linenos">343</span></a>            <span class="c1"># computes activations for each layer</span>
</span><span id="MLP._stochastic_descent-344"><a href="#MLP._stochastic_descent-344"><span class="linenos">344</span></a>            <span class="c1"># and saves the last layer activations</span>
</span><span id="MLP._stochastic_descent-345"><a href="#MLP._stochastic_descent-345"><span class="linenos">345</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="MLP._stochastic_descent-346"><a href="#MLP._stochastic_descent-346"><span class="linenos">346</span></a>
</span><span id="MLP._stochastic_descent-347"><a href="#MLP._stochastic_descent-347"><span class="linenos">347</span></a>            <span class="c1"># verifies that there is no mismatch between </span>
</span><span id="MLP._stochastic_descent-348"><a href="#MLP._stochastic_descent-348"><span class="linenos">348</span></a>            <span class="c1"># the sizes of the expected and actual output arrays</span>
</span><span id="MLP._stochastic_descent-349"><a href="#MLP._stochastic_descent-349"><span class="linenos">349</span></a>            <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">shape</span>
</span><span id="MLP._stochastic_descent-350"><a href="#MLP._stochastic_descent-350"><span class="linenos">350</span></a>
</span><span id="MLP._stochastic_descent-351"><a href="#MLP._stochastic_descent-351"><span class="linenos">351</span></a>            <span class="c1"># computes activation derivative values for the last layer</span>
</span><span id="MLP._stochastic_descent-352"><a href="#MLP._stochastic_descent-352"><span class="linenos">352</span></a>            <span class="c1"># as a difference between expected and actual outputs of MLP</span>
</span><span id="MLP._stochastic_descent-353"><a href="#MLP._stochastic_descent-353"><span class="linenos">353</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP._stochastic_descent-354"><a href="#MLP._stochastic_descent-354"><span class="linenos">354</span></a>
</span><span id="MLP._stochastic_descent-355"><a href="#MLP._stochastic_descent-355"><span class="linenos">355</span></a>            <span class="c1"># sums up squares of error values </span>
</span><span id="MLP._stochastic_descent-356"><a href="#MLP._stochastic_descent-356"><span class="linenos">356</span></a>            <span class="c1"># of each activation in the last layer</span>
</span><span id="MLP._stochastic_descent-357"><a href="#MLP._stochastic_descent-357"><span class="linenos">357</span></a>            <span class="n">total_err</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">da_next</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="MLP._stochastic_descent-358"><a href="#MLP._stochastic_descent-358"><span class="linenos">358</span></a>
</span><span id="MLP._stochastic_descent-359"><a href="#MLP._stochastic_descent-359"><span class="linenos">359</span></a>            <span class="c1"># performs backpropagation and computes the gradient</span>
</span><span id="MLP._stochastic_descent-360"><a href="#MLP._stochastic_descent-360"><span class="linenos">360</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_backprop</span><span class="p">(</span><span class="n">da_next</span><span class="p">)</span>
</span><span id="MLP._stochastic_descent-361"><a href="#MLP._stochastic_descent-361"><span class="linenos">361</span></a>
</span><span id="MLP._stochastic_descent-362"><a href="#MLP._stochastic_descent-362"><span class="linenos">362</span></a>            <span class="n">sample_counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="MLP._stochastic_descent-363"><a href="#MLP._stochastic_descent-363"><span class="linenos">363</span></a>            <span class="c1"># checks if the batch size is reached</span>
</span><span id="MLP._stochastic_descent-364"><a href="#MLP._stochastic_descent-364"><span class="linenos">364</span></a>            <span class="k">if</span> <span class="n">sample_counter</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">batch_ratio</span><span class="p">:</span>
</span><span id="MLP._stochastic_descent-365"><a href="#MLP._stochastic_descent-365"><span class="linenos">365</span></a>                <span class="c1"># applies accumulated gradient values to the parameters</span>
</span><span id="MLP._stochastic_descent-366"><a href="#MLP._stochastic_descent-366"><span class="linenos">366</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_grad</span><span class="p">(</span><span class="n">sample_counter</span><span class="p">)</span>
</span><span id="MLP._stochastic_descent-367"><a href="#MLP._stochastic_descent-367"><span class="linenos">367</span></a>                <span class="n">sample_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MLP._stochastic_descent-368"><a href="#MLP._stochastic_descent-368"><span class="linenos">368</span></a>
</span><span id="MLP._stochastic_descent-369"><a href="#MLP._stochastic_descent-369"><span class="linenos">369</span></a>        <span class="c1"># applies the gradient of the samples</span>
</span><span id="MLP._stochastic_descent-370"><a href="#MLP._stochastic_descent-370"><span class="linenos">370</span></a>        <span class="c1"># that didn&#39;t reach the size of the batch</span>
</span><span id="MLP._stochastic_descent-371"><a href="#MLP._stochastic_descent-371"><span class="linenos">371</span></a>        <span class="k">if</span> <span class="n">sample_counter</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MLP._stochastic_descent-372"><a href="#MLP._stochastic_descent-372"><span class="linenos">372</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_apply_grad</span><span class="p">(</span><span class="n">sample_counter</span><span class="p">)</span>
</span><span id="MLP._stochastic_descent-373"><a href="#MLP._stochastic_descent-373"><span class="linenos">373</span></a>
</span><span id="MLP._stochastic_descent-374"><a href="#MLP._stochastic_descent-374"><span class="linenos">374</span></a>        <span class="c1"># updates the current cost of the model </span>
</span><span id="MLP._stochastic_descent-375"><a href="#MLP._stochastic_descent-375"><span class="linenos">375</span></a>        <span class="c1"># as an average error value per 1 sample</span>
</span><span id="MLP._stochastic_descent-376"><a href="#MLP._stochastic_descent-376"><span class="linenos">376</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span> <span class="o">=</span> <span class="n">total_err</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">)</span>    
</span></pre></div>


            <div class="docstring"><p>The function implements the stochastic gradient descent algorithm for a neural
network, updating the parameters based on the computed gradients.
It does the following operations:
    Keeps track of the total accumulated error value and the count of current samples.
    Shuffles the training data randomly and iterates through each data sample.
    For each sample, it 
        1. splits the training data array into input and expected output subarrays;
        2. computes activations for each layer, and saves the last layer activations (actual output);
        3. sums up squares of error values of each activation in the last layer;
        4. performs backpropagation and computes the gradient.
    If the batch size is reached, it applies the accumulated gradient values to the parameters.
    Finally, it updates the current cost of the model as an average error value per one sample.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>batch_ratio</strong>:  The <code>batch_ratio</code> parameter is a float value that represents the ratio of the
total number of training samples that should be used in each batch. For example, if <code>batch_ratio</code> is
set to <code>0.5</code>, it means that each batch will contain <code>50%</code> of the total training samples.</li>
</ul>
</div>


                            </div>
                            <div id="MLP._backprop" class="classattr">
                                        <input id="MLP._backprop-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">_backprop</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">da_next</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP._backprop-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP._backprop"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP._backprop-377"><a href="#MLP._backprop-377"><span class="linenos">377</span></a>    <span class="k">def</span> <span class="nf">_backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">da_next</span><span class="p">):</span>
</span><span id="MLP._backprop-378"><a href="#MLP._backprop-378"><span class="linenos">378</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP._backprop-379"><a href="#MLP._backprop-379"><span class="linenos">379</span></a><span class="sd">        The function performs backpropagation to compute and accumulate the gradients of the weights and biases for later application to the parameters of a neural network.</span>
</span><span id="MLP._backprop-380"><a href="#MLP._backprop-380"><span class="linenos">380</span></a><span class="sd">        </span>
</span><span id="MLP._backprop-381"><a href="#MLP._backprop-381"><span class="linenos">381</span></a><span class="sd">        :param da_next: da_next is the derivative of the cost function with respect to the activations of</span>
</span><span id="MLP._backprop-382"><a href="#MLP._backprop-382"><span class="linenos">382</span></a><span class="sd">        the next layer. It represents the backpropagated error from the next layer</span>
</span><span id="MLP._backprop-383"><a href="#MLP._backprop-383"><span class="linenos">383</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP._backprop-384"><a href="#MLP._backprop-384"><span class="linenos">384</span></a>        <span class="c1"># iterates through each layer backwards starting from the last one</span>
</span><span id="MLP._backprop-385"><a href="#MLP._backprop-385"><span class="linenos">385</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
</span><span id="MLP._backprop-386"><a href="#MLP._backprop-386"><span class="linenos">386</span></a>
</span><span id="MLP._backprop-387"><a href="#MLP._backprop-387"><span class="linenos">387</span></a>            <span class="c1"># gets the weights for the current layer</span>
</span><span id="MLP._backprop-388"><a href="#MLP._backprop-388"><span class="linenos">388</span></a>            <span class="n">layer_wghts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP._backprop-389"><a href="#MLP._backprop-389"><span class="linenos">389</span></a>            
</span><span id="MLP._backprop-390"><a href="#MLP._backprop-390"><span class="linenos">390</span></a>            <span class="c1"># gets the derivative of activation func for the current layer</span>
</span><span id="MLP._backprop-391"><a href="#MLP._backprop-391"><span class="linenos">391</span></a>            <span class="n">der_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP._backprop-392"><a href="#MLP._backprop-392"><span class="linenos">392</span></a>
</span><span id="MLP._backprop-393"><a href="#MLP._backprop-393"><span class="linenos">393</span></a>            <span class="c1"># computes the derivative expression vectorized for all activations</span>
</span><span id="MLP._backprop-394"><a href="#MLP._backprop-394"><span class="linenos">394</span></a>            <span class="n">der_expr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lncosh_der</span><span class="p">(</span><span class="n">da_next</span><span class="p">)</span> <span class="o">*</span> <span class="n">der_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</span><span id="MLP._backprop-395"><a href="#MLP._backprop-395"><span class="linenos">395</span></a>
</span><span id="MLP._backprop-396"><a href="#MLP._backprop-396"><span class="linenos">396</span></a>            <span class="c1"># computes bias derivatives for the entire layer</span>
</span><span id="MLP._backprop-397"><a href="#MLP._backprop-397"><span class="linenos">397</span></a>            <span class="n">db</span> <span class="o">=</span> <span class="n">der_expr</span>
</span><span id="MLP._backprop-398"><a href="#MLP._backprop-398"><span class="linenos">398</span></a>
</span><span id="MLP._backprop-399"><a href="#MLP._backprop-399"><span class="linenos">399</span></a>            <span class="c1"># computes weights derivatives for the entire layer</span>
</span><span id="MLP._backprop-400"><a href="#MLP._backprop-400"><span class="linenos">400</span></a>            <span class="n">dw</span> <span class="o">=</span> <span class="n">der_expr</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP._backprop-401"><a href="#MLP._backprop-401"><span class="linenos">401</span></a>            <span class="n">dw</span> <span class="o">=</span> <span class="n">dw</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_acts</span><span class="p">[</span><span class="n">layer</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</span><span id="MLP._backprop-402"><a href="#MLP._backprop-402"><span class="linenos">402</span></a>
</span><span id="MLP._backprop-403"><a href="#MLP._backprop-403"><span class="linenos">403</span></a>            <span class="c1"># computes the gradient for current layer</span>
</span><span id="MLP._backprop-404"><a href="#MLP._backprop-404"><span class="linenos">404</span></a>            <span class="n">grad_mxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dw</span><span class="p">,</span> <span class="n">db</span><span class="p">]</span>
</span><span id="MLP._backprop-405"><a href="#MLP._backprop-405"><span class="linenos">405</span></a>
</span><span id="MLP._backprop-406"><a href="#MLP._backprop-406"><span class="linenos">406</span></a>            <span class="c1"># adds up the gradient values of the current sample</span>
</span><span id="MLP._backprop-407"><a href="#MLP._backprop-407"><span class="linenos">407</span></a>            <span class="c1"># to the gradient sum of previously computed samples</span>
</span><span id="MLP._backprop-408"><a href="#MLP._backprop-408"><span class="linenos">408</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">]):</span>
</span><span id="MLP._backprop-409"><a href="#MLP._backprop-409"><span class="linenos">409</span></a>                <span class="n">mx</span> <span class="o">+=</span> <span class="n">grad_mxs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span><span id="MLP._backprop-410"><a href="#MLP._backprop-410"><span class="linenos">410</span></a>
</span><span id="MLP._backprop-411"><a href="#MLP._backprop-411"><span class="linenos">411</span></a>            <span class="c1"># computes activation derivatives for the previous layer</span>
</span><span id="MLP._backprop-412"><a href="#MLP._backprop-412"><span class="linenos">412</span></a>            <span class="n">da_next</span> <span class="o">=</span> <span class="n">der_expr</span> <span class="o">@</span> <span class="n">layer_wghts</span><span class="o">.</span><span class="n">T</span>
</span></pre></div>


            <div class="docstring"><p>The function performs backpropagation to compute and accumulate the gradients of the weights and biases for later application to the parameters of a neural network.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>da_next</strong>:  da_next is the derivative of the cost function with respect to the activations of
the next layer. It represents the backpropagated error from the next layer</li>
</ul>
</div>


                            </div>
                            <div id="MLP._apply_grad" class="classattr">
                                        <input id="MLP._apply_grad-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">_apply_grad</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">num_samples</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP._apply_grad-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP._apply_grad"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP._apply_grad-413"><a href="#MLP._apply_grad-413"><span class="linenos">413</span></a>    <span class="k">def</span> <span class="nf">_apply_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
</span><span id="MLP._apply_grad-414"><a href="#MLP._apply_grad-414"><span class="linenos">414</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP._apply_grad-415"><a href="#MLP._apply_grad-415"><span class="linenos">415</span></a><span class="sd">        The function applies the stored gradient values to the parameters of a neural network.</span>
</span><span id="MLP._apply_grad-416"><a href="#MLP._apply_grad-416"><span class="linenos">416</span></a><span class="sd">        </span>
</span><span id="MLP._apply_grad-417"><a href="#MLP._apply_grad-417"><span class="linenos">417</span></a><span class="sd">        :param num_samples: The `num_samples` parameter represents the number of samples used to compute the</span>
</span><span id="MLP._apply_grad-418"><a href="#MLP._apply_grad-418"><span class="linenos">418</span></a><span class="sd">        gradient. It is used to normalize the gradient update step by dividing it by the number of samples.</span>
</span><span id="MLP._apply_grad-419"><a href="#MLP._apply_grad-419"><span class="linenos">419</span></a><span class="sd">        This helps to ensure that the gradient update is not too large or too small, regardless of the</span>
</span><span id="MLP._apply_grad-420"><a href="#MLP._apply_grad-420"><span class="linenos">420</span></a><span class="sd">        number of samples used in the batch.</span>
</span><span id="MLP._apply_grad-421"><a href="#MLP._apply_grad-421"><span class="linenos">421</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP._apply_grad-422"><a href="#MLP._apply_grad-422"><span class="linenos">422</span></a>        <span class="c1"># iterates through parameters</span>
</span><span id="MLP._apply_grad-423"><a href="#MLP._apply_grad-423"><span class="linenos">423</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">param_mxs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP._apply_grad-424"><a href="#MLP._apply_grad-424"><span class="linenos">424</span></a>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param_mx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_mxs</span><span class="p">):</span>
</span><span id="MLP._apply_grad-425"><a href="#MLP._apply_grad-425"><span class="linenos">425</span></a>                <span class="c1"># gets the gradient values</span>
</span><span id="MLP._apply_grad-426"><a href="#MLP._apply_grad-426"><span class="linenos">426</span></a>                <span class="n">grad_mx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</span><span id="MLP._apply_grad-427"><a href="#MLP._apply_grad-427"><span class="linenos">427</span></a>                <span class="c1"># subtracts the gradient from the parameters</span>
</span><span id="MLP._apply_grad-428"><a href="#MLP._apply_grad-428"><span class="linenos">428</span></a>                <span class="n">param_mx</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_mx</span> <span class="o">/</span> <span class="n">num_samples</span><span class="p">)</span>
</span><span id="MLP._apply_grad-429"><a href="#MLP._apply_grad-429"><span class="linenos">429</span></a>                <span class="c1"># resets the gradient to zero for next training cycle</span>
</span><span id="MLP._apply_grad-430"><a href="#MLP._apply_grad-430"><span class="linenos">430</span></a>                <span class="n">grad_mx</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
</span></pre></div>


            <div class="docstring"><p>The function applies the stored gradient values to the parameters of a neural network.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>num_samples</strong>:  The <code>num_samples</code> parameter represents the number of samples used to compute the
gradient. It is used to normalize the gradient update step by dividing it by the number of samples.
This helps to ensure that the gradient update is not too large or too small, regardless of the
number of samples used in the batch.</li>
</ul>
</div>


                            </div>
                            <div id="MLP.learn" class="classattr">
                                        <input id="MLP.learn-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">learn</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">num_epochs</span>,</span><span class="param">	<span class="n">algorithm</span><span class="o">=&lt;</span><span class="n">function</span> <span class="n"><a href="#MLP._stochastic_descent">MLP._stochastic_descent</a></span><span class="o">&gt;</span>,</span><span class="param">	<span class="n">rate</span><span class="o">=</span><span class="mi">1</span>,</span><span class="param">	<span class="n">batch_ratio</span><span class="o">=</span><span class="mf">0.1</span>,</span><span class="param">	<span class="n">threshold</span><span class="o">=</span><span class="mf">0.02</span>,</span><span class="param">	<span class="n">stop</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">plot_static</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">plot_dynamic</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">upd_interval</span><span class="o">=</span><span class="mi">20</span>,</span><span class="param">	<span class="n">with_full_report</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">return_cost</span><span class="o">=</span><span class="kc">False</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP.learn-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP.learn"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP.learn-433"><a href="#MLP.learn-433"><span class="linenos">433</span></a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="n">_stochastic_descent</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plot_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">plot_dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">upd_interval</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">with_full_report</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_cost</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="MLP.learn-434"><a href="#MLP.learn-434"><span class="linenos">434</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP.learn-435"><a href="#MLP.learn-435"><span class="linenos">435</span></a><span class="sd">        The function trains a neural network for a specified number of epochs using a chosen</span>
</span><span id="MLP.learn-436"><a href="#MLP.learn-436"><span class="linenos">436</span></a><span class="sd">        algorithm, reports the state of the model, plots the cost function, and computes the total run time.</span>
</span><span id="MLP.learn-437"><a href="#MLP.learn-437"><span class="linenos">437</span></a><span class="sd">        </span>
</span><span id="MLP.learn-438"><a href="#MLP.learn-438"><span class="linenos">438</span></a><span class="sd">        :param num_epochs: The number of epochs, which is the number of times the neural network will be</span>
</span><span id="MLP.learn-439"><a href="#MLP.learn-439"><span class="linenos">439</span></a><span class="sd">        trained on the dataset</span>
</span><span id="MLP.learn-440"><a href="#MLP.learn-440"><span class="linenos">440</span></a><span class="sd">        :param algorithm: Determines the learning algorithm to be used for training the neural network. </span>
</span><span id="MLP.learn-441"><a href="#MLP.learn-441"><span class="linenos">441</span></a><span class="sd">        The default value is `stochastic_descent`, but you can pass any other function that implements a </span>
</span><span id="MLP.learn-442"><a href="#MLP.learn-442"><span class="linenos">442</span></a><span class="sd">        different learning algorithm</span>
</span><span id="MLP.learn-443"><a href="#MLP.learn-443"><span class="linenos">443</span></a><span class="sd">        :param rate: The learning rate of the neural network. It determines how quickly the parameters of the </span>
</span><span id="MLP.learn-444"><a href="#MLP.learn-444"><span class="linenos">444</span></a><span class="sd">        network are updated during training. A higher learning rate can result in faster convergence, but it </span>
</span><span id="MLP.learn-445"><a href="#MLP.learn-445"><span class="linenos">445</span></a><span class="sd">        may also cause the network to overshoot the optimal solution. Defaults to `1` (optional)</span>
</span><span id="MLP.learn-446"><a href="#MLP.learn-446"><span class="linenos">446</span></a><span class="sd">        :param batch_ratio: Determines the ratio of the training data used in each iteration of the learning </span>
</span><span id="MLP.learn-447"><a href="#MLP.learn-447"><span class="linenos">447</span></a><span class="sd">        algorithm</span>
</span><span id="MLP.learn-448"><a href="#MLP.learn-448"><span class="linenos">448</span></a><span class="sd">        :param threshold: Is used to determine when to stop the learning process. If the current cost of the </span>
</span><span id="MLP.learn-449"><a href="#MLP.learn-449"><span class="linenos">449</span></a><span class="sd">        model falls below the threshold value, the learning process will stop</span>
</span><span id="MLP.learn-450"><a href="#MLP.learn-450"><span class="linenos">450</span></a><span class="sd">        :param stop: Determines whether the learning process should stop when the threshold cost is reached. </span>
</span><span id="MLP.learn-451"><a href="#MLP.learn-451"><span class="linenos">451</span></a><span class="sd">        If set to `True`, the learning process will stop when the cost falls below the specified threshold. </span>
</span><span id="MLP.learn-452"><a href="#MLP.learn-452"><span class="linenos">452</span></a><span class="sd">        Otherwise, the learning process will continue until all epochs are completed, regardless of the cost. </span>
</span><span id="MLP.learn-453"><a href="#MLP.learn-453"><span class="linenos">453</span></a><span class="sd">        Defaults to `True` (optional)</span>
</span><span id="MLP.learn-454"><a href="#MLP.learn-454"><span class="linenos">454</span></a><span class="sd">        :param plot_static: Determines whether to plot the graph of the cost function after training the neural </span>
</span><span id="MLP.learn-455"><a href="#MLP.learn-455"><span class="linenos">455</span></a><span class="sd">        network. If set to `True`, the graph will be plotted using the `static_plot` method. Defaults to `False` </span>
</span><span id="MLP.learn-456"><a href="#MLP.learn-456"><span class="linenos">456</span></a><span class="sd">        (optional)</span>
</span><span id="MLP.learn-457"><a href="#MLP.learn-457"><span class="linenos">457</span></a><span class="sd">        :param plot_dynamic: Determines whether or not to plot the cost function in real-time during the learning </span>
</span><span id="MLP.learn-458"><a href="#MLP.learn-458"><span class="linenos">458</span></a><span class="sd">        process. If set to `True`, a separate process will be created to handle the plotting. Defaults to `False` </span>
</span><span id="MLP.learn-459"><a href="#MLP.learn-459"><span class="linenos">459</span></a><span class="sd">        (optional)</span>
</span><span id="MLP.learn-460"><a href="#MLP.learn-460"><span class="linenos">460</span></a><span class="sd">        :param upd_interval: Determines the number of epochs after which the dynamic cost plot is updated during </span>
</span><span id="MLP.learn-461"><a href="#MLP.learn-461"><span class="linenos">461</span></a><span class="sd">        the learning process. Defaults to `20` (optional)</span>
</span><span id="MLP.learn-462"><a href="#MLP.learn-462"><span class="linenos">462</span></a><span class="sd">        :param with_full_report: Determines whether to include a full report of the neural network&#39;s state after </span>
</span><span id="MLP.learn-463"><a href="#MLP.learn-463"><span class="linenos">463</span></a><span class="sd">        training. If set to `True`, the full report will be printed to the console. Otherwise, only a summary </span>
</span><span id="MLP.learn-464"><a href="#MLP.learn-464"><span class="linenos">464</span></a><span class="sd">        of the neural network&#39;s state will be printed. Defaults to `False` (optional)</span>
</span><span id="MLP.learn-465"><a href="#MLP.learn-465"><span class="linenos">465</span></a><span class="sd">        :param return_cost: Determines whether the function should return the list of cost values. If set to </span>
</span><span id="MLP.learn-466"><a href="#MLP.learn-466"><span class="linenos">466</span></a><span class="sd">        `True`, the function will return a list of cost values for each epoch. Defaults to `False` (optional)</span>
</span><span id="MLP.learn-467"><a href="#MLP.learn-467"><span class="linenos">467</span></a><span class="sd">        :return: If the `return_cost` parameter is set to `True`, it returns a list of cost values. Otherwise it </span>
</span><span id="MLP.learn-468"><a href="#MLP.learn-468"><span class="linenos">468</span></a><span class="sd">        returns `None`</span>
</span><span id="MLP.learn-469"><a href="#MLP.learn-469"><span class="linenos">469</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP.learn-470"><a href="#MLP.learn-470"><span class="linenos">470</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">plot_dynamic</span> <span class="ow">and</span> <span class="n">plot_static</span><span class="p">),</span> <span class="s2">&quot;Incorrect plotting mode setup&quot;</span>
</span><span id="MLP.learn-471"><a href="#MLP.learn-471"><span class="linenos">471</span></a>        <span class="c1"># initializes threshold flag</span>
</span><span id="MLP.learn-472"><a href="#MLP.learn-472"><span class="linenos">472</span></a>        <span class="n">thresh_flag</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="MLP.learn-473"><a href="#MLP.learn-473"><span class="linenos">473</span></a>        <span class="c1"># sets the learning rate</span>
</span><span id="MLP.learn-474"><a href="#MLP.learn-474"><span class="linenos">474</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">rate</span>
</span><span id="MLP.learn-475"><a href="#MLP.learn-475"><span class="linenos">475</span></a>
</span><span id="MLP.learn-476"><a href="#MLP.learn-476"><span class="linenos">476</span></a>        <span class="c1"># reports the initial state of the MLP</span>
</span><span id="MLP.learn-477"><a href="#MLP.learn-477"><span class="linenos">477</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt;&lt;&lt; INITIAL STATE &gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="MLP.learn-478"><a href="#MLP.learn-478"><span class="linenos">478</span></a>        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">(</span><span class="n">with_full_report</span><span class="p">))</span>
</span><span id="MLP.learn-479"><a href="#MLP.learn-479"><span class="linenos">479</span></a>
</span><span id="MLP.learn-480"><a href="#MLP.learn-480"><span class="linenos">480</span></a>        <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="MLP.learn-481"><a href="#MLP.learn-481"><span class="linenos">481</span></a>            <span class="c1"># initializes table for cost/epoch</span>
</span><span id="MLP.learn-482"><a href="#MLP.learn-482"><span class="linenos">482</span></a>            <span class="n">cost_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP.learn-483"><a href="#MLP.learn-483"><span class="linenos">483</span></a>        <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>  
</span><span id="MLP.learn-484"><a href="#MLP.learn-484"><span class="linenos">484</span></a>            <span class="c1"># initializes lists for plotting</span>
</span><span id="MLP.learn-485"><a href="#MLP.learn-485"><span class="linenos">485</span></a>            <span class="n">x_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP.learn-486"><a href="#MLP.learn-486"><span class="linenos">486</span></a>            <span class="n">y_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP.learn-487"><a href="#MLP.learn-487"><span class="linenos">487</span></a>        <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="MLP.learn-488"><a href="#MLP.learn-488"><span class="linenos">488</span></a>            <span class="c1"># creates a queue for communication with the plotting process</span>
</span><span id="MLP.learn-489"><a href="#MLP.learn-489"><span class="linenos">489</span></a>            <span class="n">queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">()</span>
</span><span id="MLP.learn-490"><a href="#MLP.learn-490"><span class="linenos">490</span></a>            <span class="c1"># creates the plotting process</span>
</span><span id="MLP.learn-491"><a href="#MLP.learn-491"><span class="linenos">491</span></a>            <span class="n">plotting_process</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_plot</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">queue</span><span class="p">,))</span>
</span><span id="MLP.learn-492"><a href="#MLP.learn-492"><span class="linenos">492</span></a>            <span class="n">plotting_process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span id="MLP.learn-493"><a href="#MLP.learn-493"><span class="linenos">493</span></a>
</span><span id="MLP.learn-494"><a href="#MLP.learn-494"><span class="linenos">494</span></a>        <span class="c1"># initializes the progress bar</span>
</span><span id="MLP.learn-495"><a href="#MLP.learn-495"><span class="linenos">495</span></a>        <span class="n">EMPTY</span><span class="p">,</span> <span class="n">COMPLETE</span> <span class="o">=</span> <span class="s1">&#39; - &#39;</span><span class="p">,</span> <span class="s1">&#39; # &#39;</span>
</span><span id="MLP.learn-496"><a href="#MLP.learn-496"><span class="linenos">496</span></a>        <span class="n">BAR_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>
</span><span id="MLP.learn-497"><a href="#MLP.learn-497"><span class="linenos">497</span></a>        <span class="n">CURSOR_UP</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[1A&#39;</span>
</span><span id="MLP.learn-498"><a href="#MLP.learn-498"><span class="linenos">498</span></a>        <span class="n">CLEAR</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\x1b</span><span class="s1">[2K&#39;</span>
</span><span id="MLP.learn-499"><a href="#MLP.learn-499"><span class="linenos">499</span></a>        <span class="n">CLEAR_LINE</span> <span class="o">=</span> <span class="n">CURSOR_UP</span> <span class="o">+</span> <span class="n">CLEAR</span>
</span><span id="MLP.learn-500"><a href="#MLP.learn-500"><span class="linenos">500</span></a>        <span class="n">counter</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="MLP.learn-501"><a href="#MLP.learn-501"><span class="linenos">501</span></a>        <span class="n">ratio</span> <span class="o">=</span> <span class="n">num_epochs</span> <span class="o">/</span> <span class="n">BAR_LENGTH</span>
</span><span id="MLP.learn-502"><a href="#MLP.learn-502"><span class="linenos">502</span></a>        <span class="n">progress</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">EMPTY</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">BAR_LENGTH</span><span class="si">}</span><span class="s2">]&quot;</span>
</span><span id="MLP.learn-503"><a href="#MLP.learn-503"><span class="linenos">503</span></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt;&lt;&lt; LEARNING PROGRESS &gt;&gt;&gt;</span><span class="se">\n</span><span class="si">{</span><span class="n">progress</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="MLP.learn-504"><a href="#MLP.learn-504"><span class="linenos">504</span></a>
</span><span id="MLP.learn-505"><a href="#MLP.learn-505"><span class="linenos">505</span></a>        <span class="n">start_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="MLP.learn-506"><a href="#MLP.learn-506"><span class="linenos">506</span></a>        <span class="c1"># repeats the learning procedure &lt;num_epochs&gt; times</span>
</span><span id="MLP.learn-507"><a href="#MLP.learn-507"><span class="linenos">507</span></a>        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span><span id="MLP.learn-508"><a href="#MLP.learn-508"><span class="linenos">508</span></a>            <span class="c1"># executes the selected algorithm</span>
</span><span id="MLP.learn-509"><a href="#MLP.learn-509"><span class="linenos">509</span></a>            <span class="n">algorithm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="p">)</span>
</span><span id="MLP.learn-510"><a href="#MLP.learn-510"><span class="linenos">510</span></a>            <span class="c1"># update the progress bar if needed</span>
</span><span id="MLP.learn-511"><a href="#MLP.learn-511"><span class="linenos">511</span></a>            <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">ratio</span><span class="o">*</span><span class="n">counter</span><span class="p">:</span>
</span><span id="MLP.learn-512"><a href="#MLP.learn-512"><span class="linenos">512</span></a>                <span class="nb">print</span><span class="p">(</span><span class="n">CLEAR_LINE</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</span><span id="MLP.learn-513"><a href="#MLP.learn-513"><span class="linenos">513</span></a>                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">COMPLETE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">counter</span><span class="si">}{</span><span class="n">EMPTY</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">BAR_LENGTH</span><span class="o">-</span><span class="n">counter</span><span class="p">)</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</span><span id="MLP.learn-514"><a href="#MLP.learn-514"><span class="linenos">514</span></a>                <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="MLP.learn-515"><a href="#MLP.learn-515"><span class="linenos">515</span></a>
</span><span id="MLP.learn-516"><a href="#MLP.learn-516"><span class="linenos">516</span></a>            <span class="c1"># gets the current cost of the model</span>
</span><span id="MLP.learn-517"><a href="#MLP.learn-517"><span class="linenos">517</span></a>            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_curr_cost</span>
</span><span id="MLP.learn-518"><a href="#MLP.learn-518"><span class="linenos">518</span></a>            <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="MLP.learn-519"><a href="#MLP.learn-519"><span class="linenos">519</span></a>                <span class="c1"># saves the values for the cost table</span>
</span><span id="MLP.learn-520"><a href="#MLP.learn-520"><span class="linenos">520</span></a>                <span class="n">cost_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="MLP.learn-521"><a href="#MLP.learn-521"><span class="linenos">521</span></a>            <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>
</span><span id="MLP.learn-522"><a href="#MLP.learn-522"><span class="linenos">522</span></a>                <span class="c1"># saves the values for the plot</span>
</span><span id="MLP.learn-523"><a href="#MLP.learn-523"><span class="linenos">523</span></a>                <span class="n">x_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="MLP.learn-524"><a href="#MLP.learn-524"><span class="linenos">524</span></a>                <span class="n">y_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="MLP.learn-525"><a href="#MLP.learn-525"><span class="linenos">525</span></a>            <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="MLP.learn-526"><a href="#MLP.learn-526"><span class="linenos">526</span></a>                <span class="c1"># updates the cost plot every &lt;upd_interval&gt; cycles</span>
</span><span id="MLP.learn-527"><a href="#MLP.learn-527"><span class="linenos">527</span></a>                <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">upd_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="MLP.learn-528"><a href="#MLP.learn-528"><span class="linenos">528</span></a>                    <span class="c1"># sends data to the plotting process</span>
</span><span id="MLP.learn-529"><a href="#MLP.learn-529"><span class="linenos">529</span></a>                    <span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>
</span><span id="MLP.learn-530"><a href="#MLP.learn-530"><span class="linenos">530</span></a>
</span><span id="MLP.learn-531"><a href="#MLP.learn-531"><span class="linenos">531</span></a>            <span class="c1"># detects the time when the threshold cost is reached</span>
</span><span id="MLP.learn-532"><a href="#MLP.learn-532"><span class="linenos">532</span></a>            <span class="k">if</span> <span class="n">cost</span> <span class="o">&lt;</span> <span class="n">threshold</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">thresh_flag</span><span class="p">:</span>
</span><span id="MLP.learn-533"><a href="#MLP.learn-533"><span class="linenos">533</span></a>                <span class="n">thresh_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="MLP.learn-534"><a href="#MLP.learn-534"><span class="linenos">534</span></a>                <span class="n">thresh_iter</span> <span class="o">=</span> <span class="n">epoch</span>
</span><span id="MLP.learn-535"><a href="#MLP.learn-535"><span class="linenos">535</span></a>                <span class="n">thresh_flag</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="MLP.learn-536"><a href="#MLP.learn-536"><span class="linenos">536</span></a>                <span class="c1"># stops learning</span>
</span><span id="MLP.learn-537"><a href="#MLP.learn-537"><span class="linenos">537</span></a>                <span class="k">if</span> <span class="n">stop</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
</span><span id="MLP.learn-538"><a href="#MLP.learn-538"><span class="linenos">538</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="n">CLEAR_LINE</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</span><span id="MLP.learn-539"><a href="#MLP.learn-539"><span class="linenos">539</span></a>                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">COMPLETE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">BAR_LENGTH</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</span><span id="MLP.learn-540"><a href="#MLP.learn-540"><span class="linenos">540</span></a>                    <span class="k">break</span>
</span><span id="MLP.learn-541"><a href="#MLP.learn-541"><span class="linenos">541</span></a>        
</span><span id="MLP.learn-542"><a href="#MLP.learn-542"><span class="linenos">542</span></a>        <span class="n">end_t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="MLP.learn-543"><a href="#MLP.learn-543"><span class="linenos">543</span></a>
</span><span id="MLP.learn-544"><a href="#MLP.learn-544"><span class="linenos">544</span></a>        <span class="k">if</span> <span class="n">plot_dynamic</span><span class="p">:</span>
</span><span id="MLP.learn-545"><a href="#MLP.learn-545"><span class="linenos">545</span></a>            <span class="c1"># signals the plot process to terminate</span>
</span><span id="MLP.learn-546"><a href="#MLP.learn-546"><span class="linenos">546</span></a>            <span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</span><span id="MLP.learn-547"><a href="#MLP.learn-547"><span class="linenos">547</span></a>
</span><span id="MLP.learn-548"><a href="#MLP.learn-548"><span class="linenos">548</span></a>        <span class="c1"># reports the final state of the model</span>
</span><span id="MLP.learn-549"><a href="#MLP.learn-549"><span class="linenos">549</span></a>        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">(</span><span class="n">with_full_report</span><span class="p">))</span>
</span><span id="MLP.learn-550"><a href="#MLP.learn-550"><span class="linenos">550</span></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Time elapsed: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">end_t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_t</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">s.&quot;</span><span class="p">)</span>
</span><span id="MLP.learn-551"><a href="#MLP.learn-551"><span class="linenos">551</span></a>        <span class="c1"># prints how much time was needed to pass the cost threshold</span>
</span><span id="MLP.learn-552"><a href="#MLP.learn-552"><span class="linenos">552</span></a>        <span class="k">if</span> <span class="n">thresh_flag</span><span class="p">:</span>
</span><span id="MLP.learn-553"><a href="#MLP.learn-553"><span class="linenos">553</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">threshold</span><span class="si">}</span><span class="s2">% of cost was reached in </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">thresh_t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_t</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">s (</span><span class="si">{</span><span class="n">thresh_iter</span><span class="si">}</span><span class="s2"> iterations).&quot;</span><span class="p">)</span>
</span><span id="MLP.learn-554"><a href="#MLP.learn-554"><span class="linenos">554</span></a>
</span><span id="MLP.learn-555"><a href="#MLP.learn-555"><span class="linenos">555</span></a>        <span class="k">if</span> <span class="n">plot_static</span><span class="p">:</span>
</span><span id="MLP.learn-556"><a href="#MLP.learn-556"><span class="linenos">556</span></a>            <span class="c1"># plots the cost</span>
</span><span id="MLP.learn-557"><a href="#MLP.learn-557"><span class="linenos">557</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_static_plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
</span><span id="MLP.learn-558"><a href="#MLP.learn-558"><span class="linenos">558</span></a>
</span><span id="MLP.learn-559"><a href="#MLP.learn-559"><span class="linenos">559</span></a>        <span class="k">if</span> <span class="n">return_cost</span><span class="p">:</span>
</span><span id="MLP.learn-560"><a href="#MLP.learn-560"><span class="linenos">560</span></a>            <span class="k">return</span> <span class="n">cost_vals</span>
</span></pre></div>


            <div class="docstring"><p>The function trains a neural network for a specified number of epochs using a chosen
algorithm, reports the state of the model, plots the cost function, and computes the total run time.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>num_epochs</strong>:  The number of epochs, which is the number of times the neural network will be
trained on the dataset</li>
<li><strong>algorithm</strong>:  Determines the learning algorithm to be used for training the neural network. 
The default value is <code>stochastic_descent</code>, but you can pass any other function that implements a 
different learning algorithm</li>
<li><strong>rate</strong>:  The learning rate of the neural network. It determines how quickly the parameters of the 
network are updated during training. A higher learning rate can result in faster convergence, but it 
may also cause the network to overshoot the optimal solution. Defaults to <code>1</code> (optional)</li>
<li><strong>batch_ratio</strong>:  Determines the ratio of the training data used in each iteration of the learning 
algorithm</li>
<li><strong>threshold</strong>:  Is used to determine when to stop the learning process. If the current cost of the 
model falls below the threshold value, the learning process will stop</li>
<li><strong>stop</strong>:  Determines whether the learning process should stop when the threshold cost is reached. 
If set to <code>True</code>, the learning process will stop when the cost falls below the specified threshold. 
Otherwise, the learning process will continue until all epochs are completed, regardless of the cost. 
Defaults to <code>True</code> (optional)</li>
<li><strong>plot_static</strong>:  Determines whether to plot the graph of the cost function after training the neural 
network. If set to <code>True</code>, the graph will be plotted using the <code>static_plot</code> method. Defaults to <code>False</code> 
(optional)</li>
<li><strong>plot_dynamic</strong>:  Determines whether or not to plot the cost function in real-time during the learning 
process. If set to <code>True</code>, a separate process will be created to handle the plotting. Defaults to <code>False</code> 
(optional)</li>
<li><strong>upd_interval</strong>:  Determines the number of epochs after which the dynamic cost plot is updated during 
the learning process. Defaults to <code>20</code> (optional)</li>
<li><strong>with_full_report</strong>:  Determines whether to include a full report of the neural network's state after 
training. If set to <code>True</code>, the full report will be printed to the console. Otherwise, only a summary 
of the neural network's state will be printed. Defaults to <code>False</code> (optional)</li>
<li><strong>return_cost</strong>:  Determines whether the function should return the list of cost values. If set to 
<code>True</code>, the function will return a list of cost values for each epoch. Defaults to <code>False</code> (optional)</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>If the <code>return_cost</code> parameter is set to <code>True</code>, it returns a list of cost values. Otherwise it 
  returns <code>None</code></p>
</blockquote>
</div>


                            </div>
                            <div id="MLP._static_plot" class="classattr">
                                        <input id="MLP._static_plot-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">_static_plot</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x_vals</span>, </span><span class="param"><span class="n">y_vals</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP._static_plot-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP._static_plot"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP._static_plot-563"><a href="#MLP._static_plot-563"><span class="linenos">563</span></a>    <span class="k">def</span> <span class="nf">_static_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
</span><span id="MLP._static_plot-564"><a href="#MLP._static_plot-564"><span class="linenos">564</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP._static_plot-565"><a href="#MLP._static_plot-565"><span class="linenos">565</span></a><span class="sd">        The function plots the cost of a model against the number of training iterations.</span>
</span><span id="MLP._static_plot-566"><a href="#MLP._static_plot-566"><span class="linenos">566</span></a><span class="sd">        </span>
</span><span id="MLP._static_plot-567"><a href="#MLP._static_plot-567"><span class="linenos">567</span></a><span class="sd">        :param x_vals: Is a list or array of values representing the number of learning iterations. </span>
</span><span id="MLP._static_plot-568"><a href="#MLP._static_plot-568"><span class="linenos">568</span></a><span class="sd">        These values will be plotted on the x-axis of the graph</span>
</span><span id="MLP._static_plot-569"><a href="#MLP._static_plot-569"><span class="linenos">569</span></a><span class="sd">        :param y_vals: Represents the values of the cost function for each corresponding value </span>
</span><span id="MLP._static_plot-570"><a href="#MLP._static_plot-570"><span class="linenos">570</span></a><span class="sd">        in the `x_vals` parameter. These values are used to plot the cost vs. number of training </span>
</span><span id="MLP._static_plot-571"><a href="#MLP._static_plot-571"><span class="linenos">571</span></a><span class="sd">        iterations</span>
</span><span id="MLP._static_plot-572"><a href="#MLP._static_plot-572"><span class="linenos">572</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP._static_plot-573"><a href="#MLP._static_plot-573"><span class="linenos">573</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</span><span id="MLP._static_plot-574"><a href="#MLP._static_plot-574"><span class="linenos">574</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of learning iterations&#39;</span><span class="p">)</span>
</span><span id="MLP._static_plot-575"><a href="#MLP._static_plot-575"><span class="linenos">575</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost of the model&#39;</span><span class="p">)</span>
</span><span id="MLP._static_plot-576"><a href="#MLP._static_plot-576"><span class="linenos">576</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="MLP._static_plot-577"><a href="#MLP._static_plot-577"><span class="linenos">577</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>The function plots the cost of a model against the number of training iterations.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>x_vals</strong>:  Is a list or array of values representing the number of learning iterations. 
These values will be plotted on the x-axis of the graph</li>
<li><strong>y_vals</strong>:  Represents the values of the cost function for each corresponding value 
in the <code>x_vals</code> parameter. These values are used to plot the cost vs. number of training 
iterations</li>
</ul>
</div>


                            </div>
                            <div id="MLP._dynamic_plot" class="classattr">
                                        <input id="MLP._dynamic_plot-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">_dynamic_plot</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="o">*</span><span class="n">args</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP._dynamic_plot-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP._dynamic_plot"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP._dynamic_plot-578"><a href="#MLP._dynamic_plot-578"><span class="linenos">578</span></a>    <span class="k">def</span> <span class="nf">_dynamic_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span><span id="MLP._dynamic_plot-579"><a href="#MLP._dynamic_plot-579"><span class="linenos">579</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot; @public</span>
</span><span id="MLP._dynamic_plot-580"><a href="#MLP._dynamic_plot-580"><span class="linenos">580</span></a><span class="sd">        The function plots the cost function over the number of learning iterations in real-time as the data comes in through </span>
</span><span id="MLP._dynamic_plot-581"><a href="#MLP._dynamic_plot-581"><span class="linenos">581</span></a><span class="sd">        a queue. It updates the plot with each new piece of data received through the queue, allowing for live visualization </span>
</span><span id="MLP._dynamic_plot-582"><a href="#MLP._dynamic_plot-582"><span class="linenos">582</span></a><span class="sd">        of the training process. The plot remains open and dynamically updates until a `None` value is received through the </span>
</span><span id="MLP._dynamic_plot-583"><a href="#MLP._dynamic_plot-583"><span class="linenos">583</span></a><span class="sd">        queue.</span>
</span><span id="MLP._dynamic_plot-584"><a href="#MLP._dynamic_plot-584"><span class="linenos">584</span></a>
</span><span id="MLP._dynamic_plot-585"><a href="#MLP._dynamic_plot-585"><span class="linenos">585</span></a><span class="sd">        :param args: The first element of `args` is expected to be a queue that provides tuples of `(iteration, cost)`, where iteration </span>
</span><span id="MLP._dynamic_plot-586"><a href="#MLP._dynamic_plot-586"><span class="linenos">586</span></a><span class="sd">        is an integer representing the iteration number, and cost is a float representing the cost value at that iteration</span>
</span><span id="MLP._dynamic_plot-587"><a href="#MLP._dynamic_plot-587"><span class="linenos">587</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP._dynamic_plot-588"><a href="#MLP._dynamic_plot-588"><span class="linenos">588</span></a>        <span class="c1"># gets the queue</span>
</span><span id="MLP._dynamic_plot-589"><a href="#MLP._dynamic_plot-589"><span class="linenos">589</span></a>        <span class="n">queue</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP._dynamic_plot-590"><a href="#MLP._dynamic_plot-590"><span class="linenos">590</span></a>        <span class="c1"># small constant for scaling of axes</span>
</span><span id="MLP._dynamic_plot-591"><a href="#MLP._dynamic_plot-591"><span class="linenos">591</span></a>        <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>
</span><span id="MLP._dynamic_plot-592"><a href="#MLP._dynamic_plot-592"><span class="linenos">592</span></a>        <span class="c1"># initializes the plot</span>
</span><span id="MLP._dynamic_plot-593"><a href="#MLP._dynamic_plot-593"><span class="linenos">593</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span id="MLP._dynamic_plot-594"><a href="#MLP._dynamic_plot-594"><span class="linenos">594</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-595"><a href="#MLP._dynamic_plot-595"><span class="linenos">595</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-596"><a href="#MLP._dynamic_plot-596"><span class="linenos">596</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-597"><a href="#MLP._dynamic_plot-597"><span class="linenos">597</span></a>        
</span><span id="MLP._dynamic_plot-598"><a href="#MLP._dynamic_plot-598"><span class="linenos">598</span></a>        <span class="n">x_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP._dynamic_plot-599"><a href="#MLP._dynamic_plot-599"><span class="linenos">599</span></a>        <span class="n">y_vals</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MLP._dynamic_plot-600"><a href="#MLP._dynamic_plot-600"><span class="linenos">600</span></a>
</span><span id="MLP._dynamic_plot-601"><a href="#MLP._dynamic_plot-601"><span class="linenos">601</span></a>        <span class="c1"># creates the line object</span>
</span><span id="MLP._dynamic_plot-602"><a href="#MLP._dynamic_plot-602"><span class="linenos">602</span></a>        <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-603"><a href="#MLP._dynamic_plot-603"><span class="linenos">603</span></a>        <span class="n">cost_text</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-604"><a href="#MLP._dynamic_plot-604"><span class="linenos">604</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-605"><a href="#MLP._dynamic_plot-605"><span class="linenos">605</span></a>
</span><span id="MLP._dynamic_plot-606"><a href="#MLP._dynamic_plot-606"><span class="linenos">606</span></a>        <span class="k">try</span><span class="p">:</span>
</span><span id="MLP._dynamic_plot-607"><a href="#MLP._dynamic_plot-607"><span class="linenos">607</span></a>            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span><span id="MLP._dynamic_plot-608"><a href="#MLP._dynamic_plot-608"><span class="linenos">608</span></a>                <span class="c1"># checks if there&#39;s data in the queue</span>
</span><span id="MLP._dynamic_plot-609"><a href="#MLP._dynamic_plot-609"><span class="linenos">609</span></a>                <span class="k">if</span> <span class="ow">not</span> <span class="n">queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
</span><span id="MLP._dynamic_plot-610"><a href="#MLP._dynamic_plot-610"><span class="linenos">610</span></a>                    <span class="c1"># gets values from the queue</span>
</span><span id="MLP._dynamic_plot-611"><a href="#MLP._dynamic_plot-611"><span class="linenos">611</span></a>                    <span class="n">data</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span><span id="MLP._dynamic_plot-612"><a href="#MLP._dynamic_plot-612"><span class="linenos">612</span></a>                    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="MLP._dynamic_plot-613"><a href="#MLP._dynamic_plot-613"><span class="linenos">613</span></a>                        <span class="k">break</span>
</span><span id="MLP._dynamic_plot-614"><a href="#MLP._dynamic_plot-614"><span class="linenos">614</span></a>                    
</span><span id="MLP._dynamic_plot-615"><a href="#MLP._dynamic_plot-615"><span class="linenos">615</span></a>                    <span class="c1"># stores the values for plotting</span>
</span><span id="MLP._dynamic_plot-616"><a href="#MLP._dynamic_plot-616"><span class="linenos">616</span></a>                    <span class="n">iteration</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">data</span>
</span><span id="MLP._dynamic_plot-617"><a href="#MLP._dynamic_plot-617"><span class="linenos">617</span></a>                    <span class="n">x_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-618"><a href="#MLP._dynamic_plot-618"><span class="linenos">618</span></a>                    <span class="n">y_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-619"><a href="#MLP._dynamic_plot-619"><span class="linenos">619</span></a>
</span><span id="MLP._dynamic_plot-620"><a href="#MLP._dynamic_plot-620"><span class="linenos">620</span></a>                    <span class="c1"># updates the xdata and ydata of the line</span>
</span><span id="MLP._dynamic_plot-621"><a href="#MLP._dynamic_plot-621"><span class="linenos">621</span></a>                    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-622"><a href="#MLP._dynamic_plot-622"><span class="linenos">622</span></a>
</span><span id="MLP._dynamic_plot-623"><a href="#MLP._dynamic_plot-623"><span class="linenos">623</span></a>                    <span class="c1"># sets appropriate plot limits</span>
</span><span id="MLP._dynamic_plot-624"><a href="#MLP._dynamic_plot-624"><span class="linenos">624</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span><span class="o">-</span><span class="n">eps</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-625"><a href="#MLP._dynamic_plot-625"><span class="linenos">625</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y_vals</span><span class="p">)</span><span class="o">-</span><span class="n">eps</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_vals</span><span class="p">)</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-626"><a href="#MLP._dynamic_plot-626"><span class="linenos">626</span></a>
</span><span id="MLP._dynamic_plot-627"><a href="#MLP._dynamic_plot-627"><span class="linenos">627</span></a>                    <span class="c1"># updates the cost text with the current value of y</span>
</span><span id="MLP._dynamic_plot-628"><a href="#MLP._dynamic_plot-628"><span class="linenos">628</span></a>                    <span class="n">cost_text</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Current accuracy: </span><span class="si">{</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">cost</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</span><span id="MLP._dynamic_plot-629"><a href="#MLP._dynamic_plot-629"><span class="linenos">629</span></a>
</span><span id="MLP._dynamic_plot-630"><a href="#MLP._dynamic_plot-630"><span class="linenos">630</span></a>                    <span class="c1"># draws the updated plot</span>
</span><span id="MLP._dynamic_plot-631"><a href="#MLP._dynamic_plot-631"><span class="linenos">631</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</span><span id="MLP._dynamic_plot-632"><a href="#MLP._dynamic_plot-632"><span class="linenos">632</span></a>                    <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># pauses for a short duration to display the plot</span>
</span><span id="MLP._dynamic_plot-633"><a href="#MLP._dynamic_plot-633"><span class="linenos">633</span></a>
</span><span id="MLP._dynamic_plot-634"><a href="#MLP._dynamic_plot-634"><span class="linenos">634</span></a>        <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
</span><span id="MLP._dynamic_plot-635"><a href="#MLP._dynamic_plot-635"><span class="linenos">635</span></a>            <span class="k">pass</span>
</span><span id="MLP._dynamic_plot-636"><a href="#MLP._dynamic_plot-636"><span class="linenos">636</span></a>        
</span><span id="MLP._dynamic_plot-637"><a href="#MLP._dynamic_plot-637"><span class="linenos">637</span></a>        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>The function plots the cost function over the number of learning iterations in real-time as the data comes in through 
a queue. It updates the plot with each new piece of data received through the queue, allowing for live visualization 
of the training process. The plot remains open and dynamically updates until a <code>None</code> value is received through the 
queue.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>args</strong>:  The first element of <code>args</code> is expected to be a queue that provides tuples of <code>(iteration, cost)</code>, where iteration 
is an integer representing the iteration number, and cost is a float representing the cost value at that iteration</li>
</ul>
</div>


                            </div>
                            <div id="MLP.export_mlp" class="classattr">
                                        <input id="MLP.export_mlp-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">export_mlp</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">data_path</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP.export_mlp-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP.export_mlp"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP.export_mlp-640"><a href="#MLP.export_mlp-640"><span class="linenos">640</span></a>    <span class="k">def</span> <span class="nf">export_mlp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">):</span>
</span><span id="MLP.export_mlp-641"><a href="#MLP.export_mlp-641"><span class="linenos">641</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP.export_mlp-642"><a href="#MLP.export_mlp-642"><span class="linenos">642</span></a><span class="sd">        The function creates a dictionary, that contains the current parameters and activation</span>
</span><span id="MLP.export_mlp-643"><a href="#MLP.export_mlp-643"><span class="linenos">643</span></a><span class="sd">        functions of a neural network, and saves it to a file, which can be later used by the method </span>
</span><span id="MLP.export_mlp-644"><a href="#MLP.export_mlp-644"><span class="linenos">644</span></a><span class="sd">        `import_mlp` to restore the state of the neural network.</span>
</span><span id="MLP.export_mlp-645"><a href="#MLP.export_mlp-645"><span class="linenos">645</span></a><span class="sd">        </span>
</span><span id="MLP.export_mlp-646"><a href="#MLP.export_mlp-646"><span class="linenos">646</span></a><span class="sd">        :param data_path: Is a string that represents the path where the neural network image will be saved</span>
</span><span id="MLP.export_mlp-647"><a href="#MLP.export_mlp-647"><span class="linenos">647</span></a><span class="sd">        as a numpy file</span>
</span><span id="MLP.export_mlp-648"><a href="#MLP.export_mlp-648"><span class="linenos">648</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP.export_mlp-649"><a href="#MLP.export_mlp-649"><span class="linenos">649</span></a>        <span class="n">mlp_state</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP.export_mlp-650"><a href="#MLP.export_mlp-650"><span class="linenos">650</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP.export_mlp-651"><a href="#MLP.export_mlp-651"><span class="linenos">651</span></a>            <span class="n">act_func</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_funcs</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>
</span><span id="MLP.export_mlp-652"><a href="#MLP.export_mlp-652"><span class="linenos">652</span></a>            <span class="n">mlp_state</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">act_func</span><span class="p">)</span>
</span><span id="MLP.export_mlp-653"><a href="#MLP.export_mlp-653"><span class="linenos">653</span></a>
</span><span id="MLP.export_mlp-654"><a href="#MLP.export_mlp-654"><span class="linenos">654</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">mlp_state</span><span class="p">)</span>
</span><span id="MLP.export_mlp-655"><a href="#MLP.export_mlp-655"><span class="linenos">655</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The image was successfully exported to &quot;</span> <span class="o">+</span> <span class="n">data_path</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>The function creates a dictionary, that contains the current parameters and activation
functions of a neural network, and saves it to a file, which can be later used by the method 
<code><a href="#MLP.import_mlp">import_mlp</a></code> to restore the state of the neural network.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>data_path</strong>:  Is a string that represents the path where the neural network image will be saved
as a numpy file</li>
</ul>
</div>


                            </div>
                            <div id="MLP.import_mlp" class="classattr">
                                        <input id="MLP.import_mlp-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">import_mlp</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">data_path</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MLP.import_mlp-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLP.import_mlp"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLP.import_mlp-656"><a href="#MLP.import_mlp-656"><span class="linenos">656</span></a>    <span class="k">def</span> <span class="nf">import_mlp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">):</span>
</span><span id="MLP.import_mlp-657"><a href="#MLP.import_mlp-657"><span class="linenos">657</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="MLP.import_mlp-658"><a href="#MLP.import_mlp-658"><span class="linenos">658</span></a><span class="sd">        The function imports new parameters for a neural network and updates the corresponding class instance </span>
</span><span id="MLP.import_mlp-659"><a href="#MLP.import_mlp-659"><span class="linenos">659</span></a><span class="sd">        attributes. It collaborates with the method `export_mlp` to streamline the storage of neural networks.</span>
</span><span id="MLP.import_mlp-660"><a href="#MLP.import_mlp-660"><span class="linenos">660</span></a><span class="sd">        </span>
</span><span id="MLP.import_mlp-661"><a href="#MLP.import_mlp-661"><span class="linenos">661</span></a><span class="sd">        :param data_path: Is a string that represents the path to the file from which the new parameters for </span>
</span><span id="MLP.import_mlp-662"><a href="#MLP.import_mlp-662"><span class="linenos">662</span></a><span class="sd">        the neural network will be imported</span>
</span><span id="MLP.import_mlp-663"><a href="#MLP.import_mlp-663"><span class="linenos">663</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="MLP.import_mlp-664"><a href="#MLP.import_mlp-664"><span class="linenos">664</span></a>        <span class="c1"># read new parameters from the file</span>
</span><span id="MLP.import_mlp-665"><a href="#MLP.import_mlp-665"><span class="linenos">665</span></a>        <span class="n">new_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="s1">&#39;TRUE&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="MLP.import_mlp-666"><a href="#MLP.import_mlp-666"><span class="linenos">666</span></a>        <span class="c1"># creates temp dicts to store new configuration</span>
</span><span id="MLP.import_mlp-667"><a href="#MLP.import_mlp-667"><span class="linenos">667</span></a>        <span class="n">act_funcs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP.import_mlp-668"><a href="#MLP.import_mlp-668"><span class="linenos">668</span></a>        <span class="n">new_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span id="MLP.import_mlp-669"><a href="#MLP.import_mlp-669"><span class="linenos">669</span></a>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">new_state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="MLP.import_mlp-670"><a href="#MLP.import_mlp-670"><span class="linenos">670</span></a>            <span class="c1"># checks if there is act func</span>
</span><span id="MLP.import_mlp-671"><a href="#MLP.import_mlp-671"><span class="linenos">671</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="MLP.import_mlp-672"><a href="#MLP.import_mlp-672"><span class="linenos">672</span></a>                <span class="n">new_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="MLP.import_mlp-673"><a href="#MLP.import_mlp-673"><span class="linenos">673</span></a>                <span class="c1"># retrieves act func name</span>
</span><span id="MLP.import_mlp-674"><a href="#MLP.import_mlp-674"><span class="linenos">674</span></a>                <span class="n">act_funcs</span> <span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="MLP.import_mlp-675"><a href="#MLP.import_mlp-675"><span class="linenos">675</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="MLP.import_mlp-676"><a href="#MLP.import_mlp-676"><span class="linenos">676</span></a>                <span class="n">new_params</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span>
</span><span id="MLP.import_mlp-677"><a href="#MLP.import_mlp-677"><span class="linenos">677</span></a>                <span class="c1"># assigns the default act func</span>
</span><span id="MLP.import_mlp-678"><a href="#MLP.import_mlp-678"><span class="linenos">678</span></a>                <span class="n">act_funcs</span> <span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span>
</span><span id="MLP.import_mlp-679"><a href="#MLP.import_mlp-679"><span class="linenos">679</span></a>        <span class="c1"># updated weights and biases</span>
</span><span id="MLP.import_mlp-680"><a href="#MLP.import_mlp-680"><span class="linenos">680</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">new_params</span><span class="p">)</span>
</span><span id="MLP.import_mlp-681"><a href="#MLP.import_mlp-681"><span class="linenos">681</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_mlp_depth</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span>
</span><span id="MLP.import_mlp-682"><a href="#MLP.import_mlp-682"><span class="linenos">682</span></a>        <span class="c1"># maps layers to act funcs</span>
</span><span id="MLP.import_mlp-683"><a href="#MLP.import_mlp-683"><span class="linenos">683</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_act_func_map</span><span class="p">(</span><span class="n">act_funcs</span><span class="p">)</span>
</span><span id="MLP.import_mlp-684"><a href="#MLP.import_mlp-684"><span class="linenos">684</span></a>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The image was successfully imported from &quot;</span> <span class="o">+</span> <span class="n">data_path</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>The function imports new parameters for a neural network and updates the corresponding class instance 
attributes. It collaborates with the method <code><a href="#MLP.export_mlp">export_mlp</a></code> to streamline the storage of neural networks.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>data_path</strong>:  Is a string that represents the path to the file from which the new parameters for 
the neural network will be imported</li>
</ul>
</div>


                            </div>
                </section>
                <section id="main">
                            <input id="main-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">main</span><span class="signature pdoc-code condensed">(<span class="return-annotation">):</span></span>

                <label class="view-source-button" for="main-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#main"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="main-880"><a href="#main-880"><span class="linenos">880</span></a><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span><span id="main-881"><a href="#main-881"><span class="linenos">881</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="main-882"><a href="#main-882"><span class="linenos">882</span></a><span class="sd">    The main function is used to test the neural network implementation.</span>
</span><span id="main-883"><a href="#main-883"><span class="linenos">883</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="main-884"><a href="#main-884"><span class="linenos">884</span></a>    <span class="n">multiprocessing</span><span class="o">.</span><span class="n">freeze_support</span><span class="p">()</span>
</span><span id="main-885"><a href="#main-885"><span class="linenos">885</span></a>    <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="main-886"><a href="#main-886"><span class="linenos">886</span></a>    <span class="n">rand_range</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="main-887"><a href="#main-887"><span class="linenos">887</span></a>
</span><span id="main-888"><a href="#main-888"><span class="linenos">888</span></a>    <span class="n">n_bit</span> <span class="o">=</span> <span class="mi">4</span>
</span><span id="main-889"><a href="#main-889"><span class="linenos">889</span></a>    <span class="n">adder_train</span> <span class="o">=</span> <span class="n">adder_truth_table</span><span class="p">(</span><span class="n">n_bit</span><span class="p">)</span>
</span><span id="main-890"><a href="#main-890"><span class="linenos">890</span></a>    <span class="n">act_funcs</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">}</span>
</span><span id="main-891"><a href="#main-891"><span class="linenos">891</span></a>    <span class="n">adder_layout</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">n_bit</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">n_bit</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">n_bit</span><span class="p">,</span> <span class="n">n_bit</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">act_funcs</span><span class="p">)</span>
</span><span id="main-892"><a href="#main-892"><span class="linenos">892</span></a>
</span><span id="main-893"><a href="#main-893"><span class="linenos">893</span></a>    <span class="n">adder_mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">adder_layout</span><span class="p">,</span> <span class="n">rand_range</span><span class="p">,</span> <span class="n">adder_train</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n_bit</span><span class="si">}</span><span class="s2">-bit Adder&quot;</span><span class="p">)</span>
</span><span id="main-894"><a href="#main-894"><span class="linenos">894</span></a>    <span class="n">adder_mlp</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">batch_ratio</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">adder_train</span><span class="p">),</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plot_dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>The main function is used to test the neural network implementation.</p>
</div>


                </section>
    </main>
</body>
</html>